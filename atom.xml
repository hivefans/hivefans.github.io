<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[东杰书屋]]></title>
  <subtitle><![CDATA[环境不会改变，解决之道在于改变自己。]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://blog.djstudy.net/"/>
  <updated>2016-02-04T11:30:21.000Z</updated>
  <id>http://blog.djstudy.net/</id>
  
  <author>
    <name><![CDATA[东杰]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Presto, Parquet & Airpal]]></title>
    <link href="http://blog.djstudy.net/2016/02/04/presto-parquet-airpal/"/>
    <id>http://blog.djstudy.net/2016/02/04/presto-parquet-airpal/</id>
    <published>2016-02-04T11:30:21.000Z</published>
    <updated>2016-02-04T11:30:21.000Z</updated>
    <content type="html"><![CDATA[<p>I came across a blog post from Brandon Harris recently where he discussed a credit card fraud detection project he’d been working on with a team at the University of Chicago. In the post he described how Presto and Parquet-formatted files had gone a long way to speeding up ad-hoc queries against a ~250GB dataset he’s working with.</p>
<p>Presto was born at Facebook and was open sourced within a year of it’s inception. It’s a distributed query engine capable of running interactive queries against big data sources. There’s support for data sources such as Hive, Kafka, PostgreSQL, Redis and Cassandra among many others. Netflix has blogged about their positive experiences with Presto on a 10PB Data Warehouse they’ve got that’s happily handling 2,500 ad-hoc queries a day.</p>
<p>In Brandon’s blog post there is a chart showing a query that’s executed in Hive against data stored in CSV format taking 130 seconds and then the same query run via Presto against data stored in Parquet format taking less than 5 seconds. I trust the measurements of his queries are accurate but what I’m interested in is what is involved in getting an environment up to run these sorts of queries.</p>
<p>As of this writing Bigtop’s Presto support isn’t ready (though pull requests are being worked on) so to get an environment up and running locally I’ll have to perform some of the installation steps manually.</p>
<h2 id="Launching_a_Hadoop_Cluster_in_Docker_Containers"><a href="#Launching_a_Hadoop_Cluster_in_Docker_Containers" class="headerlink" title="Launching a Hadoop Cluster in Docker Containers"></a>Launching a Hadoop Cluster in Docker Containers</h2><p>This process begins with a fresh Ubuntu 15 installation acting as the host for Docker containers that a Hadoop cluster will live within. I discuss getting Ubuntu 15 ready to run Docker in my Hadoop Up and Running blog post.</p>
<p>With Docker ready I’ll checkout the Bigtop git repository and launch Ubuntu 14.04-based containers (as of this writing this is the latest supported version of Ubuntu on Intel-based systems).<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/apache/bigtop.git</span><br><span class="line">$ <span class="built_in">cd</span> bigtop/bigtop-deploy/vm/vagrant-puppet-docker/</span><br><span class="line">$ sudo docker pull bigtop/deploy:ubuntu-<span class="number">14.04</span></span><br><span class="line">$ vi vagrantconfig.yaml</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">docker:</span><br><span class="line">        memory_size: <span class="string">"4096"</span></span><br><span class="line">        image:  <span class="string">"bigtop/deploy:ubuntu-14.04"</span></span><br><span class="line"></span><br><span class="line">boot2docker:</span><br><span class="line">        memory_size: <span class="string">"4096"</span></span><br><span class="line">        number_cpus: <span class="string">"1"</span></span><br><span class="line"></span><br><span class="line">repo: <span class="string">"http://bigtop-repos.s3.amazonaws.com/releases/1.0.0/ubuntu/trusty/x86_64"</span></span><br><span class="line">distro: debian</span><br><span class="line">components: [hadoop, yarn, hive]</span><br><span class="line">namenode_ui_port: <span class="string">"50070"</span></span><br><span class="line">yarn_ui_port: <span class="string">"8088"</span></span><br><span class="line">hbase_ui_port: <span class="string">"60010"</span></span><br><span class="line"><span class="built_in">enable</span>_<span class="built_in">local</span>_repo: <span class="literal">false</span></span><br><span class="line">smoke_<span class="built_in">test</span>_components: [mapreduce, pig]</span><br><span class="line">jdk: <span class="string">"openjdk-7-jdk"</span></span><br></pre></td></tr></table></figure>
<p>While I was writing this blog post Bigtop 1.1 was being cut and the resources from their 1.1.0 endpoint were returning HTTP 403 messages so I’ve stuck with the 1.0.0 endpoints for now.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ./docker-hadoop.sh --create <span class="number">3</span></span><br><span class="line">$ sudo vagrant ssh bigtop1</span><br></pre></td></tr></table></figure></p>
<h2 id="Getting_Hive_u2019s_Metastore_Up_and_Running"><a href="#Getting_Hive_u2019s_Metastore_Up_and_Running" class="headerlink" title="Getting Hive’s Metastore Up and Running"></a>Getting Hive’s Metastore Up and Running</h2><p>By default the Derby embedded database driver is enabled in the boilerplate Hive configurations provided by Bigtop. This driver can only allow one process at a time to connect to it. If you use it Hive’s Metastore server won’t communicate properly with Presto and you’ll get “does not exist” messages every time you try to access a table.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> track_metadata_csv;</span></span><br><span class="line">... Table hive.default.track_metadata_csv does not exist</span><br></pre></td></tr></table></figure></p>
<p>For this reason I’ve installed MySQL and used it as the data backend for Hive’s Metastore.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install \</span><br><span class="line">    libmysql-java \</span><br><span class="line">    mysql-server</span><br><span class="line">$ ln <span class="operator">-s</span> /usr/share/java/mysql-connector-java.jar \</span><br><span class="line">        /usr/lib/hive/lib/mysql-connector-java.jar</span><br><span class="line">$ /etc/init.d/mysql start</span><br><span class="line">$ mysql -uroot -proot <span class="operator">-e</span><span class="string">'CREATE DATABASE hcatalog;'</span></span><br></pre></td></tr></table></figure></p>
<p>When you install MySQL you’ll be prompted to set a root login and password. I’ve set both values to root in this example. Below is the Hive site configuration file.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/hive/conf/hive-site.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="pi">&lt;?xml version="1.0"?&gt;</span></span><br><span class="line"><span class="pi">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="title">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>bigtop1.docker<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>mr<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>jdbc:mysql://localhost/hcatalog?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>root<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>root<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="title">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">name</span>&gt;</span>hive.hwi.war.file<span class="tag">&lt;/<span class="title">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="title">value</span>&gt;</span>/usr/lib/hive/lib/hive-hwi.war<span class="tag">&lt;/<span class="title">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="title">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="title">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>I then launched Hive and made sure the Metastore’s database exists.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hive</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> metastore_db;</span></span><br></pre></td></tr></table></figure>
<p>After that I closed out of Hive and launched it’s Metastore service in the background. You’ll see it binded to port 9083 if it’s running.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hive --service metastore &amp;</span><br><span class="line">$ netstat -an | grep <span class="number">9083</span></span><br></pre></td></tr></table></figure></p>
<h2 id="Getting_some_data_to_play_with"><a href="#Getting_some_data_to_play_with" class="headerlink" title="Getting some data to play with"></a>Getting some data to play with</h2><p>I need some data to play with in this exercise so I dumped the Million Song Dataset to CSV and imported it into HDFS</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install sqlite3</span><br><span class="line">$ wget -c http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/track_metadata.db</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sqlite3 track_metadata.db &lt;&lt;!</span><br><span class="line">.headers off</span><br><span class="line">.mode csv</span><br><span class="line">.output track_metadata.csv</span><br><span class="line">SELECT track_id,</span><br><span class="line">     artist_id,</span><br><span class="line">     artist_familiarity,</span><br><span class="line">     artist_hotttnesss,</span><br><span class="line">     duration,</span><br><span class="line">     year</span><br><span class="line">FROM songs;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -put \</span><br><span class="line">    track_metadata.csv \</span><br><span class="line">    /tmp/track_metadata.csv</span><br></pre></td></tr></table></figure>
<p>With the CSV file sitting in HDFS I’ll create a Hive table for it. Once that table is created I can create a second, Parquet-formatted table and import the data from the first table into the second.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hive</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> track_metadata_csv (</span><br><span class="line">    track_id            <span class="built_in">VARCHAR</span>(<span class="number">18</span>),</span><br><span class="line">    artist_id           <span class="built_in">VARCHAR</span>(<span class="number">18</span>),</span><br><span class="line">    artist_familiarity  <span class="built_in">DECIMAL</span>(<span class="number">16</span>, <span class="number">15</span>),</span><br><span class="line">    artist_hotttnesss   <span class="built_in">DECIMAL</span>(<span class="number">16</span>, <span class="number">15</span>),</span><br><span class="line">    <span class="keyword">duration</span>            <span class="built_in">DECIMAL</span>(<span class="number">12</span>, <span class="number">8</span>),</span><br><span class="line">    <span class="keyword">year</span>                <span class="built_in">SMALLINT</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> INPATH <span class="string">'/tmp/track_metadata.csv'</span></span><br><span class="line"><span class="keyword">INTO</span> <span class="keyword">TABLE</span> track_metadata_csv;</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> track_metadata_parquet (</span><br><span class="line">    track_id            <span class="built_in">VARCHAR</span>(<span class="number">18</span>),</span><br><span class="line">    artist_id           <span class="built_in">VARCHAR</span>(<span class="number">18</span>),</span><br><span class="line">    artist_familiarity  <span class="built_in">DECIMAL</span>(<span class="number">16</span>, <span class="number">15</span>),</span><br><span class="line">    artist_hotttnesss   <span class="built_in">DECIMAL</span>(<span class="number">16</span>, <span class="number">15</span>),</span><br><span class="line">    <span class="keyword">duration</span>            <span class="built_in">DECIMAL</span>(<span class="number">12</span>, <span class="number">8</span>),</span><br><span class="line">    <span class="keyword">year</span>                <span class="built_in">SMALLINT</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> parquet;</span></span><br><span class="line"></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> track_metadata_parquet</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> track_metadata_csv;</span></span><br></pre></td></tr></table></figure>
<p>I now have that table’s metadata stored in Hive’s Metastore and a Parquet-formatted file of that data sitting on HDFS:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls \</span><br><span class="line">    /user/hive/warehouse/track_metadata_parquet</span><br><span class="line">Found <span class="number">1</span> items</span><br><span class="line">-rw-r--r--   <span class="number">3</span> root hadoop   <span class="number">45849172</span> <span class="number">2016</span>-<span class="number">01</span>-<span class="number">31</span> <span class="number">13</span>:<span class="number">31</span> /user/hive/warehouse/track_metadata_parquet/<span class="number">000000</span>_0</span><br><span class="line">$ mysqldump \</span><br><span class="line">    -uroot \</span><br><span class="line">    -proot \</span><br><span class="line">    --no-create-info \</span><br><span class="line">    --skip-add-locks \</span><br><span class="line">    --skip-disable-keys \</span><br><span class="line">    --skip-comments \</span><br><span class="line">    hcatalog | \</span><br><span class="line">    grep ^INSERT | \</span><br><span class="line">    sort</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`CDS`</span> <span class="keyword">VALUES</span> (<span class="number">1</span>),(<span class="number">2</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`COLUMNS_V2`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'artist_familiarity'</span>,<span class="string">'decimal(16,15)'</span>,<span class="number">2</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'artist_hotttnesss'</span>,<span class="string">'decimal(16,15)'</span>,<span class="number">3</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'artist_id'</span>,<span class="string">'varchar(18)'</span>,<span class="number">1</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'duration'</span>,<span class="string">'decimal(12,8)'</span>,<span class="number">4</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'track_id'</span>,<span class="string">'varchar(18)'</span>,<span class="number">0</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'year'</span>,<span class="string">'smallint'</span>,<span class="number">5</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'artist_familiarity'</span>,<span class="string">'decimal(16,15)'</span>,<span class="number">2</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'artist_hotttnesss'</span>,<span class="string">'decimal(16,15)'</span>,<span class="number">3</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'artist_id'</span>,<span class="string">'varchar(18)'</span>,<span class="number">1</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'duration'</span>,<span class="string">'decimal(12,8)'</span>,<span class="number">4</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'track_id'</span>,<span class="string">'varchar(18)'</span>,<span class="number">0</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'year'</span>,<span class="string">'smallint'</span>,<span class="number">5</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`DBS`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="string">'Default Hive database'</span>,<span class="string">'hdfs://bigtop1.docker:8020/user/hive/warehouse'</span>,<span class="string">'default'</span>,<span class="string">'public'</span>,<span class="string">'ROLE'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'hdfs://bigtop1.docker:8020/user/hive/warehouse/metastore_db.db'</span>,<span class="string">'metastore_db'</span>,<span class="string">'root'</span>,<span class="string">'USER'</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`GLOBAL_PRIVS`</span> <span class="keyword">VALUES</span> (<span class="number">1</span>,<span class="number">1454255378</span>,<span class="number">1</span>,<span class="string">'admin'</span>,<span class="string">'ROLE'</span>,<span class="string">'admin'</span>,<span class="string">'ROLE'</span>,<span class="string">'All'</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ROLES`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="number">1454255378</span>,<span class="string">'admin'</span>,<span class="string">'admin'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="number">1454255378</span>,<span class="string">'public'</span>,<span class="string">'public'</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`SDS`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="number">1</span>,<span class="string">'org.apache.hadoop.mapred.TextInputFormat'</span>,<span class="string">'\0'</span>,<span class="string">'\0'</span>,<span class="string">'hdfs://bigtop1.docker:8020/user/hive/warehouse/track_metadata_csv'</span>,-<span class="number">1</span>,<span class="string">'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</span>,<span class="number">1</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="number">2</span>,<span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'</span>,<span class="string">'\0'</span>,<span class="string">'\0'</span>,<span class="string">'hdfs://bigtop1.docker:8020/user/hive/warehouse/track_metadata_parquet'</span>,-<span class="number">1</span>,<span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'</span>,<span class="number">2</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`SEQUENCE_TABLE`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MColumnDescriptor'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MDatabase'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MGlobalPrivilege'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MRole'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MSerDeInfo'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MStorageDescriptor'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MTable'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MVersionTable'</span>,<span class="number">6</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`SERDES`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`SERDE_PARAMS`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="string">'field.delim'</span>,<span class="string">','</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="string">'serialization.format'</span>,<span class="string">','</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'serialization.format'</span>,<span class="string">'1'</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`TABLE_PARAMS`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="string">'COLUMN_STATS_ACCURATE'</span>,<span class="string">'true'</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="string">'numFiles'</span>,<span class="string">'2'</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="string">'numRows'</span>,<span class="string">'0'</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="string">'rawDataSize'</span>,<span class="string">'0'</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="string">'totalSize'</span>,<span class="string">'161179824'</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="string">'transient_lastDdlTime'</span>,<span class="string">'1454255466'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'COLUMN_STATS_ACCURATE'</span>,<span class="string">'true'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'numFiles'</span>,<span class="string">'2'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'numRows'</span>,<span class="string">'2000000'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'rawDataSize'</span>,<span class="string">'12000000'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'totalSize'</span>,<span class="string">'136579903'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'transient_lastDdlTime'</span>,<span class="string">'1454255498'</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`TBLS`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="number">1454255462</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="string">'root'</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">'track_metadata_csv'</span>,<span class="string">'MANAGED_TABLE'</span>,<span class="literal">NULL</span>,<span class="literal">NULL</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="number">1454255471</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="string">'root'</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="string">'track_metadata_parquet'</span>,<span class="string">'MANAGED_TABLE'</span>,<span class="literal">NULL</span>,<span class="literal">NULL</span>);</span></span><br><span class="line"><span class="operator"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`VERSION`</span> <span class="keyword">VALUES</span> (<span class="number">1</span>,<span class="string">'0.14.0'</span>,<span class="string">'Set by MetaStore'</span>);</span></span><br></pre></td></tr></table></figure>
<h2 id="Presto_Up_and_Running"><a href="#Presto_Up_and_Running" class="headerlink" title="Presto Up and Running"></a>Presto Up and Running</h2><p>Presto requires Java 8 so I’ll install that first.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ add-apt-repository ppa:webupd8team/java</span><br><span class="line">$ apt-get update</span><br><span class="line">$ apt-get install oracle-java8-installer</span><br></pre></td></tr></table></figure></p>
<p>I have yet to see recent Debian packages for Presto so I’ll download the binaries instead.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~</span><br><span class="line">$ wget -c https://repo1.maven.org/maven2/com/facebook/presto/presto-server/<span class="number">0.133</span>/presto-server-<span class="number">0.133</span>.tar.gz</span><br><span class="line">$ tar xzf presto-server-<span class="number">0.133</span>.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>Presto requires a data folder for it to store locks, logs and a few other items and also requires a number of configuration files before it can begin to work properly.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p /root/datap</span><br><span class="line">$ mkdir -p ~/presto-server-<span class="number">0.133</span>/etc/catalog</span><br><span class="line">$ <span class="built_in">cd</span> ~/presto-server-<span class="number">0.133</span>/etc</span><br></pre></td></tr></table></figure></p>
<p>Normally I wouldn’t suggest creating a data folder in the root partition but this work is all taking place in a Docker container that has exposed 13GB of space on the root partition.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ df -H</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">none             <span class="number">13</span>G  <span class="number">6.4</span>G  <span class="number">5.6</span>G  <span class="number">54</span>% /</span><br><span class="line">tmpfs           <span class="number">4.2</span>G  <span class="number">8.2</span>k  <span class="number">4.2</span>G   <span class="number">1</span>% /dev</span><br><span class="line">tmpfs           <span class="number">4.2</span>G     <span class="number">0</span>  <span class="number">4.2</span>G   <span class="number">0</span>% /sys/fs/cgroup</span><br><span class="line">/dev/sda1        <span class="number">13</span>G  <span class="number">6.4</span>G  <span class="number">5.6</span>G  <span class="number">54</span>% /vagrant</span><br><span class="line">shm              <span class="number">68</span>M     <span class="number">0</span>   <span class="number">68</span>M   <span class="number">0</span>% /dev/shm</span><br></pre></td></tr></table></figure></p>
<p>Next I need to create six configuration files. Below is an outline of where they live within the ~/presto-server-0.133/etc folder:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ tree ~/presto-server-<span class="number">0.133</span>/etc</span><br><span class="line">etc</span><br><span class="line">|-- catalog</span><br><span class="line">|   |-- hive.properties</span><br><span class="line">|   `-- jmx.properties</span><br><span class="line">|-- config.properties</span><br><span class="line">|-- jvm.config</span><br><span class="line">|-- log.properties</span><br><span class="line">`-- node.properties</span><br></pre></td></tr></table></figure></p>
<p>Here are the commands to set the contents on each of the configuration files.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/presto-server-<span class="number">0.133</span>/etc/config.properties</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">coordinator=<span class="literal">true</span></span><br><span class="line">node-scheduler.include-coordinator=<span class="literal">true</span></span><br><span class="line">http-server.http.port=<span class="number">8080</span></span><br><span class="line">query.max-memory=<span class="number">800</span>MB</span><br><span class="line">query.max-memory-per-node=<span class="number">200</span>MB</span><br><span class="line">discovery-server.enabled=<span class="literal">true</span></span><br><span class="line">discovery.uri=http://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">8080</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/presto-server-<span class="number">0.133</span>/etc/jvm.config</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-server</span><br><span class="line">-Xmx800M</span><br><span class="line">-XX:+UseG1GC</span><br><span class="line">-XX:G1HeapRegionSize=<span class="number">32</span>M</span><br><span class="line">-XX:+UseGCOverheadLimit</span><br><span class="line">-XX:+ExplicitGCInvokesConcurrent</span><br><span class="line">-XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line">-XX:OnOutOfMemoryError=<span class="built_in">kill</span> -<span class="number">9</span> %p</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/presto-server-<span class="number">0.133</span>/etc/log.properties</span><br><span class="line">com.facebook.presto=INFO</span><br><span class="line">$ vi ~/presto-server-<span class="number">0.133</span>/etc/node.properties</span><br><span class="line">node.environment=dev</span><br><span class="line">node.id=ffffffff-ffff-ffff-ffff-ffffffffffff</span><br><span class="line">node.data-dir=/root/datap</span><br></pre></td></tr></table></figure>
<p>The JMX connector provides the ability to query Java Management Extensions (JMX) information from all nodes in a Presto cluster. Presto itself is heavily instrumented via JMX.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/presto-server-<span class="number">0.133</span>/etc/catalog/jmx.properties</span><br><span class="line">connector.name=jmx</span><br></pre></td></tr></table></figure></p>
<p>This file allows Presto to know where our Hive Metastore is and which connector to use to communicate with it.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/presto-server-<span class="number">0.133</span>/etc/catalog/hive.properties</span><br><span class="line">hive.metastore.uri=thrift://bigtop1.docker:<span class="number">9083</span></span><br><span class="line">connector.name=hive-hadoop2</span><br></pre></td></tr></table></figure></p>
<p>With those in place you can launch Presto’s server.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ~/presto-server-<span class="number">0.133</span>/bin/launcher start</span><br></pre></td></tr></table></figure></p>
<p>It should expose a web frontend on port 8080 if it’s running properly.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl localhost:<span class="number">8080</span></span><br></pre></td></tr></table></figure></p>
<h2 id="Presto_u2019s_CLI_Up_and_Running"><a href="#Presto_u2019s_CLI_Up_and_Running" class="headerlink" title="Presto’s CLI Up and Running"></a>Presto’s CLI Up and Running</h2><p>I’ll download the CLI JAR file for Presto, rename it to presto and then I can use it to connect to the Presto Server.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~</span><br><span class="line">$ wget -c https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/<span class="number">0.133</span>/presto-cli-<span class="number">0.133</span>-executable.jar</span><br><span class="line">$ mv presto-cli-<span class="number">0.133</span>-executable.jar presto</span><br><span class="line">$ chmod +x presto</span><br><span class="line">$ ./presto --server localhost:<span class="number">8080</span> --catalog hive --schema default</span><br></pre></td></tr></table></figure></p>
<p>I’ll then see if Presto can see the schemas in the Hive Metastore.</p>
<p>show schemas from hive;</p>
<pre><code>Schema
</code></pre><hr>
<p> default<br> information_schema<br> metastore_db<br>(3 rows)</p>
<h2 id="Executing_Queries_in_Presto"><a href="#Executing_Queries_in_Presto" class="headerlink" title="Executing Queries in Presto"></a>Executing Queries in Presto</h2><p>With the CLI communicating with the server properly I’ll run two queries. The first will count how many records per year exist in our million song database using the data in the CSV-backed table and the second will do the same against the Parquet-backed table.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator"><span class="keyword">SELECT</span> <span class="keyword">year</span>, <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> track_metadata_csv</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">year</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">year</span>;</span></span><br><span class="line"> year | _col1</span><br><span class="line"><span class="comment">------+--------</span></span><br><span class="line">    0 | 968848</span><br><span class="line"> 1922 |     12</span><br><span class="line"> 1924 |     10</span><br><span class="line"> ...</span><br><span class="line"> 2008 |  69540</span><br><span class="line"> 2009 |  62102</span><br><span class="line"> 2010 |  18794</span><br><span class="line"> 2011 |      2</span><br><span class="line">(90 rows)</span><br><span class="line"></span><br><span class="line">Query 20160131_160009_00002_s4z65, FINISHED, 1 node</span><br><span class="line">Splits: 6 total, 6 done (100.00%)</span><br><span class="line">0:09 [2M rows, 154MB] [221K rows/s, 17MB/s]</span><br><span class="line"><span class="operator"><span class="keyword">SELECT</span> <span class="keyword">year</span>, <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> track_metadata_parquet</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">year</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">year</span>;</span></span><br><span class="line"> year | _col1</span><br><span class="line"><span class="comment">------+--------</span></span><br><span class="line">    0 | 968848</span><br><span class="line"> 1922 |     12</span><br><span class="line"> 1924 |     10</span><br><span class="line"> ...</span><br><span class="line"> 2008 |  69540</span><br><span class="line"> 2009 |  62102</span><br><span class="line"> 2010 |  18794</span><br><span class="line"> 2011 |      2</span><br><span class="line">(90 rows)</span><br><span class="line"></span><br><span class="line">Query 20160131_160411_00005_s4z65, FINISHED, 1 node</span><br><span class="line">Splits: 5 total, 5 done (100.00%)</span><br><span class="line">0:04 [3M rows, 87MB] [799K rows/s, 23.2MB/s]</span><br></pre></td></tr></table></figure></p>
<p>The query finished in 9 seconds with the CSV-formatted data and in 4 seconds with the Parquet-formatted data. I grepped the server log to see the time lines recorded by the Query Monitor.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ grep <span class="number">20160131</span>_160009_00002_s4z65 ~/datap/var/<span class="built_in">log</span>/server.log</span><br><span class="line">...elapsed <span class="number">3886.00</span>ms :: planning <span class="number">644.26</span>ms :: scheduling <span class="number">1099.00</span>ms :: running <span class="number">2094.00</span>ms :: finishing <span class="number">49.00</span>ms...</span><br><span class="line"></span><br><span class="line">$ grep <span class="number">20160131</span>_160411_00005_s4z65 ~/datap/var/<span class="built_in">log</span>/server.log</span><br><span class="line">...elapsed <span class="number">1406.00</span>ms :: planning <span class="number">91.68</span>ms :: scheduling <span class="number">55.00</span>ms :: running <span class="number">1221.00</span>ms :: finishing <span class="number">38.00</span>ms...</span><br></pre></td></tr></table></figure></p>
<p>I’m not surprised that the columnar-based, Parquet-format-backed query was twice as fast but I’m not sure how the timings the Query Monitor recorded relate to the times reported by the CLI. It gives me something to dig into at a later point.</p>
<p>I took a look at the query plans to see if they offer anything of interest. In the case of these two queries they were identical bar the table names.<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN</span><br><span class="line">SELECT year, count(*)</span><br><span class="line">FROM track_metadata_csv</span><br><span class="line">GROUP BY year</span><br><span class="line">ORDER BY year;</span><br><span class="line">                                                                  </span><br><span class="line">----------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> - Output[year, _col1] =&gt; [year:bigint, count:bigint]</span><br><span class="line">         _col1 := count</span><br><span class="line">     - Sort[year ASC_NULLS_LAST] =&gt; [year:bigint, count:bigint]</span><br><span class="line">         - Exchange[GATHER] =&gt; year:bigint, count:bigint</span><br><span class="line">             - Aggregate(FINAL)[year] =&gt; [year:bigint, count:bigint]</span><br><span class="line">                     count := "count"("count_9")</span><br><span class="line">                 - Exchange[REPARTITION] =&gt; year:bigint, count_9:bigint</span><br><span class="line">                     - Aggregate(PARTIAL)[year] =&gt; [year:bigint, count_9:bigint]</span><br><span class="line">                             count_9 := "count"(*)</span><br><span class="line">                         - TableScan[hive:hive:default:track_metadata_csv, originalConstraint = true] =&gt; [year:bigint]</span><br><span class="line">                                 LAYOUT: hive</span><br><span class="line">                                 year := HiveColumnHandle&#123;clientId=hive, name=year, hiveType=smallint, hiveColumnIndex=5, partitionKey=false&#125;</span><br><span class="line">EXPLAIN</span><br><span class="line">SELECT year, count(*)</span><br><span class="line">FROM track_metadata_parquet</span><br><span class="line">GROUP BY year</span><br><span class="line">ORDER BY year;</span><br><span class="line">                                                                  Query Plan</span><br><span class="line">----------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> - Output[year, _col1] =&gt; [year:bigint, count:bigint]</span><br><span class="line">         _col1 := count</span><br><span class="line">     - Sort[year ASC_NULLS_LAST] =&gt; [year:bigint, count:bigint]</span><br><span class="line">         - Exchange[GATHER] =&gt; year:bigint, count:bigint</span><br><span class="line">             - Aggregate(FINAL)[year] =&gt; [year:bigint, count:bigint]</span><br><span class="line">                     count := "count"("count_9")</span><br><span class="line">                 - Exchange[REPARTITION] =&gt; year:bigint, count_9:bigint</span><br><span class="line">                     - Aggregate(PARTIAL)[year] =&gt; [year:bigint, count_9:bigint]</span><br><span class="line">                             count_9 := "count"(*)</span><br><span class="line">                         - TableScan[hive:hive:default:track_metadata_parquet, originalConstraint = true] =&gt; [year:bigint]</span><br><span class="line">                                 LAYOUT: hive</span><br><span class="line">                                 year := HiveColumnHandle&#123;clientId=hive, name=year, hiveType=smallint, hiveColumnIndex=5, partitionKey=false&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="Airpal_3A_A_Web_Interface_for_Presto"><a href="#Airpal_3A_A_Web_Interface_for_Presto" class="headerlink" title="Airpal: A Web Interface for Presto"></a>Airpal: A Web Interface for Presto</h2><p>In March 2015 AirBNB announced Airpal, a web-based query tool that works with Presto. Beyond a visual interface to run queries it offers access controls, metadata exploration, query progress tracking and CSV exporting of results.</p>
<p>These are the steps I took to install this software and launch it within the same Docker container as Presto.</p>
<p>The first thing I had to do was to get a copy of Airpal’s git repository and build the Airpal JAR file.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install \</span><br><span class="line">    build-essential \</span><br><span class="line">    git \</span><br><span class="line">    gradle \</span><br><span class="line">    mysql-server</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/airbnb/airpal.git</span><br><span class="line">$ <span class="built_in">cd</span> airpal</span><br><span class="line">$ ./gradlew clean shadowJar</span><br></pre></td></tr></table></figure></p>
<p>I then created a MySQL database for Airpal to store it’s configuration and logs.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ mysql \</span><br><span class="line">    -uroot \</span><br><span class="line">    -proot \</span><br><span class="line">    <span class="operator">-e</span><span class="string">'CREATE DATABASE `airpal` CHARACTER SET utf8 COLLATE utf8_general_ci;'</span></span><br></pre></td></tr></table></figure></p>
<p>Airpal needs a configuration file to tell it’s JAR file how to, among other things, connect with the MySQL data backend.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi reference.yml</span><br></pre></td></tr></table></figure></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># Logging settings</span><br><span class="line">logging:</span><br><span class="line"></span><br><span class="line">  loggers:</span><br><span class="line">    org.apache.shiro: INFO</span><br><span class="line"></span><br><span class="line">  # The default level of all loggers. Can be OFF, ERROR, WARN, INFO, DEBUG, TRACE, or ALL.</span><br><span class="line">  level: INFO</span><br><span class="line"></span><br><span class="line"># HTTP-specific options.</span><br><span class="line">server:</span><br><span class="line">  applicationConnectors:</span><br><span class="line">    - type: http</span><br><span class="line">      port: 8081</span><br><span class="line">      idleTimeout: 10 seconds</span><br><span class="line"></span><br><span class="line">  adminConnectors:</span><br><span class="line">    - type: http</span><br><span class="line">      port: 8082</span><br><span class="line"></span><br><span class="line">shiro:</span><br><span class="line">  iniConfigs: ["classpath:shiro_allow_all.ini"]</span><br><span class="line"></span><br><span class="line">dataSourceFactory:</span><br><span class="line">  driverClass: com.mysql.jdbc.Driver</span><br><span class="line">  user: root</span><br><span class="line">  password: root</span><br><span class="line">  url: jdbc:mysql://127.0.0.1:3306/airpal</span><br><span class="line"></span><br><span class="line"># The URL to the Presto coordinator.</span><br><span class="line">prestoCoordinator: http://127.0.0.1:8080</span><br></pre></td></tr></table></figure>
<p>With that in place I setup Airpal’s database schema and launched the server in the background.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ java -Duser.timezone=UTC \</span><br><span class="line">       -cp build/libs/airpal-*-all.jar \</span><br><span class="line">       com.airbnb.airpal.AirpalApplication \</span><br><span class="line">       db migrate reference.yml</span><br><span class="line">$ java -server \</span><br><span class="line">       -Duser.timezone=UTC \</span><br><span class="line">       -cp build/libs/airpal-*-all.jar \</span><br><span class="line">       com.airbnb.airpal.AirpalApplication \</span><br><span class="line">       server reference.yml &amp;</span><br></pre></td></tr></table></figure></p>
<p>Their is a health check endpoint on port 8082.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ curl <span class="operator">-s</span> http://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">8082</span>/healthcheck | \</span><br><span class="line">    python -m json.tool</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"deadlocks"</span>: &#123;</span><br><span class="line">        <span class="string">"healthy"</span>: <span class="literal">true</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"mysql"</span>: &#123;</span><br><span class="line">        <span class="string">"healthy"</span>: <span class="literal">true</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"presto"</span>: &#123;</span><br><span class="line">        <span class="string">"healthy"</span>: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>If that all looks well the web interface is configured to be exposed on port 8081.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://<span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">8081</span>/app</span><br></pre></td></tr></table></figure></p>
<p>Thank you for taking the time to read this post. If you’re considering using Digital Ocean, the hosting provider this blog is hosted on, please consider using this link to sign up.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>I came across a blog post from Brandon Harris recently where he discussed a credit card fraud detection project he’d been working on with]]>
    </summary>
    
      <category term="parquet" scheme="http://blog.djstudy.net/tags/parquet/"/>
    
      <category term="presto" scheme="http://blog.djstudy.net/tags/presto/"/>
    
      <category term="big data" scheme="http://blog.djstudy.net/categories/big-data/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark Sort Based Shuffle内存分析]]></title>
    <link href="http://blog.djstudy.net/2016/01/27/spark-sort-shuffle-analyse/"/>
    <id>http://blog.djstudy.net/2016/01/27/spark-sort-shuffle-analyse/</id>
    <published>2016-01-27T12:00:22.000Z</published>
    <updated>2016-01-27T12:00:22.000Z</updated>
    <content type="html"><![CDATA[<h2 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h2><p>借用和董神的一段对话说下背景：</p>
<p>shuffle共有三种，别人讨论的是hash shuffle，这是最原始的实现，曾经有两个版本，第一版是每个map产生r个文件，一共产生mr个文件，由于产生的中间文件太大影响扩展性，社区提出了第二个优化版本，让一个core上map共用文件，减少文件数目，这样共产生corer个文件，好多了，但中间文件数目仍随任务数线性增加，仍难以应对大作业，但hash shuffle已经优化到头了。为了解决hash shuffle性能差的问题，又引入sort shuffle，完全借鉴mapreduce实现，每个map产生一个文件，彻底解决了扩展性问题<br>目前Sort Based Shuffle 是作为默认Shuffle类型的。Shuffle 是一个很复杂的过程，任何一个环节都足够写一篇文章。所以这里，我尝试换个方式，从实用的角度出发，让读者有两方面的收获：</p>
<p>剖析哪些环节，哪些代码可能会让内存产生问题<br>控制相关内存的参数<br>有时候，我们宁可程序慢点，也不要OOM，至少要先跑步起来，希望这篇文章能够让你达成这个目标。</p>
<p>同时我们会提及一些类名，这些类方便你自己想更深入了解时，可以方便的找到他们，自己去探个究竟。</p>
<h2 id="Shuffle__u6982_u89C8"><a href="#Shuffle__u6982_u89C8" class="headerlink" title="Shuffle 概览"></a>Shuffle 概览</h2><p>Spark 的Shuffle 分为 Write,Read 两阶段。我们预先建立三个概念：</p>
<p>Write 对应的是ShuffleMapTask,具体的写操作ExternalSorter来负责</p>
<p>Read 阶段由ShuffleRDD里的HashShuffleReader来完成。如果拉来的数据如果过大，需要落地，则也由ExternalSorter来完成的</p>
<p>所有Write 写完后，才会执行Read。 他们被分成了两个不同的Stage阶段。</p>
<p>也就是说，Shuffle Write ,Shuffle Read 两阶段都可能需要落磁盘，并且通过Disk Merge 来完成最后的Sort归并排序。</p>
<h2 id="Shuffle_Write__u5185_u5B58_u6D88_u8017_u5206_u6790"><a href="#Shuffle_Write__u5185_u5B58_u6D88_u8017_u5206_u6790" class="headerlink" title="Shuffle Write 内存消耗分析"></a>Shuffle Write 内存消耗分析</h2><p>Shuffle Write 的入口链路为：</p>
<blockquote>
<p>org.apache.spark.scheduler.ShuffleMapTask<br> —&gt; org.apache.spark.shuffle.sort.SortShuffleWriter<br>  —&gt; org.apache.spark.util.collection.ExternalSorter</p>
</blockquote>
<p>会产生内存瓶颈的其实就是 org.apache.spark.util.collection.ExternalSorter。我们看看这个复杂的ExternalSorter都有哪些地方在占用内存：</p>
<p>第一个地：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> map = <span class="keyword">new</span> <span class="type">PartitionedAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br></pre></td></tr></table></figure>
<p>我们知道，数据都是先写内存，内存不够了，才写磁盘。这里的map就是那个放数据的内存了。</p>
<p>这个PartitionedAppendOnlyMap内部维持了一个数组，是这样的：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> data = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">AnyRef</span>](<span class="number">2</span> * capacity)</span><br></pre></td></tr></table></figure></p>
<p>也就是他消耗的并不是Storage的内存，所谓Storage内存，指的是由blockManager管理起来的内存。</p>
<p>PartitionedAppendOnlyMap 放不下，要落地，那么不能硬生生的写磁盘，所以需要个buffer,然后把buffer再一次性写入磁盘文件。这个buffer是由参数</p>
<p>spark.shuffle.file.buffer=32k<br>控制的。数据获取的过程中，序列化反序列化，也是需要空间的，所以Spark 对数量做了限制，通过如下参数控制：</p>
<p> spark.shuffle.spill.batchSize=10000<br>假设一个Executor的可使用的Core为 C个，那么对应需要的内存消耗为：</p>
<p> C <em> 32k + C </em> 10000个Record + C <em> PartitionedAppendOnlyMap<br>这么看来，写文件的buffer不是问题，而序列化的batchSize也不是问题，几万或者十几万个Record 而已。那C </em> PartitionedAppendOnlyMap 到底会有多大呢？我先给个结论:</p>
<p>   C <em> PartitionedAppendOnlyMap &lt; ExecutorHeapMemeory </em> 0.2 * 0.8<br>怎么得到上面的结论呢？核心店就是要判定PartitionedAppendOnlyMap 需要占用多少内存，而它到底能占用内存，则由触发写磁盘动作决定，因为一旦写磁盘，PartitionedAppendOnlyMap所占有的内存就会被释放。下面是判断是否写磁盘的逻辑代码：</p>
<blockquote>
<p>estimatedSize = map.estimateSize()<br>if (maybeSpill(map, estimatedSize)) {<br>        map = new PartitionedAppendOnlyMap[K, C]<br>}</p>
</blockquote>
<p>每放一条记录，就会做一次内存的检查，看PartitionedAppendOnlyMap 到底占用了多少内存。如果真是这样，假设检查一次内存1ms, 1kw 就不得了的时间了。所以肯定是不行的，所以 estimateSize其实是使用采样算法来做的。</p>
<p>第二个，我们也不希望mayBeSpill太耗时,所以 maybeSpill 方法里就搞了很多东西，减少耗时。我们看看都设置了哪些防线</p>
<p>首先会判定要不要执行内部逻辑：</p>
<p>   elementsRead % 32 == 0 &amp;&amp; currentMemory &gt;= myMemoryThreshold<br>每隔32次会进行一次检查，并且要当前PartitionedAppendOnlyMap currentMemory &gt; myMemoryThreshold 才会进一步判定是不是要spill.</p>
<p>其中 myMemoryThreshold可通过如下配置获得初始值</p>
<p>spark.shuffle.spill.initialMemoryThreshold =  5 <em> 1024 </em> 1024<br>接着会向 shuffleMemoryManager 要 2 * currentMemory - myMemoryThreshold 的内存，shuffleMemoryManager 是被Executor 所有正在运行的Task(Core) 共享的，能够分配出去的内存是：</p>
<p>ExecutorHeapMemeory <em> 0.2 </em> 0.8<br>上面的数字可通过下面两个配置来更改：</p>
<p>spark.shuffle.memoryFraction=0.2<br>spark.shuffle.safetyFraction=0.8<br>如果无法获取到足够的内存，就会触发真的spill操作了。</p>
<p>看到这里，上面的结论就显而易见了。</p>
<p>然而，这里我们忽略了一个很大的问题，就是</p>
<p> estimatedSize = map.estimateSize()<br>为什么说它是大问题，前面我们说了，estimateSize 是近似估计，所以有可能估的不准，也就是实际内存会远远超过预期。</p>
<p>具体的大家可以看看 org.apache.spark.util.collection.SizeTracker</p>
<p>我这里给出一个结论：</p>
<p>如果你内存开的比较大，其实反倒风险更高，因为estimateSize 并不是每次都去真实的算缓存。它是通过采样来完成的，而采样的周期不是固定的，而是指数增长的，比如第一次采样完后，PartitionedAppendOnlyMap 要经过1.1次的update/insert操作之后才进行第二次采样，然后经过1.1*.1.1次之后进行第三次采样，以此递推，假设你内存开的大，那PartitionedAppendOnlyMap可能要经过几十万次更新之后之后才会进行一次采样，然后才能计算出新的大小，这个时候几十万次更新带来的新的内存压力，可能已经让你的GC不堪重负了。</p>
<p>当然，这是一种折中，因为确实不能频繁采样。</p>
<p>如果你不想出现这种问题，要么自己替换实现这个类，要么将</p>
<p>spark.shuffle.safetyFraction=0.8<br>设置的更小一些。</p>
<p>Shuffle Read 内存消耗分析<br>Shuffle Read 的入口链路为：</p>
<p>org.apache.spark.rdd.ShuffledRDD<br>—&gt; org.apache.spark.shuffle.sort.HashShuffleReader<br>   —&gt;  org.apache.spark.util.collection.ExternalAppendOnlyMap<br>   —&gt;  org.apache.spark.util.collection.ExternalSorter<br>Shuffle Read 会更复杂些，尤其是从各个节点拉取数据。但这块不是不是我们的重点。按流程，主要有：</p>
<p>获取待拉取数据的迭代器<br>使用AppendOnlyMap/ExternalAppendOnlyMap 做combine<br>如果需要对key排序，则使用ExternalSorter<br>其中1后续会单独列出文章。3我们在write阶段已经讨论过。所以这里重点是第二个步骤，combine阶段。</p>
<p>如果你开启了</p>
<p>spark.shuffle.spill=true<br>则使用ExternalAppendOnlyMap，否则使用AppendOnlyMap。两者的区别是，前者如果内存不够，则落磁盘，会发生spill操作，后者如果内存不够，直接OOM了。</p>
<p>这里我们会重点分析ExternalAppendOnlyMap。</p>
<p>ExternalAppendOnlyMap 作为内存缓冲数据的对象如下：</p>
<p> private var currentMap = new SizeTrackingAppendOnlyMap[K, C]<br>如果currentMap 对象向申请不到内存，就会触发spill动作。判定内存是否充足的逻辑和Shuffle Write 完全一致。</p>
<p>Combine做完之后，ExternalAppendOnlyMap 会返回一个Iterator，叫做ExternalIterator,这个Iterator背后的数据源是所有spill文件以及当前currentMap里的数据。</p>
<p>我们进去 ExternalIterator 看看，唯一的一个占用内存的对象是这个优先队列：</p>
<p>   private val mergeHeap = new mutable.PriorityQueue[StreamBuffer]<br>mergeHeap 里元素数量等于所有spill文件个数加一。StreamBuffer 的结构：</p>
<p> private class StreamBuffer(<br>                    val iterator: BufferedIterator[(K, C)],<br>                    val pairs: ArrayBuffer[(K, C)])<br>其中iterator 只是一个对象引用，pairs 应该保存的是iterator里的第一个元素(如果hash有冲突的话，则为多个)</p>
<p>所以mergeHeap 应该不占用什么内存。到这里我们看看应该占用多少内存。依然假设 CoreNum 为 C,则</p>
<p>  C <em> 32k + C  </em> mergeHeap  + C * SizeTrackingAppendOnlyMap<br>所以这一段占用内存较大的依然是 SizeTrackingAppendOnlyMap ，一样的，他的值也符合如下公式</p>
<p> C <em> SizeTrackingAppendOnlyMap &lt; ExecutorHeapMemeory </em> 0.2 * 0.8<br>ExternalAppendOnlyMap 的目的是做Combine,然后如果你还设置了Order,那么接着会启用 ExternalSorter 来完成排序。</p>
<p>经过上文对Shuffle Write的使用，相比大家也对ExternalSorter有一定的了解了，此时应该占用内存的地方最大不超过下面的这个值：</p>
<p> C <em> SizeTrackingAppendOnlyMap  + C </em> PartitionedAppendOnlyMap<br>不过即使如此，因为他们共享一个shuffleMemoryManager，则理论上只有这么大：</p>
<p> C <em> SizeTrackingAppendOnlyMap &lt;  ExecutorHeapMemeory </em> 0.2 * 0.8<br>分析到这里，我们可以做个总结：</p>
<p>Shuffle Read阶段如果内存不足，有两个阶段会落磁盘，分别是Combine 和 Sort 阶段。对应的都会spill小文件，并且产生读。<br>Shuffle Read 阶段如果开启了spill功能，则基本能保证内存控制在 ExecutorHeapMemeory <em> 0.2 </em> 0.8 之内。<br>后话<br>如果大家对Sort Shuffle 落磁盘文件这块感兴趣，还可以看看这篇文章 <a href="http://www.jianshu.com/p/2d837bf2dab6" target="_blank" rel="external">Spark Shuffle Write阶段磁盘文件分析</a><br>转载 <a href="http://www.jianshu.com/p/c83bb237caa8" target="_blank" rel="external">http://www.jianshu.com/p/c83bb237caa8</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h2><p>借用和董神的一段对话说下背景：</p>
<p>shuffle共有三种，别人讨论的是hash shuffl]]>
    </summary>
    
      <category term="shuffle" scheme="http://blog.djstudy.net/tags/shuffle/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[scala入门笔记]]></title>
    <link href="http://blog.djstudy.net/2016/01/27/scala-rumen-biji/"/>
    <id>http://blog.djstudy.net/2016/01/27/scala-rumen-biji/</id>
    <published>2016-01-27T10:39:15.000Z</published>
    <updated>2016-01-27T10:39:15.000Z</updated>
    <content type="html"><![CDATA[<h3 id="Scala_u7B80_u4ECB"><a href="#Scala_u7B80_u4ECB" class="headerlink" title="Scala简介"></a>Scala简介</h3><p>Scala 是一门多范式的编程语言, 由Martin Odersky 于2001年基于Funnel的工作开始设计Scala并于2004年正式发布<br>Scala是一种纯面向对象的语言，每个值都是对象<br>Scala是一门多范式编程语言, 支持命令交互式, 函数式, 面向对象<br>编译型高性能语言(静态)<br>与Java无缝兼容, 可以使用任何Java库</p>
<h3 id="u4EE3_u7801_u98CE_u683C"><a href="#u4EE3_u7801_u98CE_u683C" class="headerlink" title="代码风格"></a>代码风格</h3><ol>
<li>函数和变量以小驼峰命名</li>
<li>类和特质以大驼峰命名</li>
<li>常量使用全大写命名</li>
<li>一般使用两格缩进</li>
<li>Scala大部分情况可以忽略语句末尾的分号<h3 id="Scala_u53D8_u91CF"><a href="#Scala_u53D8_u91CF" class="headerlink" title="Scala变量"></a>Scala变量</h3>Scala中尽量避免使用变量, 函数式编程的一个重要特性是不可变性(不可变变量没有副作用)<br>Scala是静态类型语言, 但是不需要显式的指明变量类型, Scala采用类型推断(Type Inference)<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义一个变量</span></span><br><span class="line"><span class="keyword">var</span> x = <span class="number">0</span></span><br><span class="line"><span class="comment">// 定义</span></span><br><span class="line"><span class="keyword">val</span> y = <span class="number">1</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Scala_u57FA_u672C_u7C7B_u578B_u548C_u64CD_u4F5C"><a href="#Scala_u57FA_u672C_u7C7B_u578B_u548C_u64CD_u4F5C" class="headerlink" title="Scala基本类型和操作"></a>Scala基本类型和操作</h3><p>String和值类型Byte, Short, Int, Long, Float, Double, Char, Boolean</p>
<ul>
<li>Scala的操作符不是特殊的语言语法, 任何方法都可以是操作符</li>
<li>操作符分为前缀, 中缀, 后缀</li>
<li>Scala中所有操作符都是方法调用<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 前缀</span><br><span class="line">scala&gt; (<span class="number">2.0</span>).unary_-</span><br><span class="line">res1: <span class="type">Double</span> = -<span class="number">2.0</span></span><br><span class="line"># 中缀</span><br><span class="line">scala&gt; x indexOf 'o'</span><br><span class="line">res0: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"># 后缀</span><br><span class="line">scala&gt; <span class="keyword">val</span> x = <span class="string">"Hello, World"</span></span><br><span class="line">x: <span class="type">String</span> = <span class="type">Hello</span>, <span class="type">World</span></span><br><span class="line">scala&gt; x.toLowerCase</span><br><span class="line">res0: <span class="type">String</span> = hello, world</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>中缀操作符的两个操作数, 一个在左一个在右<br>前缀操作符方法名在操作符上加了unary_前缀(+, -, !, ~)<br>后缀操作符是不用点或括号调用的不带任何参数的方法<br>算术操作符: +, -, *, /, %<br>关系, 逻辑和位操作: &gt;, &lt;, &gt;=, &lt;=, ==, !=, &amp;&amp;, ||, &amp;, |, ^, ~(反码)<br>位移操作: &lt;&lt;, &gt;&gt;, &gt;&gt;&gt;(无符号右移)</p>
<h3 id="Scala_u51FD_u6570"><a href="#Scala_u51FD_u6570" class="headerlink" title="Scala函数"></a>Scala函数</h3><p>函数式语言的一个主要特征是, 函数是第一类结构<br>函数定义如下图:<br><img src="http://ww4.sinaimg.cn/large/ab508d3djw1eznwrn1zjcj20iv0d6taj.jpg" alt="scala function" title="scala 函数"><br>Unit 的结果类型指的是函数没有返回有用的值</p>
<h3 id="u51FD_u6570_u5F0F_u5BF9_u8C61"><a href="#u51FD_u6570_u5F0F_u5BF9_u8C61" class="headerlink" title="函数式对象"></a>函数式对象</h3><p>object和class的区别在于: object关键字创建一个单例对象<br>主构造器是类的唯一入口, 只有主构造器可以调用超类构造器<br>override关键字用于在重载父类的非抽象成员和成员函数<br>同一个类内函数名相同而参数类型和个数不同的函数重载不需要override<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rational</span> (</span>n: <span class="type">Int</span>, d: <span class="type">Int</span>) &#123;</span><br><span class="line">  <span class="comment">//precondition</span></span><br><span class="line">  require(d != <span class="number">0</span>)</span><br><span class="line">  <span class="comment">// 私有成员</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> g = gcd(n.abs, d.abs)</span><br><span class="line">  <span class="keyword">var</span> numer: <span class="type">Int</span> = n / g</span><br><span class="line">  <span class="keyword">var</span> denom: <span class="type">Int</span> = d / g</span><br><span class="line">  <span class="comment">// auxiliary constructor, 相当于python中__init__构造函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span>(</span>n: <span class="type">Int</span>) = <span class="keyword">this</span>(n, <span class="number">1</span>)</span><br><span class="line">  <span class="comment">// 函数重载</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span> =</span> n + <span class="string">"/"</span> + d;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">add</span>(</span>other: <span class="type">Rational</span>): <span class="type">Rational</span> =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Rational</span>(numer * other.denom + other.numer * denom, denom * other.denom)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">-</span>(</span>other: <span class="type">Rational</span>): <span class="type">Rational</span> =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Rational</span>(numer * other.denom - other.numer * denom, denom * other.denom)</span><br><span class="line">  <span class="comment">// 函数重载</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">-</span>(</span>i: <span class="type">Int</span>): <span class="type">Rational</span> =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Rational</span>(numer - i * denom, denom)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">*</span>(</span>other: <span class="type">Rational</span>): <span class="type">Rational</span> =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Rational</span>(numer * other.numer, denom * other.denom)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">lessThan</span>(</span>other: <span class="type">Rational</span>): <span class="type">Boolean</span> =</span><br><span class="line">    <span class="keyword">this</span>.numer * other.denom &lt; other.numer * <span class="keyword">this</span>.denom</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">max</span>(</span>other: <span class="type">Rational</span>): <span class="type">Rational</span> =</span><br><span class="line">    <span class="keyword">if</span> (lessThan(other)) other <span class="keyword">else</span> <span class="keyword">this</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">gcd</span>(</span>a: <span class="type">Int</span>, b: <span class="type">Int</span>): <span class="type">Int</span> =</span><br><span class="line">    <span class="keyword">if</span> (b == <span class="number">0</span>) a <span class="keyword">else</span> gcd(b, a % b)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> x = <span class="keyword">new</span> <span class="type">Rational</span>(<span class="number">1</span>, <span class="number">3</span>);</span><br><span class="line"><span class="keyword">var</span> y = <span class="keyword">new</span> <span class="type">Rational</span>(<span class="number">5</span>, <span class="number">7</span>);</span><br><span class="line">println(x add y)</span><br><span class="line">println(x * y)</span><br><span class="line">println(x - <span class="number">1</span>)</span><br><span class="line"><span class="comment">// 隐式转换, 放在解释器方位内</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">intToRational</span>(</span>x: <span class="type">Int</span>) = <span class="keyword">new</span> <span class="type">Rational</span>(x)</span><br><span class="line">println(<span class="number">1</span> - x)</span><br></pre></td></tr></table></figure></p>
<h3 id="u7EE7_u627F_u548C_u591A_u6001"><a href="#u7EE7_u627F_u548C_u591A_u6001" class="headerlink" title="继承和多态"></a>继承和多态</h3><p>继承<br>多态和动态绑定特性<br>动态绑定的特性即父类指针可以指向子类对象, 通过父类指针调用成员方法时, 会查找实际所指向的对象, 然后调用对象的内的对应方法<br><img src="http://ww4.sinaimg.cn/large/ab508d3djw1ezyaprbvcmj20qo0ecmyq.jpg" alt="继承和多态"><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> el: <span class="type">Element</span> = <span class="keyword">new</span> <span class="type">ArrayElement</span>(<span class="type">Array</span>(<span class="string">"hello"</span>))</span><br><span class="line"><span class="keyword">val</span> e2: <span class="type">ArrayElement</span> = <span class="keyword">new</span> <span class="type">LineElement</span>(<span class="string">"hello"</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="u5185_u5EFA_u63A7_u5236_u7ED3_u6784"><a href="#u5185_u5EFA_u63A7_u5236_u7ED3_u6784" class="headerlink" title="内建控制结构"></a>内建控制结构</h3><p>表示式会产生一个值<br>Scala中if是能返回值的表达式, Scala中没有三元操作符, 但通过if (condition) var1 else var2 可以实现三元操作符的功能<br>while和do-while被称为循环, 不产生有意义的结果<br>Scala中for语句非常强大, for {子句} yield {循环体}<br>match表达式可以产生值, match远强大与其他语言中的switch, 而且不需要显示的声明break<br>变量范围: 大括号引入了一个新的范围, 内部变量会遮盖同名的外部变量<br>占位符语法<br>函数文本(匿名函数, 类似于python中的lamda)<br><img src="http://ww3.sinaimg.cn/large/ab508d3djw1eznwvjaswlj20dv05bgmc.jpg" alt="scala函数文本语法"><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> filename = <span class="keyword">if</span> (!args.isEmpty) args(<span class="number">0</span>) <span class="keyword">else</span> <span class="string">"default.txt"</span></span><br><span class="line"><span class="keyword">var</span> filesList = (<span class="keyword">new</span> <span class="type">File</span>(<span class="string">"."</span>)).listFiles</span><br><span class="line"><span class="comment">// i &lt;- 1 to 4(包含4), i &lt;- 1 until 4 (不包含4)</span></span><br><span class="line"><span class="keyword">for</span> (file &lt;- filesList</span><br><span class="line">     <span class="keyword">if</span> file.isFile;</span><br><span class="line">     <span class="keyword">if</span> file.getName.endsWith(<span class="string">".scala"</span>))  <span class="comment">// 过滤器使用分号隔开</span></span><br><span class="line">    println(file)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">var</span> firstArg = <span class="keyword">if</span>(args.length &gt; <span class="number">0</span>) args(<span class="number">0</span>) <span class="keyword">else</span> <span class="string">""</span></span><br><span class="line"><span class="comment">// 任何种类的常量和其他都可以作为case</span></span><br><span class="line">firstArg <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"text"</span> =&gt; println(<span class="string">"text"</span>)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println(<span class="string">"default"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 匿名函数的写法(lambda)</span></span><br><span class="line">scala&gt; <span class="keyword">var</span> someNumbers = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">scala&gt; someNumbers.filter(x =&gt; x % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line"><span class="comment">// 占位符语法</span></span><br><span class="line">scala&gt; someNumbers.filter(_ % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line"><span class="comment">// 偏函数 partial funciton</span></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">sum</span>(</span>a: <span class="type">Int</span>, b: <span class="type">Int</span>, c: <span class="type">Int</span>) = a + b + c</span><br><span class="line">scala&gt; <span class="keyword">val</span> a = sum(<span class="number">1</span>, _: <span class="type">Int</span>, <span class="number">3</span>)</span><br><span class="line">scala&gt; a(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 变长参数(Array[String])</span></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">echo</span>(</span>args: <span class="type">String</span>*) = <span class="keyword">for</span>(arg &lt;- args) println(arg)</span><br><span class="line">scala&gt; echo(<span class="string">"one"</span>)</span><br><span class="line">scala&gt; echo(<span class="string">"one"</span>, <span class="string">"two"</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultConstructor</span> (</span> name:<span class="type">String</span> , age:<span class="type">Int</span>)&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span>(</span>name:<span class="type">String</span>)&#123;</span><br><span class="line">    <span class="comment">/*自定义构造器，必需首先调用默认构造器*/</span></span><br><span class="line">    <span class="keyword">this</span>(name , <span class="number">24</span>) ; </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">show</span>(</span>)&#123;</span><br><span class="line">    println( name + <span class="string">"--&gt;"</span> + age ) ;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>柯里化(carry)<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 普通函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span>(</span>x: <span class="type">Int</span>, y: <span class="type">Int</span>) = x + y</span><br><span class="line"><span class="comment">// 柯里化函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span>(</span>x: <span class="type">Int</span>)(y: <span class="type">Int</span>) = x + y</span><br><span class="line"><span class="comment">// 实际执行</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span>(</span>x: <span class="type">Int</span>) = (y: <span class="type">Int</span>) =&gt; x + y</span><br></pre></td></tr></table></figure></p>
<h3 id="u7279_u8D28_28trait_29"><a href="#u7279_u8D28_28trait_29" class="headerlink" title="特质(trait)"></a>特质(trait)</h3><p>特质就像带有具体方法的java接口<br>特质和抽象类的区别: 抽象类主要用于有明确的父子继承关系的类树, 而特质可以用于任何类<br>特质定义使用trait关键字<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Person</span>(</span>) &#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detail</span>(</span>) &#123;</span><br><span class="line">        println(<span class="string">"I'm angry!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 使用extends或with混入特质, 从特质继承的方法可以像从超类继承的方法使用</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword"><span class="keyword">extends</span></span> <span class="title">Person</span> <span class="keyword"><span class="keyword">with</span></span> <span class="title">Boy</span> &#123;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="u6570_u636E_u7ED3_u6784"><a href="#u6570_u636E_u7ED3_u6784" class="headerlink" title="数据结构"></a>数据结构</h3><p>Python中常用list, tuple, set, dict<br>Scala对应的数据结构为List, Tuple[X], Set, Map(HashMap)<br>Scala中默认为不可变对象, 操作会生成一个新的对象</p>
<h3 id="u5305"><a href="#u5305" class="headerlink" title="包"></a>包</h3><p>Scala采用java平台的包机制<br>使用import来进行引用<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhihu.antispam</span><br></pre></td></tr></table></figure></p>
<h3 id="u53C2_u8003_u94FE_u63A5"><a href="#u53C2_u8003_u94FE_u63A5" class="headerlink" title="参考链接"></a>参考链接</h3><p><a href="http://stackoverflow.com/questions/1755345/difference-between-object-and-class-in-scala" target="_blank" rel="external">Difference between object and class in Scala</a><br><a href="https://www.zybuluo.com/Great-Chinese/note/254972" target="_blank" rel="external">scala 入门</a><br><a href="http://colobu.com/2016/01/04/Scala-magic-functions/?hmsr=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io" target="_blank" rel="external">Scala 魔法函数</a><br><a href="http://blog.yunglinho.com/blog/2012/04/22/dependency-injection-in-scala/" target="_blank" rel="external">Dependency Injection in Scala</a><br><a href="http://www.hawstein.com/posts/databricks-scala-guide.html" target="_blank" rel="external">Databricks Scala 编程风格指南</a><br><a href="http://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala" target="_blank" rel="external">What are all the uses of an underscore in Scala?</a></p>
<p>转载 <a href="http://andrewliu.in/2016/01/16/Scala%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/" target="_blank" rel="external">http://andrewliu.in/2016/01/16/Scala%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="Scala_u7B80_u4ECB"><a href="#Scala_u7B80_u4ECB" class="headerlink" title="Scala简介"></a>Scala简介</h3><p>Scala 是一门多范式的编程语言, 由Martin Ode]]>
    </summary>
    
      <category term="scala" scheme="http://blog.djstudy.net/tags/scala/"/>
    
      <category term="scala" scheme="http://blog.djstudy.net/categories/scala/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[将 Spark 中的文本转换为 Parquet 以提升性能]]></title>
    <link href="http://blog.djstudy.net/2016/01/24/spark-convert-text-parquet/"/>
    <id>http://blog.djstudy.net/2016/01/24/spark-convert-text-parquet/</id>
    <published>2016-01-24T11:03:24.000Z</published>
    <updated>2016-01-24T11:03:24.000Z</updated>
    <content type="html"><![CDATA[<p>列式存储布局（比如 Parquet）可以加速查询，因为它只检查所有需要的列并对它们的值执行计算，因此只读取一个数据文件或表的小部分数据。Parquet 还支持灵活的压缩选项，因此可以显著减少磁盘上的存储。<br>如果您在 HDFS 上拥有基于文本的数据文件或表，而且正在使用 Spark SQL 对它们执行查询，那么强烈推荐将文本数据文件转换为 Parquet 数据文件，以实现性能和存储收益。当然，转换需要时间，但查询性能的提升在某些情况下可能达到 30 倍或更高，存储的节省可高达 75%！<br>已有文章介绍使用 Parquet 存储为 BigSQL、Hive 和 Impala 带来类似的性能收益，本文将介绍如何编写一个简单的 Scala 应用程序，将现有的基于文本的数据文件或表转换为 Parquet 数据文件，还将展示给 Spark SQL 带来的实际存储节省和查询性能提升。</p>
<h3 id="u8BA9_u6211_u4EEC_u8F6C_u6362_u4E3A_Parquet__u5427_uFF01"><a href="#u8BA9_u6211_u4EEC_u8F6C_u6362_u4E3A_Parquet__u5427_uFF01" class="headerlink" title="让我们转换为 Parquet 吧！"></a>让我们转换为 Parquet 吧！</h3><p>Spark SQL 提供了对读取和写入 Parquet 文件的支持，能够自动保留原始数据的模式。Parquet 模式通过 Data Frame API，使数据文件对 Spark SQL 应用程序 “不言自明”。当然，Spark SQL 还支持读取已存储为 Parquet 的现有 Hive 表，但您需要配置 Spark，以便使用 Hive 的元存储来加载所有信息。在我们的示例中，不涉及 Hive 元存储。<br>以下 Scala 代码示例将读取一个基于文本的 CSV 表，并将它写入 Parquet 表：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert</span>(</span>sqlContext: <span class="type">SQLContext</span>, filename: <span class="type">String</span>, schema: <span class="type">StructType</span>, tablename: <span class="type">String</span>) &#123;</span><br><span class="line">     <span class="comment">// import text-based table first into a data frame</span></span><br><span class="line">     <span class="keyword">val</span> df = sqlContext.read.format(<span class="string">"com.databricks.spark.csv"</span>).</span><br><span class="line">       schema(schema).option(<span class="string">"delimiter"</span>, <span class="string">"|"</span>).load(filename)</span><br><span class="line">     <span class="comment">// now simply write to a parquet file</span></span><br><span class="line">     df.write.parquet(<span class="string">"/user/spark/data/parquet/"</span>+tablename)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// usage exampe -- a tpc-ds table called catalog_page</span></span><br><span class="line"> schema= <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_catalog_page_sk"</span>,        <span class="type">IntegerType</span>,<span class="literal">false</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_catalog_page_id"</span>,        <span class="type">StringType</span>,<span class="literal">false</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_start_date_sk"</span>,          <span class="type">IntegerType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_end_date_sk"</span>,            <span class="type">IntegerType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_department"</span>,             <span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_catalog_number"</span>,         <span class="type">LongType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_catalog_page_number"</span>,    <span class="type">LongType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_description"</span>,            <span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_type"</span>,                   <span class="type">StringType</span>,<span class="literal">true</span>)))</span><br><span class="line"> convert(sqlContext,</span><br><span class="line">         hadoopdsPath+<span class="string">"/catalog_page/*"</span>,</span><br><span class="line">         schema,</span><br><span class="line">         <span class="string">"catalog_page"</span>)</span><br></pre></td></tr></table></figure></p>
<p>上面的代码将会读取 hadoopdsPath+”/catalog_page/* 中基于文本的 CSV 文件，并将转换的 Parquet 文件保存在 /user/spark/data/parquet/ 下。此外，转换的 Parquet 文件会在 gzip 中自动压缩，因为 Spark 变量 spark.sql.parquet.compression.codec 已在默认情况下设置为 gzip。您还可以将压缩编解码器设置为 uncompressed、snappy 或 lzo。</p>
<h3 id="u8F6C_u6362_1_TB__u6570_u636E_u5C06_u82B1_u8D39_u591A_u957F_u65F6_u95F4_uFF1F"><a href="#u8F6C_u6362_1_TB__u6570_u636E_u5C06_u82B1_u8D39_u591A_u957F_u65F6_u95F4_uFF1F" class="headerlink" title="转换 1 TB 数据将花费多长时间？"></a>转换 1 TB 数据将花费多长时间？</h3><p>50 分钟，在一个 6 数据节点的 Spark v1.5.1 集群上可达到约 20 GB/分的吞吐量。使用的总内存约为 500GB。HDFS 上最终的 Parquet 文件的格式为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/user/spark/data/parquet/catalog_page/part-r-<span class="number">00000</span>-<span class="number">9</span>ff58e65-<span class="number">0674</span>-<span class="number">440</span>a-<span class="number">883</span>d-<span class="number">256370</span>f33c66.gz.parquet</span><br><span class="line">/user/spark/data/parquet/catalog_page/part-r-<span class="number">00001</span>-<span class="number">9</span>ff58e65-<span class="number">0674</span>-<span class="number">440</span>a-<span class="number">883</span>d-<span class="number">256370</span>f33c66.gz.parquet</span><br></pre></td></tr></table></figure></p>
<h3 id="u5B58_u50A8_u8282_u7701"><a href="#u5B58_u50A8_u8282_u7701" class="headerlink" title="存储节省"></a>存储节省</h3><p>以下 Linux 输出显示了 TEXT 和 PARQUET 在 HDFS 上的大小比较：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -du -h <span class="operator">-s</span> /user/spark/hadoopds1000g</span><br><span class="line">    <span class="number">897.9</span> G  /user/spark/hadoopds1000g</span><br><span class="line">    % hadoop fs -du -h <span class="operator">-s</span> /user/spark/data/parquet</span><br><span class="line">    <span class="number">231.4</span> G  /user/spark/data/parquet</span><br></pre></td></tr></table></figure></p>
<p>1 TB 数据的存储节省了将近 75%！</p>
<h3 id="u67E5_u8BE2_u6027_u80FD_u63D0_u5347"><a href="#u67E5_u8BE2_u6027_u80FD_u63D0_u5347" class="headerlink" title="查询性能提升"></a>查询性能提升</h3><p>Parquet 文件是自描述性的，所以保留了模式。要将 Parquet 文件加载到 DataFrame 中并将它注册为一个 temp 表，可执行以下操作：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = sqlContext.read.parquet(filename)</span><br><span class="line">      df.show</span><br><span class="line">      df.registerTempTable(tablename)</span><br></pre></td></tr></table></figure></p>
<p>要对比性能，然后可以分别对 TEXT 和 PARQUET 表运行以下查询（假设所有其他 tpc-ds 表也都已转换为 Parquet）。您可以利用 spark-sql-perf 测试工具包来执行查询测试。举例而言，现在来看看 TPC-DS 基准测试中的查询 #76，<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">("q76", """</span><br><span class="line">            | <span class="operator"><span class="keyword">SELECT</span></span><br><span class="line">            |    channel, col_name, d_year, d_qoy, i_category, <span class="keyword">COUNT</span>(*) sales_cnt,</span><br><span class="line">            |    <span class="keyword">SUM</span>(ext_sales_price) sales_amt</span><br><span class="line">            | <span class="keyword">FROM</span>(</span><br><span class="line">            |    <span class="keyword">SELECT</span></span><br><span class="line">            |        <span class="string">'store'</span> <span class="keyword">as</span> channel, ss_store_sk col_name, d_year, d_qoy, i_category,</span><br><span class="line">            |        ss_ext_sales_price ext_sales_price</span><br><span class="line">            |    <span class="keyword">FROM</span> store_sales, item, date_dim</span><br><span class="line">            |    <span class="keyword">WHERE</span> ss_store_sk <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line">            |      <span class="keyword">AND</span> ss_sold_date_sk=d_date_sk</span><br><span class="line">            |      <span class="keyword">AND</span> ss_item_sk=i_item_sk</span><br><span class="line">            |    <span class="keyword">UNION</span> ALL</span><br><span class="line">            |    <span class="keyword">SELECT</span></span><br><span class="line">            |        <span class="string">'web'</span> <span class="keyword">as</span> channel, ws_ship_customer_sk col_name, d_year, d_qoy, i_category,</span><br><span class="line">            |        ws_ext_sales_price ext_sales_price</span><br><span class="line">            |    <span class="keyword">FROM</span> web_sales, item, date_dim</span><br><span class="line">            |    <span class="keyword">WHERE</span> ws_ship_customer_sk <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line">            |      <span class="keyword">AND</span> ws_sold_date_sk=d_date_sk</span><br><span class="line">            |      <span class="keyword">AND</span> ws_item_sk=i_item_sk</span><br><span class="line">            |    <span class="keyword">UNION</span> ALL</span><br><span class="line">            |    <span class="keyword">SELECT</span></span><br><span class="line">            |        <span class="string">'catalog'</span> <span class="keyword">as</span> channel, cs_ship_addr_sk col_name, d_year, d_qoy, i_category,</span><br><span class="line">            |        cs_ext_sales_price ext_sales_price</span><br><span class="line">            |    <span class="keyword">FROM</span> catalog_sales, item, date_dim</span><br><span class="line">            |    <span class="keyword">WHERE</span> cs_ship_addr_sk <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line">            |      <span class="keyword">AND</span> cs_sold_date_sk=d_date_sk</span><br><span class="line">            |      <span class="keyword">AND</span> cs_item_sk=i_item_sk) foo</span><br><span class="line">            | <span class="keyword">GROUP</span> <span class="keyword">BY</span> channel, col_name, d_year, d_qoy, i_category</span><br><span class="line">            | <span class="keyword">ORDER</span> <span class="keyword">BY</span> channel, col_name, d_year, d_qoy, i_category</span><br><span class="line">            | <span class="keyword">limit</span> <span class="number">100</span></span></span><br></pre></td></tr></table></figure></p>
<p>查询时间如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TIME               TEXT     PARQUET</span><br><span class="line">Query time (sec)    <span class="number">698</span>          <span class="number">21</span></span><br></pre></td></tr></table></figure></p>
<p>参考资料<br><a href="https://developer.ibm.com/hadoop/blog/2015/12/03/parquet-for-spark-sql/" target="_blank" rel="external">英文原文。</a><br>在 <a href="http://www.ibm.com/developerworks/cn/bigdata/" target="_blank" rel="external">developerWorks 大数据和分析专区</a>，了解关于大数据的更多信息，获取技术文档、how-to 文章、培训、下载、产品信息以及其他资源。<br>加入 <a href="http://www.ibm.com/developerworks/cn/community/" target="_blank" rel="external">developerWorks 中文社区</a>，查看开发人员推动的博客、论坛、组和维基，并与其他 developerWorks 用户交流。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>列式存储布局（比如 Parquet）可以加速查询，因为它只检查所有需要的列并对它们的值执行计算，因此只读取一个数据文件或表的小部分数据。Parquet 还支持灵活的压缩选项，因此可以显著减少磁盘上的存储。<br>如果您在 HDFS 上拥有基于文本的数据文件或表，而且正在使用]]>
    </summary>
    
      <category term="parquet" scheme="http://blog.djstudy.net/tags/parquet/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark Standalone模式HA环境搭建]]></title>
    <link href="http://blog.djstudy.net/2016/01/24/spark-ha/"/>
    <id>http://blog.djstudy.net/2016/01/24/spark-ha/</id>
    <published>2016-01-24T08:13:04.000Z</published>
    <updated>2016-01-24T08:13:04.000Z</updated>
    <content type="html"><![CDATA[<p>Spark Standalone模式常见的HA部署方式有两种：基于文件系统的HA和基于ZK的HA<br>本篇只介绍基于ZK的HA环境搭建：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ SPARK_HOME/conf/spark-env.sh</span><br></pre></td></tr></table></figure></p>
<p>添加SPARK_DAEMON_JAVA_OPTS的配置信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop000:2181,hadoop001:2181,hadoop002:2181 -Dspark.deploy.zookeeper.dir=/spark"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="u914D_u7F6E_u53C2_u6570_u8BF4_u660E_uFF1A"><a href="#u914D_u7F6E_u53C2_u6570_u8BF4_u660E_uFF1A" class="headerlink" title="配置参数说明："></a>配置参数说明：</h3><p>spark.deploy.recoveryMode: 设置恢复模式为zk，默认为NONE<br>spark.deploy.zookeeper.url: 设置ZK集群的url，形如：192.168.1.100:2181,192.168.1.101:2181<br>spark.deploy.zookeeper.dir: 设置zk保存恢复状态的路径，默认为spark<br>实现HA的原理：利用ZK的Leader Election机制，选择一个Active状态的Master，其余的Master均为Standby状态；当Active状态的Master死掉后，通过ZK选举一个Standby状态的Master为Active状态。</p>
<h3 id="u6D4B_u8BD5_u6B65_u9AA4_uFF1A"><a href="#u6D4B_u8BD5_u6B65_u9AA4_uFF1A" class="headerlink" title="测试步骤："></a>测试步骤：</h3><p>启动standalone集群后，在各个Standby节点上启动start-master.sh，jps观察是否已经正确启动Master进程；<br>将Active状态的Master kill掉，观察8080端口对应的页面，发现已经从Standby状态中选举出一个当作Active状态。<br>采用ZK后由于会有多个Master，在提交任务时不知道哪个为Active状态的Master，可以采用如下的方式提交：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark-shell –master spark://hadoop000:<span class="number">7077</span>,hadoop001:<span class="number">7077</span>,hadoop002:<span class="number">7077</span> –executor-memory <span class="number">2</span>g –total-executor-cores <span class="number">1</span></span><br><span class="line">详细信息参见官方文档：http://spark.apache.org/docs/latest/spark-standalone.html<span class="comment">#standby-masters-with-zookeeper</span></span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Spark Standalone模式常见的HA部署方式有两种：基于文件系统的HA和基于ZK的HA<br>本篇只介绍基于ZK的HA环境搭建：<br><figure class="highlight bash"><table><tr><td class="gutter"><pr]]>
    </summary>
    
      <category term="ha" scheme="http://blog.djstudy.net/tags/ha/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spark on yarn]]></title>
    <link href="http://blog.djstudy.net/2016/01/24/spark-on-yarn/"/>
    <id>http://blog.djstudy.net/2016/01/24/spark-on-yarn/</id>
    <published>2016-01-24T08:08:20.000Z</published>
    <updated>2016-01-24T08:08:20.000Z</updated>
    <content type="html"><![CDATA[<h2 id="u4E3A_u4EC0_u4E48_u8981_u4F7F_u7528YARN_3F"><a href="#u4E3A_u4EC0_u4E48_u8981_u4F7F_u7528YARN_3F" class="headerlink" title="为什么要使用YARN?"></a>为什么要使用YARN?</h2><p>数据共享、资源利用率、更方便的管理集群等。<br>详情参见：<a href="http://www.cnblogs.com/luogankun/p/3887019.html" target="_blank" rel="external">http://www.cnblogs.com/luogankun/p/3887019.html</a></p>
<h2 id="Spark_YARN_u7248_u672C_u7F16_u8BD1"><a href="#Spark_YARN_u7248_u672C_u7F16_u8BD1" class="headerlink" title="Spark YARN版本编译"></a>Spark YARN版本编译</h2><p>编译hadoop对应的支持YARN的Spark版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> MAVEN_OPTS=<span class="string">"-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"</span></span><br><span class="line">$ mvn clean package -DskipTests -Phadoop-<span class="number">2.3</span> -Dhadoop.version=<span class="number">2.3</span>.<span class="number">0</span>-cdh5.<span class="number">0.0</span> -Dprotobuf.version=<span class="number">2.5</span>.<span class="number">0</span> -Pyarn -Phive</span><br></pre></td></tr></table></figure></p>
<p>详情参见：<a href="http://www.cnblogs.com/luogankun/p/3798403.html" target="_blank" rel="external">Spark On YARN</a></p>
<p>Spark的Cluster Manager负责管理启动executor进程，集群可以是Standalone、YARN和Mesos<br>每个SparkContext（换句话说是：Application）对应一个ApplicationMaster（Application启动过程中的第一个容器<br>ApplicationMaster负责和ResourceManager打交道，并请求资源，当获取资源之后通知NodeManager为其启动container； 每个Container中运行一个ExecutorBackend<br>ResourceManager决定哪些Application可以运行、什么时候运行以及在哪些NodeManager上运行； NodeManager的Container上运行executor进程<br>在Standalone模式中有Worker的概念，而在Spark On YARN中没有Worker的概念<br>由于executor是运行在container中，故container内存要大于executor的内存<br>Spark On YARN有两种：</p>
<h3 id="yarn-client"><a href="#yarn-client" class="headerlink" title="yarn-client"></a>yarn-client</h3><p>Client和Driver运行在一起，ApplicationMaster只负责获取资源<br>　　Client会和请求到的资源container通信来调度他们进行工作，也就是说Client不能退出滴；<br>　　日志信息输出能输出在终端控制台上，适用于交互或者调试，也就是希望快速地看到application的输出，比如SparkStreaming<br><img src="/images/yarn-client.png" alt="yarn-client" title="yarn client"></p>
<h3 id="yarn-cluster"><a href="#yarn-cluster" class="headerlink" title="yarn-cluster"></a>yarn-cluster</h3><p>Driver和ApplicationMaster运行在一起；负责向YARN申请资源，并检测作业的运行状况；executor运行在container中<br>　　提交Application之后，即使关掉了Client，作业仍然会继续在YARN上运行<br>　　日志信息不会输出在终端控制台上<br><img src="/images/yarn-cluster.png" alt="yarn-cluster" title="yarn cluster"></p>
<h3 id="u63D0_u4EA4Spark_u4F5C_u4E1A_u5230YARN"><a href="#u63D0_u4EA4Spark_u4F5C_u4E1A_u5230YARN" class="headerlink" title="提交Spark作业到YARN"></a>提交Spark作业到YARN</h3><p>提交命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class &lt;main-class&gt;</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">  ... <span class="comment"># other options</span></span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure></p>
<h3 id="u63D0_u4EA4_u672C_u5730jar"><a href="#u63D0_u4EA4_u672C_u5730jar" class="headerlink" title="提交本地jar"></a>提交本地jar</h3><p>提交到yarn-cluster/yarn-client<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn-cluster \  <span class="comment"># can also be `yarn-client` for client mode</span></span><br><span class="line">  --executor-memory <span class="number">20</span>G \</span><br><span class="line">  --num-executors <span class="number">50</span> \</span><br><span class="line">  /path/to/examples.jar \</span><br></pre></td></tr></table></figure></p>
<p>如果采用的是yarn-cluster的方式运行的话，想停止执行应用，需要去多个node上干掉；而在yarn-client模式运行时，只需要在client上干掉应用即可。<br>提交到standalone<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://<span class="number">207.184</span>.<span class="number">161.138</span>:<span class="number">7077</span> \</span><br><span class="line">  --executor-memory <span class="number">20</span>G \</span><br><span class="line">  --total-executor-cores <span class="number">100</span> \</span><br><span class="line">  /path/to/examples.jar \</span><br></pre></td></tr></table></figure></p>
<h3 id="u63D0_u4EA4hdfs_u4E0A_u7684jar"><a href="#u63D0_u4EA4hdfs_u4E0A_u7684jar" class="headerlink" title="提交hdfs上的jar"></a>提交hdfs上的jar</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn-cluster \  <span class="comment"># can also be `yarn-client` for client mode</span></span><br><span class="line">  --executor-memory <span class="number">20</span>G \</span><br><span class="line">  --num-executors <span class="number">50</span> \</span><br><span class="line">  hdfs://hadoop000:<span class="number">8020</span>/lib/examples.jar \</span><br></pre></td></tr></table></figure>
<p>如果没有在spark-env.sh文件中配置HADOOP_CONF_DIR或者YARN_CONF_DIR，可以在提交作业前指定形如<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> HADOOP_CONF_DIR=XXX</span><br><span class="line">$ ./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn-cluster \  <span class="comment"># can also be `yarn-client` for client mode</span></span><br><span class="line">  --executor-memory <span class="number">20</span>G \</span><br><span class="line">  --num-executors <span class="number">50</span> \</span><br><span class="line">  /path/to/examples.jar</span><br></pre></td></tr></table></figure></p>
<p>详情参见：<a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/submitting-applications.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="u4E3A_u4EC0_u4E48_u8981_u4F7F_u7528YARN_3F"><a href="#u4E3A_u4EC0_u4E48_u8981_u4F7F_u7528YARN_3F" class="headerlink" title="为什么要使用YA]]>
    </summary>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="yarn" scheme="http://blog.djstudy.net/tags/yarn/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ambari hdp中部署apache spark运行spark shell遇到的错误解决]]></title>
    <link href="http://blog.djstudy.net/2016/01/24/hdp-spark-shell-error/"/>
    <id>http://blog.djstudy.net/2016/01/24/hdp-spark-shell-error/</id>
    <published>2016-01-24T08:04:10.000Z</published>
    <updated>2016-01-24T08:04:10.000Z</updated>
    <content type="html"><![CDATA[<p>在运行spark-shell中遇到的ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library 解决方法<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell –driver-library-path :/usr/hdp/<span class="number">2.2</span>.<span class="number">4.2</span>-<span class="number">2</span>/hadoop/lib/native/Linux-amd64-<span class="number">64</span> /usr/hdp/<span class="number">2.2</span>.<span class="number">4.2</span>-<span class="number">2</span>/hadoop/lib/hadoop-lzo-<span class="number">0.6</span>.<span class="number">0.2</span>.<span class="number">2.4</span>.<span class="number">2</span>-<span class="number">2</span>.jar</span><br></pre></td></tr></table></figure></p>
<p>在运行spark-shell中遇到的Compression codec com.hadoop.compression.lzo.LzoCodec not found 错误可以配置文件spark-defaults.conf<br>spark.executor.extraClassPath /usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jar<br>spark.driver.extraClassPath /usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jar<br>保存文件重启spark服务集群即可。<br>再提供一个Unable to load native-hadoop library 和 Snappy native library not loaded的解决方案。这个问题主要是jre目录下缺少了libgplcompression.so , libhadoop.so和libsnappy.so两个文件。具体是，spark-shell依赖的是scala，scala依赖的是JAVA_HOME下的jdk，libhadoop.so和libsnappy.so两个文件应该放到JAVA_HOME/jre/lib/amd64下面。要注意的是要知道真正依赖到的JAVA_HOME是哪一个，把两个.so放对地方。这两个so：libhadoop.so和libsnappy.so。前一个so可以在HADOOP_HOME下找到，比如hadoop\lib\native\Linux-amd64-64。第二个libsnappy.so需要下载一个snappy-1.1.0.tar.gz，然后./configure，make编译出来。snappy是google的一个压缩算法，在hadoop jira下<a href="https://issues.apache.org/jira/browse/HADOOP-7206记录了这次集成。" target="_blank" rel="external">https://issues.apache.org/jira/browse/HADOOP-7206记录了这次集成。</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>在运行spark-shell中遇到的ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library 解决方法<br><figure class="highlight bash"><table><tr><td ]]>
    </summary>
    
      <category term="ambari" scheme="http://blog.djstudy.net/tags/ambari/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="yarn" scheme="http://blog.djstudy.net/tags/yarn/"/>
    
      <category term="hadoop" scheme="http://blog.djstudy.net/categories/hadoop/"/>
    
  </entry>
  
</feed>
