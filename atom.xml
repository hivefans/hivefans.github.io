<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[东杰书屋]]></title>
  <subtitle><![CDATA[环境不会改变，解决之道在于改变自己。]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://blog.djstudy.net/"/>
  <updated>2016-01-24T11:03:24.000Z</updated>
  <id>http://blog.djstudy.net/</id>
  
  <author>
    <name><![CDATA[东杰]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[将 Spark 中的文本转换为 Parquet 以提升性能]]></title>
    <link href="http://blog.djstudy.net/2016/01/24/spark-convert-text-parquet/"/>
    <id>http://blog.djstudy.net/2016/01/24/spark-convert-text-parquet/</id>
    <published>2016-01-24T11:03:24.000Z</published>
    <updated>2016-01-24T11:03:24.000Z</updated>
    <content type="html"><![CDATA[<p>列式存储布局（比如 Parquet）可以加速查询，因为它只检查所有需要的列并对它们的值执行计算，因此只读取一个数据文件或表的小部分数据。Parquet 还支持灵活的压缩选项，因此可以显著减少磁盘上的存储。<br>如果您在 HDFS 上拥有基于文本的数据文件或表，而且正在使用 Spark SQL 对它们执行查询，那么强烈推荐将文本数据文件转换为 Parquet 数据文件，以实现性能和存储收益。当然，转换需要时间，但查询性能的提升在某些情况下可能达到 30 倍或更高，存储的节省可高达 75%！<br>已有文章介绍使用 Parquet 存储为 BigSQL、Hive 和 Impala 带来类似的性能收益，本文将介绍如何编写一个简单的 Scala 应用程序，将现有的基于文本的数据文件或表转换为 Parquet 数据文件，还将展示给 Spark SQL 带来的实际存储节省和查询性能提升。</p>
<h3 id="u8BA9_u6211_u4EEC_u8F6C_u6362_u4E3A_Parquet__u5427_uFF01"><a href="#u8BA9_u6211_u4EEC_u8F6C_u6362_u4E3A_Parquet__u5427_uFF01" class="headerlink" title="让我们转换为 Parquet 吧！"></a>让我们转换为 Parquet 吧！</h3><p>Spark SQL 提供了对读取和写入 Parquet 文件的支持，能够自动保留原始数据的模式。Parquet 模式通过 Data Frame API，使数据文件对 Spark SQL 应用程序 “不言自明”。当然，Spark SQL 还支持读取已存储为 Parquet 的现有 Hive 表，但您需要配置 Spark，以便使用 Hive 的元存储来加载所有信息。在我们的示例中，不涉及 Hive 元存储。<br>以下 Scala 代码示例将读取一个基于文本的 CSV 表，并将它写入 Parquet 表：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert</span>(</span>sqlContext: <span class="type">SQLContext</span>, filename: <span class="type">String</span>, schema: <span class="type">StructType</span>, tablename: <span class="type">String</span>) &#123;</span><br><span class="line">     <span class="comment">// import text-based table first into a data frame</span></span><br><span class="line">     <span class="keyword">val</span> df = sqlContext.read.format(<span class="string">"com.databricks.spark.csv"</span>).</span><br><span class="line">       schema(schema).option(<span class="string">"delimiter"</span>, <span class="string">"|"</span>).load(filename)</span><br><span class="line">     <span class="comment">// now simply write to a parquet file</span></span><br><span class="line">     df.write.parquet(<span class="string">"/user/spark/data/parquet/"</span>+tablename)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// usage exampe -- a tpc-ds table called catalog_page</span></span><br><span class="line"> schema= <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_catalog_page_sk"</span>,        <span class="type">IntegerType</span>,<span class="literal">false</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_catalog_page_id"</span>,        <span class="type">StringType</span>,<span class="literal">false</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_start_date_sk"</span>,          <span class="type">IntegerType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_end_date_sk"</span>,            <span class="type">IntegerType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_department"</span>,             <span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_catalog_number"</span>,         <span class="type">LongType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_catalog_page_number"</span>,    <span class="type">LongType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_description"</span>,            <span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_type"</span>,                   <span class="type">StringType</span>,<span class="literal">true</span>)))</span><br><span class="line"> convert(sqlContext,</span><br><span class="line">         hadoopdsPath+<span class="string">"/catalog_page/*"</span>,</span><br><span class="line">         schema,</span><br><span class="line">         <span class="string">"catalog_page"</span>)</span><br></pre></td></tr></table></figure></p>
<p>上面的代码将会读取 hadoopdsPath+”/catalog_page/* 中基于文本的 CSV 文件，并将转换的 Parquet 文件保存在 /user/spark/data/parquet/ 下。此外，转换的 Parquet 文件会在 gzip 中自动压缩，因为 Spark 变量 spark.sql.parquet.compression.codec 已在默认情况下设置为 gzip。您还可以将压缩编解码器设置为 uncompressed、snappy 或 lzo。</p>
<h3 id="u8F6C_u6362_1_TB__u6570_u636E_u5C06_u82B1_u8D39_u591A_u957F_u65F6_u95F4_uFF1F"><a href="#u8F6C_u6362_1_TB__u6570_u636E_u5C06_u82B1_u8D39_u591A_u957F_u65F6_u95F4_uFF1F" class="headerlink" title="转换 1 TB 数据将花费多长时间？"></a>转换 1 TB 数据将花费多长时间？</h3><p>50 分钟，在一个 6 数据节点的 Spark v1.5.1 集群上可达到约 20 GB/分的吞吐量。使用的总内存约为 500GB。HDFS 上最终的 Parquet 文件的格式为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/user/spark/data/parquet/catalog_page/part-r-<span class="number">00000</span>-<span class="number">9</span>ff58e65-<span class="number">0674</span>-<span class="number">440</span>a-<span class="number">883</span>d-<span class="number">256370</span>f33c66.gz.parquet</span><br><span class="line">/user/spark/data/parquet/catalog_page/part-r-<span class="number">00001</span>-<span class="number">9</span>ff58e65-<span class="number">0674</span>-<span class="number">440</span>a-<span class="number">883</span>d-<span class="number">256370</span>f33c66.gz.parquet</span><br></pre></td></tr></table></figure></p>
<h3 id="u5B58_u50A8_u8282_u7701"><a href="#u5B58_u50A8_u8282_u7701" class="headerlink" title="存储节省"></a>存储节省</h3><p>以下 Linux 输出显示了 TEXT 和 PARQUET 在 HDFS 上的大小比较：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -du -h <span class="operator">-s</span> /user/spark/hadoopds1000g</span><br><span class="line">    <span class="number">897.9</span> G  /user/spark/hadoopds1000g</span><br><span class="line">    % hadoop fs -du -h <span class="operator">-s</span> /user/spark/data/parquet</span><br><span class="line">    <span class="number">231.4</span> G  /user/spark/data/parquet</span><br></pre></td></tr></table></figure></p>
<p>1 TB 数据的存储节省了将近 75%！</p>
<h3 id="u67E5_u8BE2_u6027_u80FD_u63D0_u5347"><a href="#u67E5_u8BE2_u6027_u80FD_u63D0_u5347" class="headerlink" title="查询性能提升"></a>查询性能提升</h3><p>Parquet 文件是自描述性的，所以保留了模式。要将 Parquet 文件加载到 DataFrame 中并将它注册为一个 temp 表，可执行以下操作：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = sqlContext.read.parquet(filename)</span><br><span class="line">      df.show</span><br><span class="line">      df.registerTempTable(tablename)</span><br></pre></td></tr></table></figure></p>
<p>要对比性能，然后可以分别对 TEXT 和 PARQUET 表运行以下查询（假设所有其他 tpc-ds 表也都已转换为 Parquet）。您可以利用 spark-sql-perf 测试工具包来执行查询测试。举例而言，现在来看看 TPC-DS 基准测试中的查询 #76，<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">("q76", """</span><br><span class="line">            | <span class="operator"><span class="keyword">SELECT</span></span><br><span class="line">            |    channel, col_name, d_year, d_qoy, i_category, <span class="keyword">COUNT</span>(*) sales_cnt,</span><br><span class="line">            |    <span class="keyword">SUM</span>(ext_sales_price) sales_amt</span><br><span class="line">            | <span class="keyword">FROM</span>(</span><br><span class="line">            |    <span class="keyword">SELECT</span></span><br><span class="line">            |        <span class="string">'store'</span> <span class="keyword">as</span> channel, ss_store_sk col_name, d_year, d_qoy, i_category,</span><br><span class="line">            |        ss_ext_sales_price ext_sales_price</span><br><span class="line">            |    <span class="keyword">FROM</span> store_sales, item, date_dim</span><br><span class="line">            |    <span class="keyword">WHERE</span> ss_store_sk <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line">            |      <span class="keyword">AND</span> ss_sold_date_sk=d_date_sk</span><br><span class="line">            |      <span class="keyword">AND</span> ss_item_sk=i_item_sk</span><br><span class="line">            |    <span class="keyword">UNION</span> ALL</span><br><span class="line">            |    <span class="keyword">SELECT</span></span><br><span class="line">            |        <span class="string">'web'</span> <span class="keyword">as</span> channel, ws_ship_customer_sk col_name, d_year, d_qoy, i_category,</span><br><span class="line">            |        ws_ext_sales_price ext_sales_price</span><br><span class="line">            |    <span class="keyword">FROM</span> web_sales, item, date_dim</span><br><span class="line">            |    <span class="keyword">WHERE</span> ws_ship_customer_sk <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line">            |      <span class="keyword">AND</span> ws_sold_date_sk=d_date_sk</span><br><span class="line">            |      <span class="keyword">AND</span> ws_item_sk=i_item_sk</span><br><span class="line">            |    <span class="keyword">UNION</span> ALL</span><br><span class="line">            |    <span class="keyword">SELECT</span></span><br><span class="line">            |        <span class="string">'catalog'</span> <span class="keyword">as</span> channel, cs_ship_addr_sk col_name, d_year, d_qoy, i_category,</span><br><span class="line">            |        cs_ext_sales_price ext_sales_price</span><br><span class="line">            |    <span class="keyword">FROM</span> catalog_sales, item, date_dim</span><br><span class="line">            |    <span class="keyword">WHERE</span> cs_ship_addr_sk <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line">            |      <span class="keyword">AND</span> cs_sold_date_sk=d_date_sk</span><br><span class="line">            |      <span class="keyword">AND</span> cs_item_sk=i_item_sk) foo</span><br><span class="line">            | <span class="keyword">GROUP</span> <span class="keyword">BY</span> channel, col_name, d_year, d_qoy, i_category</span><br><span class="line">            | <span class="keyword">ORDER</span> <span class="keyword">BY</span> channel, col_name, d_year, d_qoy, i_category</span><br><span class="line">            | <span class="keyword">limit</span> <span class="number">100</span></span></span><br></pre></td></tr></table></figure></p>
<p>查询时间如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TIME               TEXT     PARQUET</span><br><span class="line">Query time (sec)    <span class="number">698</span>          <span class="number">21</span></span><br></pre></td></tr></table></figure></p>
<p>参考资料<br><a href="https://developer.ibm.com/hadoop/blog/2015/12/03/parquet-for-spark-sql/" target="_blank" rel="external">英文原文。</a><br>在 <a href="http://www.ibm.com/developerworks/cn/bigdata/" target="_blank" rel="external">developerWorks 大数据和分析专区</a>，了解关于大数据的更多信息，获取技术文档、how-to 文章、培训、下载、产品信息以及其他资源。<br>加入 <a href="http://www.ibm.com/developerworks/cn/community/" target="_blank" rel="external">developerWorks 中文社区</a>，查看开发人员推动的博客、论坛、组和维基，并与其他 developerWorks 用户交流。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>列式存储布局（比如 Parquet）可以加速查询，因为它只检查所有需要的列并对它们的值执行计算，因此只读取一个数据文件或表的小部分数据。Parquet 还支持灵活的压缩选项，因此可以显著减少磁盘上的存储。<br>如果您在 HDFS 上拥有基于文本的数据文件或表，而且正在使用]]>
    </summary>
    
      <category term="parquet" scheme="http://blog.djstudy.net/tags/parquet/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark Standalone模式HA环境搭建]]></title>
    <link href="http://blog.djstudy.net/2016/01/24/spark-ha/"/>
    <id>http://blog.djstudy.net/2016/01/24/spark-ha/</id>
    <published>2016-01-24T08:13:04.000Z</published>
    <updated>2016-01-24T08:13:04.000Z</updated>
    <content type="html"><![CDATA[<p>Spark Standalone模式常见的HA部署方式有两种：基于文件系统的HA和基于ZK的HA<br>本篇只介绍基于ZK的HA环境搭建：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ SPARK_HOME/conf/spark-env.sh</span><br></pre></td></tr></table></figure></p>
<p>添加SPARK_DAEMON_JAVA_OPTS的配置信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop000:2181,hadoop001:2181,hadoop002:2181 -Dspark.deploy.zookeeper.dir=/spark"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="u914D_u7F6E_u53C2_u6570_u8BF4_u660E_uFF1A"><a href="#u914D_u7F6E_u53C2_u6570_u8BF4_u660E_uFF1A" class="headerlink" title="配置参数说明："></a>配置参数说明：</h3><p>spark.deploy.recoveryMode: 设置恢复模式为zk，默认为NONE<br>spark.deploy.zookeeper.url: 设置ZK集群的url，形如：192.168.1.100:2181,192.168.1.101:2181<br>spark.deploy.zookeeper.dir: 设置zk保存恢复状态的路径，默认为spark<br>实现HA的原理：利用ZK的Leader Election机制，选择一个Active状态的Master，其余的Master均为Standby状态；当Active状态的Master死掉后，通过ZK选举一个Standby状态的Master为Active状态。</p>
<h3 id="u6D4B_u8BD5_u6B65_u9AA4_uFF1A"><a href="#u6D4B_u8BD5_u6B65_u9AA4_uFF1A" class="headerlink" title="测试步骤："></a>测试步骤：</h3><p>启动standalone集群后，在各个Standby节点上启动start-master.sh，jps观察是否已经正确启动Master进程；<br>将Active状态的Master kill掉，观察8080端口对应的页面，发现已经从Standby状态中选举出一个当作Active状态。<br>采用ZK后由于会有多个Master，在提交任务时不知道哪个为Active状态的Master，可以采用如下的方式提交：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark-shell –master spark://hadoop000:<span class="number">7077</span>,hadoop001:<span class="number">7077</span>,hadoop002:<span class="number">7077</span> –executor-memory <span class="number">2</span>g –total-executor-cores <span class="number">1</span></span><br><span class="line">详细信息参见官方文档：http://spark.apache.org/docs/latest/spark-standalone.html<span class="comment">#standby-masters-with-zookeeper</span></span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Spark Standalone模式常见的HA部署方式有两种：基于文件系统的HA和基于ZK的HA<br>本篇只介绍基于ZK的HA环境搭建：<br><figure class="highlight bash"><table><tr><td class="gutter"><pr]]>
    </summary>
    
      <category term="ha" scheme="http://blog.djstudy.net/tags/ha/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spark on yarn]]></title>
    <link href="http://blog.djstudy.net/2016/01/24/spark-on-yarn/"/>
    <id>http://blog.djstudy.net/2016/01/24/spark-on-yarn/</id>
    <published>2016-01-24T08:04:19.000Z</published>
    <updated>2016-01-24T08:04:19.000Z</updated>
    <content type="html"><![CDATA[<h2 id="u4E3A_u4EC0_u4E48_u8981_u4F7F_u7528YARN_3F"><a href="#u4E3A_u4EC0_u4E48_u8981_u4F7F_u7528YARN_3F" class="headerlink" title="为什么要使用YARN?"></a>为什么要使用YARN?</h2><p>数据共享、资源利用率、更方便的管理集群等。<br>详情参见：<a href="http://www.cnblogs.com/luogankun/p/3887019.html" target="_blank" rel="external">http://www.cnblogs.com/luogankun/p/3887019.html</a></p>
<h2 id="Spark_YARN_u7248_u672C_u7F16_u8BD1"><a href="#Spark_YARN_u7248_u672C_u7F16_u8BD1" class="headerlink" title="Spark YARN版本编译"></a>Spark YARN版本编译</h2><p>编译hadoop对应的支持YARN的Spark版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> MAVEN_OPTS=<span class="string">"-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"</span></span><br><span class="line">$ mvn clean package -DskipTests -Phadoop-<span class="number">2.3</span> -Dhadoop.version=<span class="number">2.3</span>.<span class="number">0</span>-cdh5.<span class="number">0.0</span> -Dprotobuf.version=<span class="number">2.5</span>.<span class="number">0</span> -Pyarn -Phive</span><br></pre></td></tr></table></figure></p>
<p>详情参见：<a href="http://www.cnblogs.com/luogankun/p/3798403.html" target="_blank" rel="external">Spark On YARN</a></p>
<p>Spark的Cluster Manager负责管理启动executor进程，集群可以是Standalone、YARN和Mesos<br>每个SparkContext（换句话说是：Application）对应一个ApplicationMaster（Application启动过程中的第一个容器<br>ApplicationMaster负责和ResourceManager打交道，并请求资源，当获取资源之后通知NodeManager为其启动container； 每个Container中运行一个ExecutorBackend<br>ResourceManager决定哪些Application可以运行、什么时候运行以及在哪些NodeManager上运行； NodeManager的Container上运行executor进程<br>在Standalone模式中有Worker的概念，而在Spark On YARN中没有Worker的概念<br>由于executor是运行在container中，故container内存要大于executor的内存<br>Spark On YARN有两种：</p>
<h3 id="yarn-client"><a href="#yarn-client" class="headerlink" title="yarn-client"></a>yarn-client</h3><p>Client和Driver运行在一起，ApplicationMaster只负责获取资源<br>　　Client会和请求到的资源container通信来调度他们进行工作，也就是说Client不能退出滴；<br>　　日志信息输出能输出在终端控制台上，适用于交互或者调试，也就是希望快速地看到application的输出，比如SparkStreaming<br><img src="/images/yarn-client.png" alt="yarn-client" title="yarn client"></p>
<h3 id="yarn-cluster"><a href="#yarn-cluster" class="headerlink" title="yarn-cluster"></a>yarn-cluster</h3><p>Driver和ApplicationMaster运行在一起；负责向YARN申请资源，并检测作业的运行状况；executor运行在container中<br>　　提交Application之后，即使关掉了Client，作业仍然会继续在YARN上运行<br>　　日志信息不会输出在终端控制台上<br><img src="/images/yarn-cluster.png" alt="yarn-cluster" title="yarn cluster"></p>
<h3 id="u63D0_u4EA4Spark_u4F5C_u4E1A_u5230YARN"><a href="#u63D0_u4EA4Spark_u4F5C_u4E1A_u5230YARN" class="headerlink" title="提交Spark作业到YARN"></a>提交Spark作业到YARN</h3><p>提交命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class &lt;main-class&gt;</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">  ... <span class="comment"># other options</span></span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure></p>
<h3 id="u63D0_u4EA4_u672C_u5730jar"><a href="#u63D0_u4EA4_u672C_u5730jar" class="headerlink" title="提交本地jar"></a>提交本地jar</h3><p>提交到yarn-cluster/yarn-client<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn-cluster \  <span class="comment"># can also be `yarn-client` for client mode</span></span><br><span class="line">  --executor-memory <span class="number">20</span>G \</span><br><span class="line">  --num-executors <span class="number">50</span> \</span><br><span class="line">  /path/to/examples.jar \</span><br></pre></td></tr></table></figure></p>
<p>如果采用的是yarn-cluster的方式运行的话，想停止执行应用，需要去多个node上干掉；而在yarn-client模式运行时，只需要在client上干掉应用即可。<br>提交到standalone<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://<span class="number">207.184</span>.<span class="number">161.138</span>:<span class="number">7077</span> \</span><br><span class="line">  --executor-memory <span class="number">20</span>G \</span><br><span class="line">  --total-executor-cores <span class="number">100</span> \</span><br><span class="line">  /path/to/examples.jar \</span><br></pre></td></tr></table></figure></p>
<h3 id="u63D0_u4EA4hdfs_u4E0A_u7684jar"><a href="#u63D0_u4EA4hdfs_u4E0A_u7684jar" class="headerlink" title="提交hdfs上的jar"></a>提交hdfs上的jar</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn-cluster \  <span class="comment"># can also be `yarn-client` for client mode</span></span><br><span class="line">  --executor-memory <span class="number">20</span>G \</span><br><span class="line">  --num-executors <span class="number">50</span> \</span><br><span class="line">  hdfs://hadoop000:<span class="number">8020</span>/lib/examples.jar \</span><br></pre></td></tr></table></figure>
<p>如果没有在spark-env.sh文件中配置HADOOP_CONF_DIR或者YARN_CONF_DIR，可以在提交作业前指定形如<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> HADOOP_CONF_DIR=XXX</span><br><span class="line">$ ./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn-cluster \  <span class="comment"># can also be `yarn-client` for client mode</span></span><br><span class="line">  --executor-memory <span class="number">20</span>G \</span><br><span class="line">  --num-executors <span class="number">50</span> \</span><br><span class="line">  /path/to/examples.jar</span><br></pre></td></tr></table></figure></p>
<p>详情参见：<a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/submitting-applications.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="u4E3A_u4EC0_u4E48_u8981_u4F7F_u7528YARN_3F"><a href="#u4E3A_u4EC0_u4E48_u8981_u4F7F_u7528YARN_3F" class="headerlink" title="为什么要使用YA]]>
    </summary>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="yarn" scheme="http://blog.djstudy.net/tags/yarn/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ambari hdp中部署apache spark运行spark shell遇到的错误解决]]></title>
    <link href="http://blog.djstudy.net/2016/01/24/hdp-spark-shell-error/"/>
    <id>http://blog.djstudy.net/2016/01/24/hdp-spark-shell-error/</id>
    <published>2016-01-24T08:04:10.000Z</published>
    <updated>2016-01-24T08:04:10.000Z</updated>
    <content type="html"><![CDATA[<p>在运行spark-shell中遇到的ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library 解决方法<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell –driver-library-path :/usr/hdp/<span class="number">2.2</span>.<span class="number">4.2</span>-<span class="number">2</span>/hadoop/lib/native/Linux-amd64-<span class="number">64</span> /usr/hdp/<span class="number">2.2</span>.<span class="number">4.2</span>-<span class="number">2</span>/hadoop/lib/hadoop-lzo-<span class="number">0.6</span>.<span class="number">0.2</span>.<span class="number">2.4</span>.<span class="number">2</span>-<span class="number">2</span>.jar</span><br></pre></td></tr></table></figure></p>
<p>在运行spark-shell中遇到的Compression codec com.hadoop.compression.lzo.LzoCodec not found 错误可以配置文件spark-defaults.conf<br>spark.executor.extraClassPath /usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jar<br>spark.driver.extraClassPath /usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jar<br>保存文件重启spark服务集群即可。<br>再提供一个Unable to load native-hadoop library 和 Snappy native library not loaded的解决方案。这个问题主要是jre目录下缺少了libgplcompression.so , libhadoop.so和libsnappy.so两个文件。具体是，spark-shell依赖的是scala，scala依赖的是JAVA_HOME下的jdk，libhadoop.so和libsnappy.so两个文件应该放到JAVA_HOME/jre/lib/amd64下面。要注意的是要知道真正依赖到的JAVA_HOME是哪一个，把两个.so放对地方。这两个so：libhadoop.so和libsnappy.so。前一个so可以在HADOOP_HOME下找到，比如hadoop\lib\native\Linux-amd64-64。第二个libsnappy.so需要下载一个snappy-1.1.0.tar.gz，然后./configure，make编译出来。snappy是google的一个压缩算法，在hadoop jira下<a href="https://issues.apache.org/jira/browse/HADOOP-7206记录了这次集成。" target="_blank" rel="external">https://issues.apache.org/jira/browse/HADOOP-7206记录了这次集成。</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>在运行spark-shell中遇到的ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library 解决方法<br><figure class="highlight bash"><table><tr><td ]]>
    </summary>
    
      <category term="ambari" scheme="http://blog.djstudy.net/tags/ambari/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="yarn" scheme="http://blog.djstudy.net/tags/yarn/"/>
    
      <category term="hadoop" scheme="http://blog.djstudy.net/categories/hadoop/"/>
    
  </entry>
  
</feed>
