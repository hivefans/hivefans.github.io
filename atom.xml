<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[东杰书屋]]></title>
  <subtitle><![CDATA[环境不会改变，解决之道在于改变自己。]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://blog.djstudy.net/"/>
  <updated>2016-03-21T13:05:41.000Z</updated>
  <id>http://blog.djstudy.net/</id>
  
  <author>
    <name><![CDATA[东杰]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[scala学习笔记(二)]]></title>
    <link href="http://blog.djstudy.net/2016/03/19/scala-note-2/"/>
    <id>http://blog.djstudy.net/2016/03/19/scala-note-2/</id>
    <published>2016-03-19T02:23:48.000Z</published>
    <updated>2016-03-21T13:05:41.000Z</updated>
    <content type="html"><![CDATA[<h2 id="scala_u8BBF_u95EE_u4FEE_u9970_u7B26"><a href="#scala_u8BBF_u95EE_u4FEE_u9970_u7B26" class="headerlink" title="scala访问修饰符"></a>scala访问修饰符</h2><p>包，类或对象的成员可以标记访问修饰符private和protected，如果我们不使用这两种关键字，那么访问将被默认设置为public。这些修饰 限制为成员的代码的某些区域访问。</p>
<h3 id="u79C1_u6709_u6210_u5458"><a href="#u79C1_u6709_u6210_u5458" class="headerlink" title="私有成员"></a>私有成员</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Outer</span> </span>&#123;</span><br><span class="line">   <span class="class"><span class="keyword">class</span> <span class="title">Inner</span> </span>&#123;</span><br><span class="line">      <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">f</span></span>() &#123; println(<span class="string">"f"</span>) &#125;</span><br><span class="line">      <span class="class"><span class="keyword">class</span> <span class="title">InnerMost</span> </span>&#123;</span><br><span class="line">         f() <span class="comment">// OK</span></span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   (<span class="keyword">new</span> <span class="type">Inner</span>).f() <span class="comment">// Error: f is not accessible</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在Scala中，访问 (new Inner).f() 是非法的，因为f被声明为private内部类并且访问不是在内部类内。与此相反，到f第一接入类最内层是确定的，因为该访问包含在类内的主体。 Java将允许这两种访问，因为它可以让其内部类的外部类访问私有成员。</p>
</blockquote>
<h3 id="u4FDD_u62A4_u6210_u5458"><a href="#u4FDD_u62A4_u6210_u5458" class="headerlink" title="保护成员"></a>保护成员</h3><p>在 scala 中，对保护（Protected）成员的访问比 java 更严格一些。因为它只允许保护成员在定义了该成员的的类的子类中被访问。而在java中，用protected关键字修饰的成员，除了定义了该成员的类的子类可以访问，同一个包里的其他类也可以进行访问。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> p &#123;</span><br><span class="line">   <span class="class"><span class="keyword">class</span> <span class="title">Super</span> </span>&#123;</span><br><span class="line">      <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">f</span></span>() &#123; println(<span class="string">"f"</span>) &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="class"><span class="keyword">class</span> <span class="title">Sub</span> <span class="keyword">extends</span> <span class="title">Super</span> </span>&#123;</span><br><span class="line">      f()</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="class"><span class="keyword">class</span> <span class="title">Other</span> </span>&#123;</span><br><span class="line">     (<span class="keyword">new</span> <span class="type">Super</span>).f() <span class="comment">// Error: f is not accessible</span></span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上例中，Sub 类对 f 的访问没有问题，因为 f 在 Super 中被声明为 protected，而 Sub 是 Super 的子类。相反，Other 对 f 的访问不被允许，因为 other 没有继承自 Super。而后者在 java 里同样被认可，因为 Other 与 Sub 在同一包里。</p>
<h3 id="u516C_u5171_28Public_29_u6210_u5458"><a href="#u516C_u5171_28Public_29_u6210_u5458" class="headerlink" title="公共(Public)成员"></a>公共(Public)成员</h3><p>Scala中，如果没有指定任何的修饰符，则默认为 public。这样的成员在任何地方都可以被访问。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Outer</span> </span>&#123;</span><br><span class="line">   <span class="class"><span class="keyword">class</span> <span class="title">Inner</span> </span>&#123;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">f</span></span>() &#123; println(<span class="string">"f"</span>) &#125;</span><br><span class="line">      <span class="class"><span class="keyword">class</span> <span class="title">InnerMost</span> </span>&#123;</span><br><span class="line">         f() <span class="comment">// 正确</span></span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   (<span class="keyword">new</span> <span class="type">Inner</span>).f() <span class="comment">// 正确因为 f() 是 public</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="u4F5C_u7528_u57DF_u4FDD_u62A4"><a href="#u4F5C_u7528_u57DF_u4FDD_u62A4" class="headerlink" title="作用域保护"></a>作用域保护</h3><p>Scala中，访问修饰符可以通过使用限定词强调。格式为:<br>private[x] </p>
<p>或 </p>
<p>protected[x]<br>这里的x指代某个所属的包、类或单例对象。如果写成private[x],读作”这个成员除了对[…]中的类或[…]中的包中的类及它们的伴生对像可见外，对其它所有类都是private。<br>这种技巧在横跨了若干包的大型项目中非常有用，它允许你定义一些在你项目的若干子包中可见但对于项目外部的客户却始终不可见的东西。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> bobsrocckets&#123;</span><br><span class="line">    <span class="keyword">package</span> navigation&#123;</span><br><span class="line">        <span class="keyword">private</span>[bobsrockets] <span class="class"><span class="keyword">class</span> <span class="title">Navigator</span></span>&#123;</span><br><span class="line">         <span class="keyword">protected</span>[navigation] <span class="function"><span class="keyword">def</span> <span class="title">useStarChart</span></span>()&#123;&#125;</span><br><span class="line">         <span class="class"><span class="keyword">class</span> <span class="title">LegOfJourney</span></span>&#123;</span><br><span class="line">             <span class="keyword">private</span>[<span class="type">Navigator</span>] <span class="keyword">val</span> distance = <span class="number">100</span></span><br><span class="line">             &#125;</span><br><span class="line">            <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> speed = <span class="number">200</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">package</span> launch&#123;</span><br><span class="line">        <span class="keyword">import</span> navigation._</span><br><span class="line">        <span class="class"><span class="keyword">object</span> <span class="title">Vehicle</span></span>&#123;</span><br><span class="line">        <span class="keyword">private</span>[launch] <span class="keyword">val</span> guide = <span class="keyword">new</span> <span class="type">Navigator</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上述例子中，类Navigator被标记为private[bobsrockets]就是说这个类对包含在bobsrockets包里的所有的类和对象可见。<br>比如说，从Vehicle对象里对Navigator的访问是被允许的，因为对象Vehicle包含在包launch中，而launch包在bobsrockets中，相反，所有在包bobsrockets之外的代码都不能访问类Navigator。</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="scala_u8BBF_u95EE_u4FEE_u9970_u7B26"><a href="#scala_u8BBF_u95EE_u4FEE_u9970_u7B26" class="headerlink" title="scala访问修饰符"></a>scala访]]>
    </summary>
    
      <category term="scala" scheme="http://blog.djstudy.net/tags/scala/"/>
    
      <category term="scala" scheme="http://blog.djstudy.net/categories/scala/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[scala学习笔记(一)]]></title>
    <link href="http://blog.djstudy.net/2016/03/16/scala-note-1/"/>
    <id>http://blog.djstudy.net/2016/03/16/scala-note-1/</id>
    <published>2016-03-16T08:05:41.000Z</published>
    <updated>2016-03-21T13:02:36.000Z</updated>
    <content type="html"><![CDATA[<h2 id="u57FA_u7840_u8BED_u6CD5"><a href="#u57FA_u7840_u8BED_u6CD5" class="headerlink" title="基础语法"></a>基础语法</h2><p>区分大小写 -  Scala是大小写敏感的，这意味着标识Hello 和 hello在Scala中会有不同的含义<br>类名 - 对于所有的类名的第一个字母要大写。<br>如果需要使用几个单词来构成一个类的名称，每个单词的第一个字母要大写。<br>示例：class MyFirstScalaClass<br>方法名称 - 所有的方法名称的第一个字母用小写。<br>如果若干单词被用于构成方法的名称，则每个单词的第一个字母应大写。<br>示例：def myMethodName()<br>程序文件名 - 程序文件的名称应该与对象名称完全匹配。<br>保存文件时，应该保存它使用的对象名称（记住Scala是区分大小写），并追加“.scala”为文件扩展名。 （如果文件名和对象名称不匹配，程序将无法编译）。</p>
<p>示例: 假设“HelloWorld”是对象的名称。那么该文件应保存为’HelloWorld.scala“<br>def main(args: Array[String]) - Scala程序从main()方法开始处理，这是每一个Scala程序的强制程序入口部分。</p>
<h2 id="u5B57_u7B26_u4E32"><a href="#u5B57_u7B26_u4E32" class="headerlink" title="字符串"></a>字符串</h2><p>Scala中单引号和双引号包裹是有区别的，单引号用于字符，双引号用于字符串。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> c1 = 'c'</span><br><span class="line">c1: <span class="type">Char</span> = c</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> 字符<span class="number">2</span> = '杨'</span><br><span class="line">字符<span class="number">2</span>: <span class="type">Char</span> = 杨</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> s1 = <span class="string">"scala基础语法"</span></span><br><span class="line">s1: <span class="type">String</span> = scala基础语法</span><br></pre></td></tr></table></figure></p>
<h2 id="Scala_u4FEE_u9970_u7B26_3A"><a href="#Scala_u4FEE_u9970_u7B26_3A" class="headerlink" title="Scala修饰符:"></a>Scala修饰符:</h2><p>所有的Scala的组件需要名称。使用对象，类，变量和方法名被称为标识符。关键字不能用作标识符和标识是区分大小写的。Scala支持以下四种类型标识符：</p>
<h3 id="u6587_u5B57_u6807_u8BC6_u7B26"><a href="#u6587_u5B57_u6807_u8BC6_u7B26" class="headerlink" title="文字标识符"></a>文字标识符</h3><p>字母数字标识符开始以字母或下划线，可以使用字母，数字或下划线。“$”字符在Scala中是保留关键字，标识符不能使用。以下是合法的字母标识符：</p>
<p>age, salary, _value,  __1_value<br>以下是非法标识符：</p>
<p>$salary, 123abc, -salary</p>
<h3 id="u8FD0_u7B97_u7B26_u6807_u8BC6"><a href="#u8FD0_u7B97_u7B26_u6807_u8BC6" class="headerlink" title="运算符标识"></a>运算符标识</h3><p>运算符识别符由一个或多个运算符字符。操作字符是可打印的ASCII字符，如+, :, ?, ~ 或#。以下是合法的运算符标识：</p>
<ul>
<li>++ ::: &lt;?&gt; :&gt;<br>Scala编译器将在内部“轧”操作符标识符使它们成为合法的Java标识符，并嵌入$字符。例如，所述标识符:-&gt;将内部表示为$colon$minus$greater。</li>
</ul>
<h3 id="u6DF7_u5408_u6807_u8BC6_u7B26"><a href="#u6DF7_u5408_u6807_u8BC6_u7B26" class="headerlink" title="混合标识符"></a>混合标识符</h3><p>混合标识符由一个字母数字识别符，随后是一个下划线和运算符标识。以下是合法的混合标识符：</p>
<p>unary<em>+,  myvar</em>=<br>在这里，作为一个方法名unary<em>+定义了一个一元+运算符和myvar</em>=用来作为方法名称定义了一个赋值运算符。</p>
<h3 id="u7ACB_u5373_u6570_u6807_u8BC6_u7B26"><a href="#u7ACB_u5373_u6570_u6807_u8BC6_u7B26" class="headerlink" title="立即数标识符"></a>立即数标识符</h3><p>一个文字标识是包含在反引号(<code>. . .</code>)的任意字符串。以下是合法的文字标识：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">g</span></span>(x : <span class="type">Int</span>) = <span class="number">5</span> <span class="keyword">match</span> &#123; <span class="keyword">case</span> `x` =&gt; <span class="string">"yup"</span>; <span class="keyword">case</span> _ =&gt; <span class="string">"nope"</span>&#125;</span><br><span class="line">g: (x: <span class="type">Int</span>)java.lang.<span class="type">String</span></span><br><span class="line"></span><br><span class="line">scala&gt; g(<span class="number">5</span>)</span><br><span class="line">res3: java.lang.<span class="type">String</span> = yup</span><br></pre></td></tr></table></figure></p>
<h2 id="Scala__u5305"><a href="#Scala__u5305" class="headerlink" title="Scala 包"></a>Scala 包</h2><h3 id="u5B9A_u4E49_u5305"><a href="#u5B9A_u4E49_u5305" class="headerlink" title="定义包"></a>定义包</h3><p>Scala 使用 package 关键字定义包，在Scala将代码定义到某个包中有两种方式：<br>第一种方法和 Java 一样，在文件的头定义包名，这种方法就后续所有代码都放在该报中。 比如：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.runoob</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HelloWorld</span></span></span><br></pre></td></tr></table></figure></p>
<p>第二种方法有些类似 C#，如：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.runoob &#123;</span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">HelloWorld</span> </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>第二种方法，可以在一个文件中定义多个包。</p>
<h3 id="u5F15_u7528"><a href="#u5F15_u7528" class="headerlink" title="引用"></a>引用</h3><p>Scala 使用 import 关键字引用包。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.awt.<span class="type">Color</span>  <span class="comment">// 引入Color</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> java.awt._  <span class="comment">// 引入包内所有成员</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handler</span></span>(evt: event.<span class="type">ActionEvent</span>) &#123; <span class="comment">// java.awt.event.ActionEvent</span></span><br><span class="line">  ...  <span class="comment">// 因为引入了java.awt，所以可以省去前面的部分</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>import语句可以出现在任何地方，而不是只能在文件顶部。import的效果从开始延伸到语句块的结束。这可以大幅减少名称冲突的可能性。<br>如果想要引入包中的几个成员，可以使用selector（选取器）：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.awt.&#123;<span class="type">Color</span>, <span class="type">Font</span>&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 重命名成员</span></span><br><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">HashMap</span> =&gt; <span class="type">JavaHashMap</span>&#125;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 隐藏成员</span></span><br><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">HashMap</span> =&gt; _, _&#125; <span class="comment">// 引入了util包的所有成员，但是HashMap被隐藏了</span></span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>注意：默认情况下，Scala 总会引入 java.lang.<em> 、 scala.</em> 和 Predef._，这里也能解释，为什么以scala开头的包，在使用时都是省去scala.的。</p>
</blockquote>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="u57FA_u7840_u8BED_u6CD5"><a href="#u57FA_u7840_u8BED_u6CD5" class="headerlink" title="基础语法"></a>基础语法</h2><p>区分大小写 -  Scala是大小写敏感的，这意]]>
    </summary>
    
      <category term="scala" scheme="http://blog.djstudy.net/tags/scala/"/>
    
      <category term="scala" scheme="http://blog.djstudy.net/categories/scala/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Optimizing Elasticsearch-How Many Shards per Index]]></title>
    <link href="http://blog.djstudy.net/2016/02/27/how-many-shards-index/"/>
    <id>http://blog.djstudy.net/2016/02/27/how-many-shards-index/</id>
    <published>2016-02-27T02:58:40.000Z</published>
    <updated>2016-02-27T02:58:52.000Z</updated>
    <content type="html"><![CDATA[<p>A key question in the minds of most Elasticsearch users when they create an index is “How many shards should I use?” In this article, we explain the design tradeoffs and performance consequences of choosing different values for the number of shards. Continue reading if you want to learn how to demystify and optimize your sharding strategy.</p>
<h3 id="Why_Bother_3F"><a href="#Why_Bother_3F" class="headerlink" title="Why Bother?"></a>Why Bother?</h3><p>This is an important topic, and many users are apprehensive as they approach it – and for good reason. A major mistake in shard allocation could cause scaling problems in a production environment that maintains an ever-growing dataset.</p>
<p>On the other hand, we know that there is little Elasticsearch documentation on this topic. Most users just want answers – and they want specific answers, not vague number ranges and warnings for arbitrarily large numbers.</p>
<p>Well, we have some answers. After covering a few definitions and some clarifications, we present several common use cases and provide our recommendations for each.</p>
<h3 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h3><p>If you’re fairly new to Elasticsearch, it’s important that you understand the basic jargon and grasp the elemental concepts. </p>
<p>(If you have some expertise with ES, you might want to skip to the next section.)</p>
<p>Consider this simple diagram of an Elasticsearch cluster:”&gt;”&gt;<br><img src="https://qbox.io/img/blog/elasticsearch_cluster.png?t=1439423553643&amp;width=700" alt="es element"><br>Remember these definitions while refering to this diagram:</p>
<p>“&gt;</p>
<p>cluster – An Elasticsearch cluster consists of one or more nodes and is identifiable by its cluster name.</p>
<p>node – A single Elasticsearch instance. In most environments, each node runs on a separate box or virtual machine.</p>
<p>index – In Elasticsearch, an index is a collection of documents.</p>
<p>shard – Because Elasticsearch is a distributed search engine, an index is usually split into elements known as shards that are distributed across multiple nodes. Elasticsearch automatically manages the arrangement of these shards. It also rebalances the shards as necessary, so users need not worry about the details.</p>
<p>replica – By default, Elasticsearch creates five primary shards and one replica for each index. This means that each index will consist of five primary shards, and each shard will have one copy.</p>
<p>Allocating multiple shards and replicas is the essence of the design for distributed search capability, providing for high availability and quick access in searches against the documents within an index. The main difference between a primary and a replica shard is that only the primary shard can accept indexing requests. Both replica and primary shards can serve querying requests.</p>
<p>In the diagram above, we have an Elasticsearch cluster consisting of two nodes in a default shard configuration. Elasticsearch automatically arranges the five primary shards split across the two nodes. There is one replica shard that corresponds to each primary shard, but the arrangement of these replica shards is altogether different from that of the primary shards. Again, think distribution.</p>
<p>Allow us to clarify: Remember, the number_of_shards value pertains to indexes—not to the cluster as whole. This value specifies the number of shards for <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html" target="_blank" rel="external">each index</a>(not the total primary shards in the cluster).</p>
<h3 id="A_Word_about_Replicas"><a href="#A_Word_about_Replicas" class="headerlink" title="A Word about Replicas"></a>A Word about Replicas</h3><p>We don’t elaborate in this article on Elasticsearch replicas. That is an entirely separate topic that <a href="http://blog.qbox.io/announcing-replicated-elasticsearch-clusters" target="_blank" rel="external">we cover elsewhere</a>. Replicas are primarily for search performance, and a user can add or remove them at any time. As we explain in <a href="http://blog.qbox.io/announcing-replicated-elasticsearch-clusters" target="_blank" rel="external">that article</a>, additional replicas give you additional capacity, higher throughput, and stronger failover.</p>
<h3 id="Allocate_Shards_Carefully"><a href="#Allocate_Shards_Carefully" class="headerlink" title="Allocate Shards Carefully"></a>Allocate Shards Carefully</h3><p>After you configure an Elasticsearch cluster, it’s critically important to realize that you cannot modify the shard allocation later. If you later find it necessary to change the number of shards, then you would need to reindex all the source documents. (Although reindexing is a long process, it can be done without downtime).</p>
<p>The primary shard configuration is quite analogous to a hard disk partition, in which a repartition of raw disk space requires a user to back up, configure a new partition, and rewrite data onto the new partition.</p>
<h3 id="Small_Static_Dataset_2C_2-3_GB"><a href="#Small_Static_Dataset_2C_2-3_GB" class="headerlink" title="Small Static Dataset, 2-3 GB"></a>Small Static Dataset, 2-3 GB</h3><p>The key consideration as you allocate shards is your expectation for the growth of your dataset. </p>
<p>We quite often see the tendency to unnecessarily overallocate on shard count. Since share count such a hot topic within the ES community, users may assume that overallocation is a safe bet. (By overallocation, we simply mean specifying more shards per index than is necessary for the current size (document count) for a particular dataset.)</p>
<p>Elastic was promoting this idea in the early days, but then many users began taking it too far—such as allocating 1,000 shards. Elastic now provides a bit more cautious rationale:</p>
<p>“A little overallocation is good. A kagillion shards is bad. It is difficult to define what constitutes too many shards, as it depends on their size and how they are being used. A hundred shards that are seldom used may be fine, while two shards experiencing very heavy usage could be too many.”<br>Remember that there is an additional cost for each shard that you allocate:</p>
<p>“&gt;</p>
<p>“&gt;</p>
<p>Since a shard is essentially a Lucene index, it consumes file handles, memory, and CPU resources.<br>Each search request will touch a copy of every shard in the index, which isn’t a problem when the shards are spread across several nodes. Contention arises and performance decreases when the shards are competing for the same hardware resources.<br>Elasticsearch uses <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-intro.html" target="_blank" rel="external">term frequency statistics to calculate relevance</a>, but these statistics correspond to individual shards. Maintaining only a small amount of data across a many shards will tend to result in poor document relevance.<br>Our customers expect their businesses to grow and their datasets to expand accordingly. There is therefore always a need for contingency planning. Many users convince themselves that they’ll encounter explosive growth (although most never actually see an unmanageable spike). In addition, we all want to minimize downtime and avoid resharding.</p>
<p>If you worry about rapid data growth, then we suggest a focus on a simple constraint: the <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/_limiting_memory_usage.html" target="_blank" rel="external">maximum JVM heap size recommendation for Elasticsearch</a> is approximately 30-32GB. This is a solid estimate on the limit of your absolute maximum shard size. For example, if you really think it possible that you could reach 200GB (but not much further without other infrastructure changes), then we recommend an allocation of 7 shards, or 8 shards at most.</p>
<p>By all means, don’t allocate for an inappropriately high goal of 10 terabytes that you might attain three years from now. It’s likely that you’ll see some performance strain—sooner than you like.</p>
<p>Although we aren’t explaining replicas in detail here, we do recommend that you plan for a modest number of shards and consider increasing the number of replicas. If you’re configuring a new environment, then perhaps you want to have a look at our <a href="http://blog.qbox.io/announcing-replicated-elasticsearch-clusters" target="_blank" rel="external">replicated clusters</a>. With a replicated cluster, you get a three-node cluster that includes one replica with an option to easily increase the number of replicas as your requirements change.</p>
<h3 id="Large_and_Growing_Dataset"><a href="#Large_and_Growing_Dataset" class="headerlink" title="Large and Growing Dataset"></a>Large and Growing Dataset</h3><p>We strongly encourage you to rely on over-allocation for large datasets—but only modestly. You can still use the 30GB maximum shard size guideline that we give above.</p>
<p>We do, however, suggest that you continue to picture the ideal scenario as being one shard per index, per node. A good launch point for capacity planning is to allocate shards with a factor of 1.5 to 3 times the number of nodes in your initial configuration. If you’re starting with 3 nodes, then we recommend that you specify at most 3 x 3 = 9 shards.</p>
<p>Your shard size may be getting too high if you’re discovering issues through the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-stats.html?q=cluster%20stat" target="_blank" rel="external">cluster stats APIs</a> or encountering minor performance degradations. If this is the case, simply add a node and ES will will rebalance the shards acccordingly.</p>
<p>Once again, please note that we’re omitting the specification of replicas from our discussion here. The same ideal shard guideline of one shard per index per node also holds true for replica shards. So if you need only one replica, then you’ll need twice as many nodes. Two replicas would require three times the number of nodes. For more details, see our article on <a href="http://blog.qbox.io/announcing-replicated-elasticsearch-clusters" target="_blank" rel="external">Replicated Clusters</a>.</p>
<p>“&gt;</p>
<h3 id="Logstash"><a href="#Logstash" class="headerlink" title="Logstash"></a>Logstash</h3><p>Do you accummulate daily indices and yet incur only small search loads? Perhaps these indices number in the hundreds, but each index is 1GB or smaller. For these and similar problem spaces, our simple recommendation is that you choose one shard.</p>
<p>If you roll with the defaults for Logstash (daily indices) and ES (5 shards), you could generate up to 890 shards in 6 months. Further, your cluster will be hurting—unless you have 15 nodes or more.</p>
<p>Think about it: most Logstash users are infrequent searchers, performing fewer than one query per minute. Accordingly, we recommend a simple economical setup. Since search performance isn’t a primary requirement for such cases, we don’t need multiple replicas. A single replica is enough for basic redundancy. The data-to-memory ratio can also be quite high.</p>
<p>If you go with a single shard per index, then you could probably run a Logstash configuration for 6 months on a three-node cluster. Ideally, you’d use at least 4GB, but we’d recommend 8GB because 8GB is where network speed starts to get significantly better on most cloud platforms and much less resource-sharing.</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>We reiterate that shards consume resources and require processing overhead.</p>
<p>To compile results from an index consisting of more than one shard, Elasticsearch must query each shard individually (although in parallel), and then it must perform operations on the aggregated results. Because of this, a machine with more IO headroom (SSDs) and a multi-core processor can definitely benefit from sharding, but you must consider the size, volatility, and future states of your dataset. While there is no one-size-for-all with respect to shard allocation, we hope that you can benefit from this discussion.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>A key question in the minds of most Elasticsearch users when they create an index is “How many shards should I use?” In this article, we ]]>
    </summary>
    
      <category term="elasticsearch" scheme="http://blog.djstudy.net/tags/elasticsearch/"/>
    
      <category term="optimizing" scheme="http://blog.djstudy.net/tags/optimizing/"/>
    
      <category term="elasticsearch" scheme="http://blog.djstudy.net/categories/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Service discovery with consul and consul-template]]></title>
    <link href="http://blog.djstudy.net/2016/02/19/service-discovery-consul-and-consul-template/"/>
    <id>http://blog.djstudy.net/2016/02/19/service-discovery-consul-and-consul-template/</id>
    <published>2016-02-19T03:27:26.000Z</published>
    <updated>2016-02-19T03:37:46.000Z</updated>
    <content type="html"><![CDATA[<p>I talked in the past about an <a href="http://agiletesting.blogspot.com/2013/12/ops-design-pattern-local-haproxy.html" target="_blank" rel="external">“Ops Design Pattern: local haproxy talking to service layer”</a>. I described how we used a local haproxy on pretty much all nodes at a given layer of our infrastructure (webapp, API, e-commerce) to talk to services offered by the layer below it. So each webapp server has a local haproxy that talks to all API nodes it sends requests to. Similarly, each API node has a local haproxy that talks to all e-commerce nodes it needs info from.</p>
<p>This seemed like a good idea at a time, but it turns out it has a couple of annoying drawbacks:<br>each local haproxy runs health checks against N nodes, so if you have M nodes running haproxy, each of the N nodes will receive M health checks; if M and N are large, then you have a health check storm on your hands<br>to take a node out of a cluster at any given layer, we tag it as ‘inactive’ in Chef, then run chef-client on all nodes that run haproxy and talk to the inactive node at layers above it; this gets old pretty fast, especially when you’re doing anything that might conflict with Chef and that the chef-client run might overwrite (I know, I know, you’re not supposed to do anything of that nature, but we are all human :-)<br>For the second point, we are experimenting with haproxyctl so that we don’t have to run chef-client on every node running haproxy. But it still feels like a heavy-handed approach.</p>
<p>If I were to do this again (which I might), I would still have an haproxy instance in front of our webapp servers, but for communicating from one layer of services to another I would use a proper service discovery tool such as grampa Apache ZooKeeper or the newer kids on the block, etcd from CoreOS and consul from HashiCorp.</p>
<p>I settled on consul for now, so in this post I am going to show how you can use consul in conjunction with the recently released consul-template to discover services and to automate configuration changes. At the same time, I wanted to experiment a bit with Ansible as a configuration management tool. So the steps I’ll describe were actually automated with Ansible, but I’ll leave that for another blog post.</p>
<p>The scenario I am going to describe involves 2 haproxy instances, each pointing to 2 Wordpress servers running Apache, PHP and MySQL, with Varnish fronting the Wordpress application. One of the 2 Wordpress servers is considered primary as far as haproxy is concerned, and the other one is a backup server, which will only get requests if the primary server is down. All servers are running Ubuntu 12.04.</p>
<p>Install and run the consul agent on all nodes</p>
<p>The agent will start in server mode on the 2 haproxy nodes, and in agent mode on the 2 Wordpress nodes.</p>
<p>I first deployed consul to the 2 haproxy nodes. I used a modified version of the ansible-consul role from jivesoftware. The configuration file /etc/consul.cfg for the first server (lb1) is:</p>
<p>{<br>  “domain”: “consul.”,<br>  “data_dir”: “/opt/consul/data”,<br>  “log_level”: “INFO”,<br>  “node_name”: “lb1”,<br>  “server”: true,<br>  “bind_addr”: “10.0.0.1”,<br>  “datacenter”: “us-west-1b”,<br>  “bootstrap”: true,<br>  “rejoin_after_leave”: true<br>}</p>
<p>(and similar for lb2, with only node_name and bind_addr changed to lb2 and 10.0.0.2 respectively)</p>
<p>The ansible-consul role also creates a consul user and group, and an upstart configuration file like this:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/init/consul.conf</span></span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Consul Agent (Upstart unit)</span></span><br><span class="line">description <span class="string">"Consul Agent"</span></span><br><span class="line">start on (<span class="built_in">local</span>-filesystems and net-device-up IFACE!=lo)</span><br><span class="line">stop on runlevel [06]</span><br><span class="line"></span><br><span class="line"><span class="built_in">exec</span> sudo -u consul -g consul /opt/consul/bin/consul agent -config-dir /etc/consul.d -config-file=/etc/consul.conf &gt;&gt; /var/<span class="built_in">log</span>/consul 2&gt;&amp;1</span><br><span class="line">respawn</span><br><span class="line">respawn <span class="built_in">limit</span> 10 10</span><br><span class="line"><span class="built_in">kill</span> timeout 10</span><br><span class="line"></span><br><span class="line">To start/stop consul, I use:</span><br><span class="line"></span><br><span class="line"><span class="comment"># start consul</span></span><br><span class="line"><span class="comment"># stop consul</span></span><br></pre></td></tr></table></figure>
<p>Note that “server” is set to true and “bootstrap” is also set to true, which means that each consul server will be the leader of a cluster with 1 member, itself. To join the 2 servers into a consul cluster, I did the following:<br>join lb1 to lb2: on lb1 run consul join 10.0.0.2<br>tail /var/log/consul on lb1, note messages complaining about both consul servers (lb1 and lb2) running in bootstrap mode<br>stop consul on lb1: stop consul<br>edit /etc/consul.conf on lb1 and set  “bootstrap”: false<br>start consul on lb1: start consul<br>tail /var/log/consul on both lb1 and lb2; it should show no more errors<br>run consul info on both lb1 and lb2; the output should show server=true on both nodes, but leader=true only on lb2<br>Next I ran the consul agent in regular non-server mode on the 2 Wordpress nodes. The configuration file /etc/consul.cfg on node wordpress1 was:</p>
<p>{<br>  “domain”: “consul.”,<br>  “data_dir”: “/opt/consul/data”,<br>  “log_level”: “INFO”,<br>  “node_name”: “wordpress1”,<br>  “server”: false,<br>  “bind_addr”: “10.0.1.1”,<br>  “datacenter”: “us-west-1b”,<br>  “rejoin_after_leave”: true<br>}</p>
<p>(and similar for wordpress2, with the node_name set to wordpress2 and bind_addr set to 10.0.1.2)</p>
<p>After starting up the agents via upstart, I joined them to lb2 (although the could be joined to any of the existing members of the cluster). I ran this on both wordpress1 and wordpress2:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># consul join 10.0.0.2</span></span><br><span class="line"></span><br><span class="line">At this point, running consul members on any of the 4 nodes should show all 4 members of the cluster:</span><br><span class="line"></span><br><span class="line">Node          Address         Status  Type    Build  Protocol</span><br><span class="line">lb1           10.0.0.1:8301   alive   server  0.4.0  2</span><br><span class="line">wordpress2    10.0.1.2:8301   alive   client  0.4.0  2</span><br><span class="line">lb2           10.0.0.2:8301   alive   server  0.4.0  2</span><br><span class="line">wordpress1    10.0.1.1:8301   alive   client  0.4.0  2</span><br></pre></td></tr></table></figure></p>
<p>Install and run dnsmasq on all nodes</p>
<p>The ansible-consul role does this for you. Consul piggybacks on DNS resolution for service naming, and by default the domain names internal to Consul start with consul. In my case they are configured in consul.cfg via “domain”: “consul.”</p>
<p>The dnsmasq configuration file for consul is:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/dnsmasq.d/10-consul</span></span><br><span class="line"></span><br><span class="line">server=/consul./127.0.0.1<span class="comment">#8600</span></span><br></pre></td></tr></table></figure></p>
<p>This causes dnsmasq to provide DNS resolution for domain names starting with consul. by querying a DNS server on 127.0.0.1 running on port 8600 (which is the port the local consul agent listens on to provide DNS resolution).</p>
<p>To start/stop dnsmasq, use: service dnsmasq start | stop.</p>
<p>Now that dnsmasq is running, you can look up names that end in .node.consul from any member node of the consul cluster (there are 4 member nodes in my cluster, 2 servers and 2 agents). For example, I ran this on lb2:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ dig wordpress1.node.consul</span><br><span class="line"></span><br><span class="line">; &lt;&lt;&gt;&gt; DiG 9.8.1-P1 &lt;&lt;&gt;&gt; wordpress1.node.consul</span><br><span class="line">;; global options: +cmd</span><br><span class="line">;; Got answer:</span><br><span class="line">;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 2511</span><br><span class="line">;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0</span><br><span class="line"></span><br><span class="line">;; QUESTION SECTION:</span><br><span class="line">;wordpress1.node.consul.  IN A</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">wordpress1.node.consul. 0 IN A 10.0.1.1</span><br><span class="line"></span><br><span class="line">;; Query time: 1 msec</span><br><span class="line">;; SERVER: 127.0.0.1<span class="comment">#53(127.0.0.1)</span></span><br><span class="line">;; WHEN: Fri Nov 14 00:09:16 2014</span><br><span class="line">;; MSG SIZE  rcvd: 76</span><br></pre></td></tr></table></figure></p>
<p>Configure services and checks on consul agent nodes</p>
<p>Internal DNS resolution within the .consul domain becomes even more useful when nodes define services and checks. For example, the 2 Wordpress nodes run varnish and apache (on port 80 and port 443) so we can define 3 services as JSON files in /etc/consul.d. On wordpress1, which is our active/primary node in haproxy, I defined these services:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">$ cat http_service.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"service"</span>: &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"http"</span>,</span><br><span class="line">        <span class="string">"tags"</span>: [<span class="string">"primary"</span>],</span><br><span class="line">        <span class="string">"port"</span>:80,</span><br><span class="line">        <span class="string">"check"</span>: &#123;</span><br><span class="line">                <span class="string">"id"</span>: <span class="string">"http_check"</span>,</span><br><span class="line">                <span class="string">"name"</span>: <span class="string">"HTTP Health Check"</span>,</span><br><span class="line">   <span class="string">"script"</span>: <span class="string">"curl -H 'Host=www.mydomain.com' http://localhost"</span>,</span><br><span class="line">         <span class="string">"interval"</span>: <span class="string">"5s"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">$ cat ssl_service.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"service"</span>: &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"ssl"</span>,</span><br><span class="line">        <span class="string">"tags"</span>: [<span class="string">"primary"</span>],</span><br><span class="line">        <span class="string">"port"</span>:443,</span><br><span class="line">        <span class="string">"check"</span>: &#123;</span><br><span class="line">                <span class="string">"id"</span>: <span class="string">"ssl_check"</span>,</span><br><span class="line">                <span class="string">"name"</span>: <span class="string">"SSL Health Check"</span>,</span><br><span class="line">   <span class="string">"script"</span>: <span class="string">"curl -k -H 'Host=www.mydomain.com' https://localhost:443"</span>,</span><br><span class="line">         <span class="string">"interval"</span>: <span class="string">"5s"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">$ cat varnish_service.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"service"</span>: &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"varnish"</span>,</span><br><span class="line">        <span class="string">"tags"</span>: [<span class="string">"primary"</span>],</span><br><span class="line">        <span class="string">"port"</span>:6081 ,</span><br><span class="line">        <span class="string">"check"</span>: &#123;</span><br><span class="line">                <span class="string">"id"</span>: <span class="string">"varnish_check"</span>,</span><br><span class="line">                <span class="string">"name"</span>: <span class="string">"Varnish Health Check"</span>,</span><br><span class="line">   <span class="string">"script"</span>: <span class="string">"curl http://localhost:6081"</span>,</span><br><span class="line">         <span class="string">"interval"</span>: <span class="string">"5s"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Each service we defined has a name, a port and a check with its own ID, name, script that runs whenever the check is executed, and an interval that specifies how often the check is run. In the examples above I specified simple curl commands against the ports that these services are running on. Note also that each service has a list of tags associated with it. In my case, the services on wordpress1 have the tag “primary”. The services defined on wordpress2 are identical to the ones on wordpress1 with the only difference being the tag, which on wordpress2 is “backup”.</p>
<p>After restarting consul on wordpress1 and wordpress2, the following service-related DNS names are available for resolution on all nodes in the consul cluster (I am going to include only relevant portions of the dig output):<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ dig varnish.service.consul</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">varnish.service.consul. 0 IN A 10.0.1.1</span><br><span class="line">varnish.service.consul. 0 IN A 10.0.1.2</span><br></pre></td></tr></table></figure></p>
<p>This name resolves in DNS round-robin fashion to the IP addresses of all nodes that are running the varnish service, regardless of their tags and regardless of the data centers that their nodes run in. In our case, it resolves to the IP addresses of wordpress1 and wordpress2.</p>
<p>Note that the IP address of a given node only appears in the DNS result set if the service running on that node has a healty check. If the check fails, then consul’s DNS service will not include the IP of the node in the result set. This is very important for the dynamic discovery of healthy services.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ dig varnish.service.us-west-1b.consul</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">varnish.service.us-west-1b.consul. 0 IN A 10.0.1.2</span><br><span class="line">varnish.service.us-west-1b.consul. 0 IN A 10.0.1.1</span><br></pre></td></tr></table></figure></p>
<p>If we include the data center (in our case us-west-1b) in the DNS name we query, then only the services running on nodes in that data center will be returned in the result set. In our case though, all nodes run in the us-west-1b data center, so this query returns, like the previous one, the IP addresses of wordpress1 and wordpress2. Note that the IPs can be returned in any order, because of DNS round-robin. In this case the IP of wordpress2 was first.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ dig SRV varnish.service.consul</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">varnish.service.consul. 0 IN SRV 1 1 6081 wordpress1.node.us-west-1b.consul.</span><br><span class="line">varnish.service.consul. 0 IN SRV 1 1 6081 wordpress2.node.us-west-1b.consul.</span><br><span class="line"></span><br><span class="line">;; ADDITIONAL SECTION:</span><br><span class="line">wordpress1.node.us-west-1b.consul. 0 IN A 10.0.1.1</span><br><span class="line">wordpress2.node.us-west-1b.consul. 0 IN A 10.0.1.2</span><br></pre></td></tr></table></figure></p>
<p>A useful feature of the consul DNS service is that it returns the port number that a given service runs on when queried for an SRV record. So this query returns the names and IPs of the nodes that the varnish service runs on, as well as the port number, which in this case is 6081. The application querying for the SRV record needs to interpret this extra piece of information, but this is very useful for the discovery of internal services that might run on non-standard port numbers.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ dig primary.varnish.service.consul</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">primary.varnish.service.consul. 0 IN A 10.0.1.1</span><br><span class="line"></span><br><span class="line">$ dig backup.varnish.service.consul</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">backup.varnish.service.consul. 0 IN A 10.0.1.2</span><br></pre></td></tr></table></figure></p>
<p>The 2 DNS queries above show that it’s possible to query a service by its tag, in our case ‘primary’ vs. ‘backup’. The result set will contain the IP addresses of the nodes tagged with the specific tag and running the specific service we asked for. This feature will prove useful when dealing with consul-template in haproxy, as I’ll show later in this post.</p>
<p>Load balance across services</p>
<p>It’s easy now to see how an application can take advantage of the internal DNS service provided by consul and load balance across services. For example, an application that needs to load balance across the 2 varnish services on wordpress1 and wordpress2 would use varnish.service.consul as the DNS name it talks to when it needs to hit varnish. Every time this DNS name is resolved, a random node from wordpress1 and wordpress2 is returned via the DNS round-robin mechanism. If varnish were to run on a non-standard port number, the application would need to issue a DNS request for the SRV record in order to obtain the port number as well as the IP address to hit.</p>
<p>Note that this method of load balancing has health checks built in. If the varnish health check fails on one of the nodes providing the varnish service, that node’s IP address will not be included in the DNS result set returned by the DNS query for that service.</p>
<p>Also note that the DNS query can be customized for the needs of the application, which can query for a specific data center, or a specific tag, as I showed in the examples above.</p>
<p>Force a node out of service</p>
<p>I am still looking for the best way to take nodes in and out of service for maintenance or other purposes. One way I found so far is to deregister a given service via the Consul HTTP API. Here is an example of a curl command that accomplishes that, executed on node wordpress1:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ curl -v http://localhost:8500/v1/agent/service/deregister/varnish</span><br><span class="line">* About to connect() to localhost port 8500 (<span class="comment">#0)</span></span><br><span class="line">*   Trying 127.0.0.1... connected</span><br><span class="line">&gt; GET /v1/agent/service/deregister/varnish HTTP/1.1</span><br><span class="line">&gt; User-Agent: curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3</span><br><span class="line">&gt; Host: localhost:8500</span><br><span class="line">&gt; Accept: */*</span><br><span class="line">&gt;</span><br><span class="line">&lt; HTTP/1.1 200 OK</span><br><span class="line">&lt; Date: Mon, 17 Nov 2014 19:01:06 GMT</span><br><span class="line">&lt; Content-Length: 0</span><br><span class="line">&lt; Content-Type: text/plain; charset=utf-8</span><br><span class="line">&lt;</span><br><span class="line">* Connection <span class="comment">#0 to host localhost left intact</span></span><br><span class="line">* Closing connection <span class="comment">#0</span></span><br></pre></td></tr></table></figure></p>
<p>The effect of this command is that the varnish service on node wordpress1 is ‘deregistered’, which for my purposes means ‘marked as down’. DNS queries for varnish.service.consul will only return the IP address of wordpress2:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ dig varnish.service.consul</span><br><span class="line"></span><br><span class="line">;; ANSWER SECTION:</span><br><span class="line">varnish.service.consul. 0 IN A 10.0.1.2</span><br></pre></td></tr></table></figure></p>
<p>We can also use the Consul HTTP API to verify that the varnish service does not appear in the list of active services on node wordpress1. We’ll use the /agent/services API call and we’ll save the output to a file called services.out, then we’ll use the jq tool to pretty-print the output:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ curl -v http://localhost:8500/v1/agent/services -o services.out</span><br><span class="line"></span><br><span class="line">$ jq . &lt;&lt;&lt; `cat services.out`</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"http"</span>: &#123;</span><br><span class="line">    <span class="string">"ID"</span>: <span class="string">"http"</span>,</span><br><span class="line">    <span class="string">"Service"</span>: <span class="string">"http"</span>,</span><br><span class="line">    <span class="string">"Tags"</span>: [</span><br><span class="line">      <span class="string">"primary"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"Port"</span>: 80</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"ssl"</span>: &#123;</span><br><span class="line">    <span class="string">"ID"</span>: <span class="string">"ssl"</span>,</span><br><span class="line">    <span class="string">"Service"</span>: <span class="string">"ssl"</span>,</span><br><span class="line">    <span class="string">"Tags"</span>: [</span><br><span class="line">      <span class="string">"primary"</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"Port"</span>: 443</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Note that only the http and ssl services are shown.</p>
<p>Force a node back in service</p>
<p>Again, I am still looking for the best way to mark as service as ‘up’ once it was marked as ‘down’. One way would be to register the service via the Consul HTTP API, and that requires issuing a POST request with the payload being the JSON configuration file for that service. Another way is to just restart the consul agent on the node in question. This will register the service that had been deregistered previously.</p>
<p>Install and configure consul-template</p>
<p>For the next few steps, I am going to show how to use consul-template in conjuction with consul for discovering services and configuring haproxy based on the discovered services.</p>
<p>I automated the installation and configuration of consul-template via an Ansible role that I put on Github, but I am going to discuss the main steps here. See also the instructions on the consul-template Github page.</p>
<p>In my Ansible role, I copy the consul-template binary to the target node (in my case the 2 haproxy nodes lb1 and lb2), then create a directory structure /opt/consul-template/{bin,config,templates}. The consul-template configuration file is /opt/consul-template/config/consul-template.cfg and it looks like this in my case:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ cat config/consul-template.cfg</span><br><span class="line">consul = <span class="string">"127.0.0.1:8500"</span></span><br><span class="line"></span><br><span class="line">template &#123;</span><br><span class="line">  <span class="built_in">source</span> = <span class="string">"/opt/consul-template/templates/haproxy.ctmpl"</span></span><br><span class="line">  destination = <span class="string">"/etc/haproxy/haproxy.cfg"</span></span><br><span class="line">  <span class="built_in">command</span> = <span class="string">"service haproxy restart"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>Note that consul-template needs to be able to talk a consul agent, which in my case is the local agent listening on port 8500. The template that consul-template maintains is defined in another file,  /opt/consul-template/templates/haproxy.ctmpl. What consul-template does is monitor changes to that file via changes to the services referenced in the file. Upon any such change, consul-template will generate a new target file based on the template and copy it to the destination file, which in my case is the haproxy config file /etc/haproxy/haproxy.cfg. Finally, consul-template will executed a command, which in my case is the restarting of the haproxy service.</p>
<p>Here is the actual template file for my haproxy config, which is written in the Go template format:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">$ cat /opt/consul-template/templates/haproxy.ctmpl</span><br><span class="line"></span><br><span class="line">global</span><br><span class="line">  <span class="built_in">log</span> 127.0.0.1   <span class="built_in">local</span>0</span><br><span class="line">  maxconn 4096</span><br><span class="line">  user haproxy</span><br><span class="line">  group haproxy</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  <span class="built_in">log</span>     global</span><br><span class="line">  mode    http</span><br><span class="line">  option  dontlognull</span><br><span class="line">  retries 3</span><br><span class="line">  option redispatch</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 50s</span><br><span class="line">  timeout server 50s</span><br><span class="line">  balance  roundrobin</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up application listeners here.</span></span><br><span class="line"></span><br><span class="line">frontend http</span><br><span class="line">  maxconn &#123;&#123;key <span class="string">"service/haproxy/maxconn"</span>&#125;&#125;</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:80</span><br><span class="line">  default_backend servers-http-varnish</span><br><span class="line"></span><br><span class="line">backend servers-http-varnish</span><br><span class="line">  balance            roundrobin</span><br><span class="line">  option httpchk GET /</span><br><span class="line">  option  httplog</span><br><span class="line">&#123;&#123;range service <span class="string">"primary.varnish"</span>&#125;&#125;</span><br><span class="line">    server &#123;&#123;.Node&#125;&#125; &#123;&#123;.Address&#125;&#125;:&#123;&#123;.Port&#125;&#125; weight 1 check port &#123;&#123;.Port&#125;&#125;</span><br><span class="line">&#123;&#123;end&#125;&#125;</span><br><span class="line">&#123;&#123;range service <span class="string">"backup.varnish"</span>&#125;&#125;</span><br><span class="line">    server &#123;&#123;.Node&#125;&#125; &#123;&#123;.Address&#125;&#125;:&#123;&#123;.Port&#125;&#125; backup weight 1 check port &#123;&#123;.Port&#125;&#125;</span><br><span class="line">&#123;&#123;end&#125;&#125;</span><br><span class="line"></span><br><span class="line">frontend https</span><br><span class="line">  maxconn            &#123;&#123;key <span class="string">"service/haproxy/maxconn"</span>&#125;&#125;</span><br><span class="line">  mode               tcp</span><br><span class="line">  <span class="built_in">bind</span>               0.0.0.0:443</span><br><span class="line">  default_backend    servers-https</span><br><span class="line"></span><br><span class="line">backend servers-https</span><br><span class="line">  mode               tcp</span><br><span class="line">  option             tcplog</span><br><span class="line">  balance            roundrobin</span><br><span class="line">&#123;&#123;range service <span class="string">"primary.ssl"</span>&#125;&#125;</span><br><span class="line">    server &#123;&#123;.Node&#125;&#125; &#123;&#123;.Address&#125;&#125;:&#123;&#123;.Port&#125;&#125; weight 1 check port &#123;&#123;.Port&#125;&#125;</span><br><span class="line">&#123;&#123;end&#125;&#125;</span><br><span class="line">&#123;&#123;range service <span class="string">"backup.ssl"</span>&#125;&#125;</span><br><span class="line">    server &#123;&#123;.Node&#125;&#125; &#123;&#123;.Address&#125;&#125;:&#123;&#123;.Port&#125;&#125; backup weight 1 check port &#123;&#123;.Port&#125;&#125;</span><br><span class="line">&#123;&#123;end&#125;&#125;</span><br></pre></td></tr></table></figure></p>
<p>To the trained eye, this looks like a regular haproxy configuration file, with the exception of the portions bolded above. These are Go template snippets which rely on a couple of template functions exposed by consul-template above and beyond what the Go templating language offers. Specifically, the key function queries a key stored in the Consul key/value store and outputs the value associated with that key (or an empty string if the value doesn’t exist). The service function queries a consul service by its DNS name and returns a result set used inside the range statement. The variables inside the result set can be inspected for properties such as Node, Address and Port, which correspond to the Consul service node name, IP address and port number for that particular service.</p>
<p>In my example above, I use the value of the key service/haproxy/maxconn as the value of maxconn. In the http-varnish backend, I used 2 sets of services names, primary.varnish and backup.varnish, because I wanted to differentiate in haproxy.cfg between the primary server (wordpress1 in my case) and the backup server (wordpress2). In the ssl backend, I did the same but with the ssl service.</p>
<p>Everything so far would work fine with the exception of the key/value pair represented by the key service/haproxy/maxconn. To define that pair, I used the Consul key/value store API (this can be run on any member of the Consul cluster):<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">$ cat <span class="built_in">set</span>_haproxy_maxconn.sh</span><br><span class="line"><span class="meta">#!/bin/bash</span><br><span class="line"></span></span><br><span class="line">MAXCONN=4000</span><br><span class="line"></span><br><span class="line">curl -X PUT <span class="_">-d</span> <span class="string">"<span class="variable">$MAXCONN</span>"</span> http://localhost:8500/v1/kv/service/haproxy/maxconn</span><br><span class="line"></span><br><span class="line">To verify that the value was <span class="built_in">set</span>, I used:</span><br><span class="line"></span><br><span class="line">$ cat query_consul_kv.sh</span><br><span class="line"><span class="meta">#!/bin/bash</span><br><span class="line"></span></span><br><span class="line">curl -v http://localhost:8500/v1/kv/?recurse</span><br><span class="line"></span><br><span class="line">$ ./query_consul_kv.sh</span><br><span class="line">* About to connect() to localhost port 8500 (<span class="comment">#0)</span></span><br><span class="line">*   Trying 127.0.0.1... connected</span><br><span class="line">&gt; GET /v1/kv/?recurse HTTP/1.1</span><br><span class="line">&gt; User-Agent: curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3</span><br><span class="line">&gt; Host: localhost:8500</span><br><span class="line">&gt; Accept: */*</span><br><span class="line">&gt;</span><br><span class="line">&lt; HTTP/1.1 200 OK</span><br><span class="line">&lt; Content-Type: application/json</span><br><span class="line">&lt; X-Consul-Index: 30563</span><br><span class="line">&lt; X-Consul-Knownleader: <span class="literal">true</span></span><br><span class="line">&lt; X-Consul-Lastcontact: 0</span><br><span class="line">&lt; Date: Mon, 17 Nov 2014 23:01:07 GMT</span><br><span class="line">&lt; Content-Length: 118</span><br><span class="line">&lt;</span><br><span class="line">* Connection <span class="comment">#0 to host localhost left intact</span></span><br><span class="line">* Closing connection <span class="comment">#0</span></span><br><span class="line">[&#123;<span class="string">"CreateIndex"</span>:10995,<span class="string">"ModifyIndex"</span>:30563,<span class="string">"LockIndex"</span>:0,<span class="string">"Key"</span>:<span class="string">"service/haproxy/maxconn"</span>,<span class="string">"Flags"</span>:0,<span class="string">"Value"</span>:<span class="string">"NDAwMA=="</span>&#125;]</span><br></pre></td></tr></table></figure></p>
<p>At this point, everything is ready for starting up the consul-template service (in Ubuntu), I did it via this Upstart configuration file:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat /etc/init/consul-template.conf</span></span><br><span class="line"><span class="comment"># Consul Template (Upstart unit)</span></span><br><span class="line">description <span class="string">"Consul Template"</span></span><br><span class="line">start on (<span class="built_in">local</span>-filesystems and net-device-up IFACE!=lo)</span><br><span class="line">stop on runlevel [06]</span><br><span class="line"></span><br><span class="line"><span class="built_in">exec</span> /opt/consul-template/bin/consul-template  -config=/opt/consul-template/config/consul-template.cfg &gt;&gt; /var/<span class="built_in">log</span>/consul-template 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line">respawn</span><br><span class="line">respawn <span class="built_in">limit</span> 10 10</span><br><span class="line"><span class="built_in">kill</span> timeout 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># start consul-template</span></span><br></pre></td></tr></table></figure></p>
<p>Once consul-template starts, it will peform the actions corresponding to the functions defined in the template file /opt/consul-template/templates/haproxy.ctmpl. In my case, it will query Consul for the value of the key service/haproxy/maxconn and for information about the 2 Consul services varnish.service and ssl.service. It will then save the generated file to /etc/haproxy/haproxy.cfg and it will restart the haproxy service. The relevant snippets from haproxy.cfg are:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">frontend http</span><br><span class="line">  maxconn 4000</span><br><span class="line">  <span class="built_in">bind</span> 0.0.0.0:80</span><br><span class="line">  default_backend servers-http</span><br><span class="line"></span><br><span class="line">backend servers-http</span><br><span class="line">  balance            roundrobin</span><br><span class="line">  option httpchk GET /</span><br><span class="line">  option  httplog</span><br><span class="line"></span><br><span class="line">    server wordpress1 10.0.1.1:6081 weight 1 check port 6081</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    server wordpress2 10.0.1.2:6081 backup weight 1 check port 6081</span><br><span class="line"></span><br><span class="line">and</span><br><span class="line"></span><br><span class="line">frontend https</span><br><span class="line">  maxconn            4000</span><br><span class="line">  mode               tcp</span><br><span class="line">  <span class="built_in">bind</span>               0.0.0.0:443</span><br><span class="line">  default_backend    servers-https</span><br><span class="line"></span><br><span class="line">backend servers-https</span><br><span class="line">  mode               tcp</span><br><span class="line">  option             tcplog</span><br><span class="line">  balance            roundrobin</span><br><span class="line"></span><br><span class="line">    server wordpress1 10.0.1.1:443 weight 1 check port 443</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    server wordpress2 10.0.1.2:443 backup weight 1 check port 443</span><br></pre></td></tr></table></figure></p>
<p>I’ve been running this as a test on lb2. I don’t consider my setup quite production-ready because I don’t have monitoring in place, and I also want to experiment with consul security tokens for better security. But this is a pattern that I think will work.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>I talked in the past about an <a href="http://agiletesting.blogspot.com/2013/12/ops-design-pattern-local-haproxy.html" target="_blank" re]]>
    </summary>
    
      <category term="consul" scheme="http://blog.djstudy.net/tags/consul/"/>
    
      <category term="distribution " scheme="http://blog.djstudy.net/categories/distribution/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Presto, Parquet & Airpal]]></title>
    <link href="http://blog.djstudy.net/2016/02/04/presto-parquet-airpal/"/>
    <id>http://blog.djstudy.net/2016/02/04/presto-parquet-airpal/</id>
    <published>2016-02-04T10:53:35.000Z</published>
    <updated>2016-02-04T11:43:10.000Z</updated>
    <content type="html"><![CDATA[<p>I came across a <a href="http://brandonharris.io/evaluating-big-data-performance-of-prestodb-and-parquet-on-s3-storage/" target="_blank" rel="external">blog post</a> from Brandon Harris recently where he discussed a credit card fraud detection project he’d been working on with a team at the University of Chicago. In the post he described how Presto and Parquet-formatted files had gone a long way to speeding up ad-hoc queries against a ~250GB dataset he’s working with.</p>
<p>Presto was born at Facebook and was <a href="https://github.com/facebook/presto" target="_blank" rel="external">open sourced</a> within a year of it’s inception. It’s a distributed query engine capable of running interactive queries against big data sources. There’s support for data sources such as Hive, Kafka, PostgreSQL, Redis and Cassandra <a href="https://prestodb.io/docs/current/connector.html" target="_blank" rel="external">among many others</a>. Netflix has blogged about their positive experiences with Presto on a <a href="http://techblog.netflix.com/2014/10/using-presto-in-our-big-data-platform.html" target="_blank" rel="external">10PB Data Warehouse</a> they’ve got that’s happily handling 2,500 ad-hoc queries a day.</p>
<p>In Brandon’s blog post there is a chart showing a query that’s executed in Hive against data stored in CSV format taking 130 seconds and then the same query run via Presto against data stored in Parquet format taking less than 5 seconds. I trust the measurements of his queries are accurate but what I’m interested in is what is involved in getting an environment up to run these sorts of queries.</p>
<p>As of this writing Bigtop’s <a href="https://issues.apache.org/jira/browse/BIGTOP-1561" target="_blank" rel="external">Presto support isn’t ready</a> (though <a href="https://github.com/apache/bigtop/pull/32/files" target="_blank" rel="external">pull requests</a> are being worked on) so to get an environment up and running locally I’ll have to perform some of the installation steps manually.</p>
<h2 id="Launching_a_Hadoop_Cluster_in_Docker_Containers"><a href="#Launching_a_Hadoop_Cluster_in_Docker_Containers" class="headerlink" title="Launching a Hadoop Cluster in Docker Containers"></a>Launching a Hadoop Cluster in Docker Containers</h2><p>This process begins with a fresh Ubuntu 15 installation acting as the host for Docker containers that a Hadoop cluster will live within. I discuss getting Ubuntu 15 ready to run Docker in my <a href="http://tech.marksblogg.com/hadoop-up-and-running.html#bigtop-run-a-dockerized-hadoop-cluster" target="_blank" rel="external">Hadoop Up and Running</a> blog post.</p>
<p>With Docker ready I’ll checkout the Bigtop git repository and launch Ubuntu 14.04-based containers (as of this writing this is the latest <a href="https://hub.docker.com/r/bigtop/deploy/tags/" target="_blank" rel="external">supported version</a> of Ubuntu on Intel-based systems).<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/apache/bigtop.git</span><br><span class="line">$ <span class="built_in">cd</span> bigtop/bigtop-deploy/vm/vagrant-puppet-docker/</span><br><span class="line">$ sudo docker pull bigtop/deploy:ubuntu-14.04</span><br><span class="line">$ vi vagrantconfig.yaml</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">docker:</span><br><span class="line">        memory_size: <span class="string">"4096"</span></span><br><span class="line">        image:  <span class="string">"bigtop/deploy:ubuntu-14.04"</span></span><br><span class="line"></span><br><span class="line">boot2docker:</span><br><span class="line">        memory_size: <span class="string">"4096"</span></span><br><span class="line">        number_cpus: <span class="string">"1"</span></span><br><span class="line"></span><br><span class="line">repo: <span class="string">"http://bigtop-repos.s3.amazonaws.com/releases/1.0.0/ubuntu/trusty/x86_64"</span></span><br><span class="line">distro: debian</span><br><span class="line">components: [hadoop, yarn, hive]</span><br><span class="line">namenode_ui_port: <span class="string">"50070"</span></span><br><span class="line">yarn_ui_port: <span class="string">"8088"</span></span><br><span class="line">hbase_ui_port: <span class="string">"60010"</span></span><br><span class="line"><span class="built_in">enable</span>_<span class="built_in">local</span>_repo: <span class="literal">false</span></span><br><span class="line">smoke_<span class="built_in">test</span>_components: [mapreduce, pig]</span><br><span class="line">jdk: <span class="string">"openjdk-7-jdk"</span></span><br></pre></td></tr></table></figure>
<p>While I was writing this blog post Bigtop 1.1 was <a href="https://github.com/apache/bigtop/commit/dbf098fe56246cf44ca8d9c2940f2c5c59df40de" target="_blank" rel="external">being cut</a> and the resources from their 1.1.0 endpoint were returning HTTP 403 messages so I’ve stuck with the 1.0.0 endpoints for now.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ./docker-hadoop.sh --create 3</span><br><span class="line">$ sudo vagrant ssh bigtop1</span><br></pre></td></tr></table></figure></p>
<h2 id="Getting_Hive_u2019s_Metastore_Up_and_Running"><a href="#Getting_Hive_u2019s_Metastore_Up_and_Running" class="headerlink" title="Getting Hive’s Metastore Up and Running"></a>Getting Hive’s Metastore Up and Running</h2><p>By default the Derby embedded database driver is enabled in the boilerplate Hive configurations provided by Bigtop. This driver can only allow one process at a time to connect to it. If you use it Hive’s Metastore server won’t communicate properly with Presto and you’ll get “does not exist” messages every time you try to access a table.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> track_metadata_csv;</span><br><span class="line">... Table hive.default.track_metadata_csv does not exist</span><br></pre></td></tr></table></figure></p>
<p>For this reason I’ve installed MySQL and used it as the data backend for Hive’s Metastore.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install \</span><br><span class="line">    libmysql-java \</span><br><span class="line">    mysql-server</span><br><span class="line">$ ln <span class="_">-s</span> /usr/share/java/mysql-connector-java.jar \</span><br><span class="line">        /usr/lib/hive/lib/mysql-connector-java.jar</span><br><span class="line">$ /etc/init.d/mysql start</span><br><span class="line">$ mysql -uroot -proot <span class="_">-e</span><span class="string">'CREATE DATABASE hcatalog;'</span></span><br></pre></td></tr></table></figure></p>
<p>When you install MySQL you’ll be prompted to set a root login and password. I’ve set both values to root in this example. Below is the Hive site configuration file.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/hive/conf/hive-site.xml</span><br></pre></td></tr></table></figure>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>bigtop1.docker<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mr<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://localhost/hcatalog?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.hwi.war.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/lib/hive/lib/hive-hwi.war<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>I then launched Hive and made sure the Metastore’s database exists.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hive</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> metastore_db;</span><br></pre></td></tr></table></figure>
<p>After that I closed out of Hive and launched it’s Metastore service in the background. You’ll see it binded to port 9083 if it’s running.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ hive --service metastore &amp;</span><br><span class="line">$ netstat -an | grep 9083</span><br></pre></td></tr></table></figure></p>
<h2 id="Getting_some_data_to_play_with"><a href="#Getting_some_data_to_play_with" class="headerlink" title="Getting some data to play with"></a>Getting some data to play with</h2><p>I need some data to play with in this exercise so I dumped the Million Song Dataset to CSV and imported it into HDFS</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install sqlite3</span><br><span class="line">$ wget -c http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/track_metadata.db</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sqlite3 track_metadata.db &lt;&lt;!</span><br><span class="line">.headers off</span><br><span class="line">.mode csv</span><br><span class="line">.output track_metadata.csv</span><br><span class="line">SELECT track_id,</span><br><span class="line">     artist_id,</span><br><span class="line">     artist_familiarity,</span><br><span class="line">     artist_hotttnesss,</span><br><span class="line">     duration,</span><br><span class="line">     year</span><br><span class="line">FROM songs;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -put \</span><br><span class="line">    track_metadata.csv \</span><br><span class="line">    /tmp/track_metadata.csv</span><br></pre></td></tr></table></figure>
<p>With the CSV file sitting in HDFS I’ll create a Hive table for it. Once that table is created I can create a second, Parquet-formatted table and import the data from the first table into the second.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hive</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> track_metadata_csv (</span><br><span class="line">    track_id            <span class="built_in">VARCHAR</span>(<span class="number">18</span>),</span><br><span class="line">    artist_id           <span class="built_in">VARCHAR</span>(<span class="number">18</span>),</span><br><span class="line">    artist_familiarity  <span class="built_in">DECIMAL</span>(<span class="number">16</span>, <span class="number">15</span>),</span><br><span class="line">    artist_hotttnesss   <span class="built_in">DECIMAL</span>(<span class="number">16</span>, <span class="number">15</span>),</span><br><span class="line">    <span class="keyword">duration</span>            <span class="built_in">DECIMAL</span>(<span class="number">12</span>, <span class="number">8</span>),</span><br><span class="line">    <span class="keyword">year</span>                <span class="built_in">SMALLINT</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span> <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> INPATH <span class="string">'/tmp/track_metadata.csv'</span></span><br><span class="line"><span class="keyword">INTO</span> <span class="keyword">TABLE</span> track_metadata_csv;</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> track_metadata_parquet (</span><br><span class="line">    track_id            <span class="built_in">VARCHAR</span>(<span class="number">18</span>),</span><br><span class="line">    artist_id           <span class="built_in">VARCHAR</span>(<span class="number">18</span>),</span><br><span class="line">    artist_familiarity  <span class="built_in">DECIMAL</span>(<span class="number">16</span>, <span class="number">15</span>),</span><br><span class="line">    artist_hotttnesss   <span class="built_in">DECIMAL</span>(<span class="number">16</span>, <span class="number">15</span>),</span><br><span class="line">    <span class="keyword">duration</span>            <span class="built_in">DECIMAL</span>(<span class="number">12</span>, <span class="number">8</span>),</span><br><span class="line">    <span class="keyword">year</span>                <span class="built_in">SMALLINT</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">STORED</span> <span class="keyword">AS</span> parquet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="keyword">TABLE</span> track_metadata_parquet</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> track_metadata_csv;</span><br></pre></td></tr></table></figure>
<p>I now have that table’s metadata stored in Hive’s Metastore and a Parquet-formatted file of that data sitting on HDFS:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -ls \</span><br><span class="line">    /user/hive/warehouse/track_metadata_parquet</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 root hadoop   45849172 2016-01-31 13:31 /user/hive/warehouse/track_metadata_parquet/000000_0</span><br><span class="line">$ mysqldump \</span><br><span class="line">    -uroot \</span><br><span class="line">    -proot \</span><br><span class="line">    --no-create-info \</span><br><span class="line">    --skip-add-locks \</span><br><span class="line">    --skip-disable-keys \</span><br><span class="line">    --skip-comments \</span><br><span class="line">    hcatalog | \</span><br><span class="line">    grep ^INSERT | \</span><br><span class="line">    sort</span><br></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`CDS`</span> <span class="keyword">VALUES</span> (<span class="number">1</span>),(<span class="number">2</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`COLUMNS_V2`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'artist_familiarity'</span>,<span class="string">'decimal(16,15)'</span>,<span class="number">2</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'artist_hotttnesss'</span>,<span class="string">'decimal(16,15)'</span>,<span class="number">3</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'artist_id'</span>,<span class="string">'varchar(18)'</span>,<span class="number">1</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'duration'</span>,<span class="string">'decimal(12,8)'</span>,<span class="number">4</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'track_id'</span>,<span class="string">'varchar(18)'</span>,<span class="number">0</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'year'</span>,<span class="string">'smallint'</span>,<span class="number">5</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'artist_familiarity'</span>,<span class="string">'decimal(16,15)'</span>,<span class="number">2</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'artist_hotttnesss'</span>,<span class="string">'decimal(16,15)'</span>,<span class="number">3</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'artist_id'</span>,<span class="string">'varchar(18)'</span>,<span class="number">1</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'duration'</span>,<span class="string">'decimal(12,8)'</span>,<span class="number">4</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'track_id'</span>,<span class="string">'varchar(18)'</span>,<span class="number">0</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'year'</span>,<span class="string">'smallint'</span>,<span class="number">5</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`DBS`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="string">'Default Hive database'</span>,<span class="string">'hdfs://bigtop1.docker:8020/user/hive/warehouse'</span>,<span class="string">'default'</span>,<span class="string">'public'</span>,<span class="string">'ROLE'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'hdfs://bigtop1.docker:8020/user/hive/warehouse/metastore_db.db'</span>,<span class="string">'metastore_db'</span>,<span class="string">'root'</span>,<span class="string">'USER'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`GLOBAL_PRIVS`</span> <span class="keyword">VALUES</span> (<span class="number">1</span>,<span class="number">1454255378</span>,<span class="number">1</span>,<span class="string">'admin'</span>,<span class="string">'ROLE'</span>,<span class="string">'admin'</span>,<span class="string">'ROLE'</span>,<span class="string">'All'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`ROLES`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="number">1454255378</span>,<span class="string">'admin'</span>,<span class="string">'admin'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="number">1454255378</span>,<span class="string">'public'</span>,<span class="string">'public'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`SDS`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="number">1</span>,<span class="string">'org.apache.hadoop.mapred.TextInputFormat'</span>,<span class="string">'\0'</span>,<span class="string">'\0'</span>,<span class="string">'hdfs://bigtop1.docker:8020/user/hive/warehouse/track_metadata_csv'</span>,<span class="number">-1</span>,<span class="string">'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'</span>,<span class="number">1</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="number">2</span>,<span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'</span>,<span class="string">'\0'</span>,<span class="string">'\0'</span>,<span class="string">'hdfs://bigtop1.docker:8020/user/hive/warehouse/track_metadata_parquet'</span>,<span class="number">-1</span>,<span class="string">'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'</span>,<span class="number">2</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`SEQUENCE_TABLE`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MColumnDescriptor'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MDatabase'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MGlobalPrivilege'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MRole'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MSerDeInfo'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MStorageDescriptor'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MTable'</span>,<span class="number">6</span>),</span><br><span class="line">    (<span class="string">'org.apache.hadoop.hive.metastore.model.MVersionTable'</span>,<span class="number">6</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`SERDES`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="literal">NULL</span>,<span class="string">'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="literal">NULL</span>,<span class="string">'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`SERDE_PARAMS`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="string">'field.delim'</span>,<span class="string">','</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="string">'serialization.format'</span>,<span class="string">','</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'serialization.format'</span>,<span class="string">'1'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`TABLE_PARAMS`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="string">'COLUMN_STATS_ACCURATE'</span>,<span class="string">'true'</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="string">'numFiles'</span>,<span class="string">'2'</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="string">'numRows'</span>,<span class="string">'0'</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="string">'rawDataSize'</span>,<span class="string">'0'</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="string">'totalSize'</span>,<span class="string">'161179824'</span>),</span><br><span class="line">    (<span class="number">1</span>,<span class="string">'transient_lastDdlTime'</span>,<span class="string">'1454255466'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'COLUMN_STATS_ACCURATE'</span>,<span class="string">'true'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'numFiles'</span>,<span class="string">'2'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'numRows'</span>,<span class="string">'2000000'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'rawDataSize'</span>,<span class="string">'12000000'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'totalSize'</span>,<span class="string">'136579903'</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="string">'transient_lastDdlTime'</span>,<span class="string">'1454255498'</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`TBLS`</span> <span class="keyword">VALUES</span></span><br><span class="line">    (<span class="number">1</span>,<span class="number">1454255462</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="string">'root'</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">'track_metadata_csv'</span>,<span class="string">'MANAGED_TABLE'</span>,<span class="literal">NULL</span>,<span class="literal">NULL</span>),</span><br><span class="line">    (<span class="number">2</span>,<span class="number">1454255471</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="string">'root'</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="string">'track_metadata_parquet'</span>,<span class="string">'MANAGED_TABLE'</span>,<span class="literal">NULL</span>,<span class="literal">NULL</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> <span class="string">`VERSION`</span> <span class="keyword">VALUES</span> (<span class="number">1</span>,<span class="string">'0.14.0'</span>,<span class="string">'Set by MetaStore'</span>);</span><br></pre></td></tr></table></figure>
<h2 id="Presto_Up_and_Running"><a href="#Presto_Up_and_Running" class="headerlink" title="Presto Up and Running"></a>Presto Up and Running</h2><p>Presto requires Java 8 so I’ll install that first.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ add-apt-repository ppa:webupd8team/java</span><br><span class="line">$ apt-get update</span><br><span class="line">$ apt-get install oracle-java8-installer</span><br></pre></td></tr></table></figure></p>
<p>I have yet to see recent Debian packages for Presto so I’ll download the binaries instead.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~</span><br><span class="line">$ wget -c https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.133/presto-server-0.133.tar.gz</span><br><span class="line">$ tar xzf presto-server-0.133.tar.gz</span><br></pre></td></tr></table></figure></p>
<p>Presto requires a data folder for it to store locks, logs and a few other items and also requires a number of configuration files before it can begin to work properly.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p /root/datap</span><br><span class="line">$ mkdir -p ~/presto-server-0.133/etc/catalog</span><br><span class="line">$ <span class="built_in">cd</span> ~/presto-server-0.133/etc</span><br></pre></td></tr></table></figure></p>
<p>Normally I wouldn’t suggest creating a data folder in the root partition but this work is all taking place in a Docker container that has exposed 13GB of space on the root partition.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ df -H</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">none             13G  6.4G  5.6G  54% /</span><br><span class="line">tmpfs           4.2G  8.2k  4.2G   1% /dev</span><br><span class="line">tmpfs           4.2G     0  4.2G   0% /sys/fs/cgroup</span><br><span class="line">/dev/sda1        13G  6.4G  5.6G  54% /vagrant</span><br><span class="line">shm              68M     0   68M   0% /dev/shm</span><br></pre></td></tr></table></figure></p>
<p>Next I need to create six configuration files. Below is an outline of where they live within the ~/presto-server-0.133/etc folder:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ tree ~/presto-server-0.133/etc</span><br><span class="line">etc</span><br><span class="line">|-- catalog</span><br><span class="line">|   |-- hive.properties</span><br><span class="line">|   `-- jmx.properties</span><br><span class="line">|-- config.properties</span><br><span class="line">|-- jvm.config</span><br><span class="line">|-- log.properties</span><br><span class="line">`-- node.properties</span><br></pre></td></tr></table></figure></p>
<p>Here are the commands to set the contents on each of the configuration files.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/presto-server-0.133/etc/config.properties</span><br></pre></td></tr></table></figure></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">coordinator=<span class="literal">true</span></span><br><span class="line">node-scheduler.include-coordinator=<span class="literal">true</span></span><br><span class="line">http-server.http.port=8080</span><br><span class="line">query.max-memory=800MB</span><br><span class="line">query.max-memory-per-node=200MB</span><br><span class="line">discovery-server.enabled=<span class="literal">true</span></span><br><span class="line">discovery.uri=http://127.0.0.1:8080</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/presto-server-0.133/etc/jvm.config</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-server</span><br><span class="line">-Xmx800M</span><br><span class="line">-XX:+UseG1GC</span><br><span class="line">-XX:G1HeapRegionSize=32M</span><br><span class="line">-XX:+UseGCOverheadLimit</span><br><span class="line">-XX:+ExplicitGCInvokesConcurrent</span><br><span class="line">-XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line">-XX:OnOutOfMemoryError=<span class="built_in">kill</span> -9 %p</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/presto-server-0.133/etc/log.properties</span><br><span class="line">com.facebook.presto=INFO</span><br><span class="line">$ vi ~/presto-server-0.133/etc/node.properties</span><br><span class="line">node.environment=dev</span><br><span class="line">node.id=ffffffff-ffff-ffff-ffff-ffffffffffff</span><br><span class="line">node.data-dir=/root/datap</span><br></pre></td></tr></table></figure>
<p>The JMX connector provides the ability to query Java Management Extensions (JMX) information from all nodes in a Presto cluster. Presto itself is heavily instrumented via JMX.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/presto-server-0.133/etc/catalog/jmx.properties</span><br><span class="line">connector.name=jmx</span><br></pre></td></tr></table></figure></p>
<p>This file allows Presto to know where our Hive Metastore is and which connector to use to communicate with it.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ vi ~/presto-server-0.133/etc/catalog/hive.properties</span><br><span class="line">hive.metastore.uri=thrift://bigtop1.docker:9083</span><br><span class="line">connector.name=hive-hadoop2</span><br></pre></td></tr></table></figure></p>
<p>With those in place you can launch Presto’s server.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ~/presto-server-0.133/bin/launcher start</span><br></pre></td></tr></table></figure></p>
<p>It should expose a web frontend on port 8080 if it’s running properly.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl localhost:8080</span><br></pre></td></tr></table></figure></p>
<h2 id="Presto_u2019s_CLI_Up_and_Running"><a href="#Presto_u2019s_CLI_Up_and_Running" class="headerlink" title="Presto’s CLI Up and Running"></a>Presto’s CLI Up and Running</h2><p>I’ll download the CLI JAR file for Presto, rename it to presto and then I can use it to connect to the Presto Server.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~</span><br><span class="line">$ wget -c https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.133/presto-cli-0.133-executable.jar</span><br><span class="line">$ mv presto-cli-0.133-executable.jar presto</span><br><span class="line">$ chmod +x presto</span><br><span class="line">$ ./presto --server localhost:8080 --catalog hive --schema default</span><br></pre></td></tr></table></figure></p>
<p>I’ll then see if Presto can see the schemas in the Hive Metastore.</p>
<p>show schemas from hive;</p>
<pre><code>Schema
</code></pre><hr>
<p> default<br> information_schema<br> metastore_db<br>(3 rows)</p>
<h2 id="Executing_Queries_in_Presto"><a href="#Executing_Queries_in_Presto" class="headerlink" title="Executing Queries in Presto"></a>Executing Queries in Presto</h2><p>With the CLI communicating with the server properly I’ll run two queries. The first will count how many records per year exist in our million song database using the data in the CSV-backed table and the second will do the same against the Parquet-backed table.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">year</span>, <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> track_metadata_csv</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">year</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">year</span>;</span><br><span class="line"> year | _col1</span><br><span class="line"><span class="comment">------+--------</span></span><br><span class="line">    0 | 968848</span><br><span class="line"> 1922 |     12</span><br><span class="line"> 1924 |     10</span><br><span class="line"> ...</span><br><span class="line"> 2008 |  69540</span><br><span class="line"> 2009 |  62102</span><br><span class="line"> 2010 |  18794</span><br><span class="line"> 2011 |      2</span><br><span class="line">(90 rows)</span><br><span class="line"></span><br><span class="line">Query 20160131_160009_00002_s4z65, FINISHED, 1 node</span><br><span class="line">Splits: 6 total, 6 done (100.00%)</span><br><span class="line">0:09 [2M rows, 154MB] [221K rows/s, 17MB/s]</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">year</span>, <span class="keyword">count</span>(*)</span><br><span class="line"><span class="keyword">FROM</span> track_metadata_parquet</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">year</span></span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> <span class="keyword">year</span>;</span><br><span class="line"> year | _col1</span><br><span class="line"><span class="comment">------+--------</span></span><br><span class="line">    0 | 968848</span><br><span class="line"> 1922 |     12</span><br><span class="line"> 1924 |     10</span><br><span class="line"> ...</span><br><span class="line"> 2008 |  69540</span><br><span class="line"> 2009 |  62102</span><br><span class="line"> 2010 |  18794</span><br><span class="line"> 2011 |      2</span><br><span class="line">(90 rows)</span><br><span class="line"></span><br><span class="line">Query 20160131_160411_00005_s4z65, FINISHED, 1 node</span><br><span class="line">Splits: 5 total, 5 done (100.00%)</span><br><span class="line">0:04 [3M rows, 87MB] [799K rows/s, 23.2MB/s]</span><br></pre></td></tr></table></figure></p>
<p>The query finished in 9 seconds with the CSV-formatted data and in 4 seconds with the Parquet-formatted data. I grepped the server log to see the time lines recorded by the Query Monitor.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ grep 20160131_160009_00002_s4z65 ~/datap/var/<span class="built_in">log</span>/server.log</span><br><span class="line">...elapsed 3886.00ms :: planning 644.26ms :: scheduling 1099.00ms :: running 2094.00ms :: finishing 49.00ms...</span><br><span class="line"></span><br><span class="line">$ grep 20160131_160411_00005_s4z65 ~/datap/var/<span class="built_in">log</span>/server.log</span><br><span class="line">...elapsed 1406.00ms :: planning 91.68ms :: scheduling 55.00ms :: running 1221.00ms :: finishing 38.00ms...</span><br></pre></td></tr></table></figure></p>
<p>I’m not surprised that the columnar-based, Parquet-format-backed query was twice as fast but I’m not sure how the timings the Query Monitor recorded relate to the times reported by the CLI. It gives me something to dig into at a later point.</p>
<p>I took a look at the query plans to see if they offer anything of interest. In the case of these two queries they were identical bar the table names.<br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN</span><br><span class="line">SELECT year, count(*)</span><br><span class="line">FROM track_metadata_csv</span><br><span class="line">GROUP BY year</span><br><span class="line">ORDER BY year;</span><br><span class="line">                                                                  </span><br><span class="line">----------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> - Output[year, _col1] =&gt; [year:bigint, count:bigint]</span><br><span class="line">         _col1 := count</span><br><span class="line">     - Sort[year ASC_NULLS_LAST] =&gt; [year:bigint, count:bigint]</span><br><span class="line">         - Exchange[GATHER] =&gt; year:bigint, count:bigint</span><br><span class="line">             - Aggregate(FINAL)[year] =&gt; [year:bigint, count:bigint]</span><br><span class="line">                     count := "count"("count_9")</span><br><span class="line">                 - Exchange[REPARTITION] =&gt; year:bigint, count_9:bigint</span><br><span class="line">                     - Aggregate(PARTIAL)[year] =&gt; [year:bigint, count_9:bigint]</span><br><span class="line">                             count_9 := "count"(*)</span><br><span class="line">                         - TableScan[hive:hive:default:track_metadata_csv, originalConstraint = true] =&gt; [year:bigint]</span><br><span class="line">                                 LAYOUT: hive</span><br><span class="line">                                 year := HiveColumnHandle&#123;clientId=hive, name=year, hiveType=smallint, hiveColumnIndex=5, partitionKey=false&#125;</span><br><span class="line">EXPLAIN</span><br><span class="line">SELECT year, count(*)</span><br><span class="line">FROM track_metadata_parquet</span><br><span class="line">GROUP BY year</span><br><span class="line">ORDER BY year;</span><br><span class="line">                                                                  Query Plan</span><br><span class="line">----------------------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line"> - Output[year, _col1] =&gt; [year:bigint, count:bigint]</span><br><span class="line">         _col1 := count</span><br><span class="line">     - Sort[year ASC_NULLS_LAST] =&gt; [year:bigint, count:bigint]</span><br><span class="line">         - Exchange[GATHER] =&gt; year:bigint, count:bigint</span><br><span class="line">             - Aggregate(FINAL)[year] =&gt; [year:bigint, count:bigint]</span><br><span class="line">                     count := "count"("count_9")</span><br><span class="line">                 - Exchange[REPARTITION] =&gt; year:bigint, count_9:bigint</span><br><span class="line">                     - Aggregate(PARTIAL)[year] =&gt; [year:bigint, count_9:bigint]</span><br><span class="line">                             count_9 := "count"(*)</span><br><span class="line">                         - TableScan[hive:hive:default:track_metadata_parquet, originalConstraint = true] =&gt; [year:bigint]</span><br><span class="line">                                 LAYOUT: hive</span><br><span class="line">                                 year := HiveColumnHandle&#123;clientId=hive, name=year, hiveType=smallint, hiveColumnIndex=5, partitionKey=false&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="Airpal_3A_A_Web_Interface_for_Presto"><a href="#Airpal_3A_A_Web_Interface_for_Presto" class="headerlink" title="Airpal: A Web Interface for Presto"></a>Airpal: A Web Interface for Presto</h2><p>In March 2015 AirBNB announced Airpal, a web-based query tool that works with Presto. Beyond a visual interface to run queries it offers access controls, metadata exploration, query progress tracking and CSV exporting of results.</p>
<p>These are the steps I took to install this software and launch it within the same Docker container as Presto.</p>
<p>The first thing I had to do was to get a copy of Airpal’s git repository and build the Airpal JAR file.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ apt-get install \</span><br><span class="line">    build-essential \</span><br><span class="line">    git \</span><br><span class="line">    gradle \</span><br><span class="line">    mysql-server</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/airbnb/airpal.git</span><br><span class="line">$ <span class="built_in">cd</span> airpal</span><br><span class="line">$ ./gradlew clean shadowJar</span><br></pre></td></tr></table></figure></p>
<p>I then created a MySQL database for Airpal to store it’s configuration and logs.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ mysql \</span><br><span class="line">    -uroot \</span><br><span class="line">    -proot \</span><br><span class="line">    <span class="_">-e</span><span class="string">'CREATE DATABASE `airpal` CHARACTER SET utf8 COLLATE utf8_general_ci;'</span></span><br></pre></td></tr></table></figure></p>
<p>Airpal needs a configuration file to tell it’s JAR file how to, among other things, connect with the MySQL data backend.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vi reference.yml</span><br></pre></td></tr></table></figure></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># Logging settings</span><br><span class="line">logging:</span><br><span class="line"></span><br><span class="line">  loggers:</span><br><span class="line">    org.apache.shiro: INFO</span><br><span class="line"></span><br><span class="line">  # The default level of all loggers. Can be OFF, ERROR, WARN, INFO, DEBUG, TRACE, or ALL.</span><br><span class="line">  level: INFO</span><br><span class="line"></span><br><span class="line"># HTTP-specific options.</span><br><span class="line">server:</span><br><span class="line">  applicationConnectors:</span><br><span class="line">    - type: http</span><br><span class="line">      port: 8081</span><br><span class="line">      idleTimeout: 10 seconds</span><br><span class="line"></span><br><span class="line">  adminConnectors:</span><br><span class="line">    - type: http</span><br><span class="line">      port: 8082</span><br><span class="line"></span><br><span class="line">shiro:</span><br><span class="line">  iniConfigs: ["classpath:shiro_allow_all.ini"]</span><br><span class="line"></span><br><span class="line">dataSourceFactory:</span><br><span class="line">  driverClass: com.mysql.jdbc.Driver</span><br><span class="line">  user: root</span><br><span class="line">  password: root</span><br><span class="line">  url: jdbc:mysql://127.0.0.1:3306/airpal</span><br><span class="line"></span><br><span class="line"># The URL to the Presto coordinator.</span><br><span class="line">prestoCoordinator: http://127.0.0.1:8080</span><br></pre></td></tr></table></figure>
<p>With that in place I setup Airpal’s database schema and launched the server in the background.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ java -Duser.timezone=UTC \</span><br><span class="line">       -cp build/libs/airpal-*-all.jar \</span><br><span class="line">       com.airbnb.airpal.AirpalApplication \</span><br><span class="line">       db migrate reference.yml</span><br><span class="line">$ java -server \</span><br><span class="line">       -Duser.timezone=UTC \</span><br><span class="line">       -cp build/libs/airpal-*-all.jar \</span><br><span class="line">       com.airbnb.airpal.AirpalApplication \</span><br><span class="line">       server reference.yml &amp;</span><br></pre></td></tr></table></figure></p>
<p>Their is a health check endpoint on port 8082.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ curl <span class="_">-s</span> http://127.0.0.1:8082/healthcheck | \</span><br><span class="line">    python -m json.tool</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"deadlocks"</span>: &#123;</span><br><span class="line">        <span class="string">"healthy"</span>: <span class="literal">true</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"mysql"</span>: &#123;</span><br><span class="line">        <span class="string">"healthy"</span>: <span class="literal">true</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"presto"</span>: &#123;</span><br><span class="line">        <span class="string">"healthy"</span>: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>If that all looks well the web interface is configured to be exposed on port 8081.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://127.0.0.1:8081/app</span><br></pre></td></tr></table></figure></p>
<p>Thank you for taking the time to read this post. If you’re considering using Digital Ocean, the hosting provider this blog is hosted on, please consider using this link to sign up.<br>转 <a href="http://tech.marksblogg.com/presto-parquet-airpal.html" target="_blank" rel="external">http://tech.marksblogg.com/presto-parquet-airpal.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>I came across a <a href="http://brandonharris.io/evaluating-big-data-performance-of-prestodb-and-parquet-on-s3-storage/" target="_blank" ]]>
    </summary>
    
      <category term="presto" scheme="http://blog.djstudy.net/tags/presto/"/>
    
      <category term="parquet" scheme="http://blog.djstudy.net/tags/parquet/"/>
    
      <category term="big data" scheme="http://blog.djstudy.net/categories/big-data/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark Sort Based Shuffle内存分析]]></title>
    <link href="http://blog.djstudy.net/2016/01/27/spark-sort-shuffle-analyse/"/>
    <id>http://blog.djstudy.net/2016/01/27/spark-sort-shuffle-analyse/</id>
    <published>2016-01-27T09:51:18.000Z</published>
    <updated>2016-01-27T12:00:22.000Z</updated>
    <content type="html"><![CDATA[<h2 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h2><p>借用和董神的一段对话说下背景：</p>
<p>shuffle共有三种，别人讨论的是hash shuffle，这是最原始的实现，曾经有两个版本，第一版是每个map产生r个文件，一共产生mr个文件，由于产生的中间文件太大影响扩展性，社区提出了第二个优化版本，让一个core上map共用文件，减少文件数目，这样共产生corer个文件，好多了，但中间文件数目仍随任务数线性增加，仍难以应对大作业，但hash shuffle已经优化到头了。为了解决hash shuffle性能差的问题，又引入sort shuffle，完全借鉴mapreduce实现，每个map产生一个文件，彻底解决了扩展性问题<br>目前Sort Based Shuffle 是作为默认Shuffle类型的。Shuffle 是一个很复杂的过程，任何一个环节都足够写一篇文章。所以这里，我尝试换个方式，从实用的角度出发，让读者有两方面的收获：</p>
<p>剖析哪些环节，哪些代码可能会让内存产生问题<br>控制相关内存的参数<br>有时候，我们宁可程序慢点，也不要OOM，至少要先跑步起来，希望这篇文章能够让你达成这个目标。</p>
<p>同时我们会提及一些类名，这些类方便你自己想更深入了解时，可以方便的找到他们，自己去探个究竟。</p>
<h2 id="Shuffle__u6982_u89C8"><a href="#Shuffle__u6982_u89C8" class="headerlink" title="Shuffle 概览"></a>Shuffle 概览</h2><p>Spark 的Shuffle 分为 Write,Read 两阶段。我们预先建立三个概念：</p>
<p>Write 对应的是ShuffleMapTask,具体的写操作ExternalSorter来负责</p>
<p>Read 阶段由ShuffleRDD里的HashShuffleReader来完成。如果拉来的数据如果过大，需要落地，则也由ExternalSorter来完成的</p>
<p>所有Write 写完后，才会执行Read。 他们被分成了两个不同的Stage阶段。</p>
<p>也就是说，Shuffle Write ,Shuffle Read 两阶段都可能需要落磁盘，并且通过Disk Merge 来完成最后的Sort归并排序。</p>
<h2 id="Shuffle_Write__u5185_u5B58_u6D88_u8017_u5206_u6790"><a href="#Shuffle_Write__u5185_u5B58_u6D88_u8017_u5206_u6790" class="headerlink" title="Shuffle Write 内存消耗分析"></a>Shuffle Write 内存消耗分析</h2><p>Shuffle Write 的入口链路为：</p>
<blockquote>
<p>org.apache.spark.scheduler.ShuffleMapTask<br> —&gt; org.apache.spark.shuffle.sort.SortShuffleWriter<br>  —&gt; org.apache.spark.util.collection.ExternalSorter</p>
</blockquote>
<p>会产生内存瓶颈的其实就是 org.apache.spark.util.collection.ExternalSorter。我们看看这个复杂的ExternalSorter都有哪些地方在占用内存：</p>
<p>第一个地：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> map = <span class="keyword">new</span> <span class="type">PartitionedAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br></pre></td></tr></table></figure>
<p>我们知道，数据都是先写内存，内存不够了，才写磁盘。这里的map就是那个放数据的内存了。</p>
<p>这个PartitionedAppendOnlyMap内部维持了一个数组，是这样的：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> data = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">AnyRef</span>](<span class="number">2</span> * capacity)</span><br></pre></td></tr></table></figure></p>
<p>也就是他消耗的并不是Storage的内存，所谓Storage内存，指的是由blockManager管理起来的内存。</p>
<p>PartitionedAppendOnlyMap 放不下，要落地，那么不能硬生生的写磁盘，所以需要个buffer,然后把buffer再一次性写入磁盘文件。这个buffer是由参数</p>
<p>spark.shuffle.file.buffer=32k<br>控制的。数据获取的过程中，序列化反序列化，也是需要空间的，所以Spark 对数量做了限制，通过如下参数控制：</p>
<p> spark.shuffle.spill.batchSize=10000<br>假设一个Executor的可使用的Core为 C个，那么对应需要的内存消耗为：</p>
<p> C <em> 32k + C </em> 10000个Record + C <em> PartitionedAppendOnlyMap<br>这么看来，写文件的buffer不是问题，而序列化的batchSize也不是问题，几万或者十几万个Record 而已。那C </em> PartitionedAppendOnlyMap 到底会有多大呢？我先给个结论:</p>
<p>   C <em> PartitionedAppendOnlyMap &lt; ExecutorHeapMemeory </em> 0.2 * 0.8<br>怎么得到上面的结论呢？核心店就是要判定PartitionedAppendOnlyMap 需要占用多少内存，而它到底能占用内存，则由触发写磁盘动作决定，因为一旦写磁盘，PartitionedAppendOnlyMap所占有的内存就会被释放。下面是判断是否写磁盘的逻辑代码：</p>
<blockquote>
<p>estimatedSize = map.estimateSize()<br>if (maybeSpill(map, estimatedSize)) {<br>        map = new PartitionedAppendOnlyMap[K, C]<br>}</p>
</blockquote>
<p>每放一条记录，就会做一次内存的检查，看PartitionedAppendOnlyMap 到底占用了多少内存。如果真是这样，假设检查一次内存1ms, 1kw 就不得了的时间了。所以肯定是不行的，所以 estimateSize其实是使用采样算法来做的。</p>
<p>第二个，我们也不希望mayBeSpill太耗时,所以 maybeSpill 方法里就搞了很多东西，减少耗时。我们看看都设置了哪些防线</p>
<p>首先会判定要不要执行内部逻辑：</p>
<p>   elementsRead % 32 == 0 &amp;&amp; currentMemory &gt;= myMemoryThreshold<br>每隔32次会进行一次检查，并且要当前PartitionedAppendOnlyMap currentMemory &gt; myMemoryThreshold 才会进一步判定是不是要spill.</p>
<p>其中 myMemoryThreshold可通过如下配置获得初始值</p>
<p>spark.shuffle.spill.initialMemoryThreshold =  5 <em> 1024 </em> 1024<br>接着会向 shuffleMemoryManager 要 2 * currentMemory - myMemoryThreshold 的内存，shuffleMemoryManager 是被Executor 所有正在运行的Task(Core) 共享的，能够分配出去的内存是：</p>
<p>ExecutorHeapMemeory <em> 0.2 </em> 0.8<br>上面的数字可通过下面两个配置来更改：</p>
<p>spark.shuffle.memoryFraction=0.2<br>spark.shuffle.safetyFraction=0.8<br>如果无法获取到足够的内存，就会触发真的spill操作了。</p>
<p>看到这里，上面的结论就显而易见了。</p>
<p>然而，这里我们忽略了一个很大的问题，就是</p>
<p> estimatedSize = map.estimateSize()<br>为什么说它是大问题，前面我们说了，estimateSize 是近似估计，所以有可能估的不准，也就是实际内存会远远超过预期。</p>
<p>具体的大家可以看看 org.apache.spark.util.collection.SizeTracker</p>
<p>我这里给出一个结论：</p>
<p>如果你内存开的比较大，其实反倒风险更高，因为estimateSize 并不是每次都去真实的算缓存。它是通过采样来完成的，而采样的周期不是固定的，而是指数增长的，比如第一次采样完后，PartitionedAppendOnlyMap 要经过1.1次的update/insert操作之后才进行第二次采样，然后经过1.1*.1.1次之后进行第三次采样，以此递推，假设你内存开的大，那PartitionedAppendOnlyMap可能要经过几十万次更新之后之后才会进行一次采样，然后才能计算出新的大小，这个时候几十万次更新带来的新的内存压力，可能已经让你的GC不堪重负了。</p>
<p>当然，这是一种折中，因为确实不能频繁采样。</p>
<p>如果你不想出现这种问题，要么自己替换实现这个类，要么将</p>
<p>spark.shuffle.safetyFraction=0.8<br>设置的更小一些。</p>
<p>Shuffle Read 内存消耗分析<br>Shuffle Read 的入口链路为：</p>
<p>org.apache.spark.rdd.ShuffledRDD<br>—&gt; org.apache.spark.shuffle.sort.HashShuffleReader<br>   —&gt;  org.apache.spark.util.collection.ExternalAppendOnlyMap<br>   —&gt;  org.apache.spark.util.collection.ExternalSorter<br>Shuffle Read 会更复杂些，尤其是从各个节点拉取数据。但这块不是不是我们的重点。按流程，主要有：</p>
<p>获取待拉取数据的迭代器<br>使用AppendOnlyMap/ExternalAppendOnlyMap 做combine<br>如果需要对key排序，则使用ExternalSorter<br>其中1后续会单独列出文章。3我们在write阶段已经讨论过。所以这里重点是第二个步骤，combine阶段。</p>
<p>如果你开启了</p>
<p>spark.shuffle.spill=true<br>则使用ExternalAppendOnlyMap，否则使用AppendOnlyMap。两者的区别是，前者如果内存不够，则落磁盘，会发生spill操作，后者如果内存不够，直接OOM了。</p>
<p>这里我们会重点分析ExternalAppendOnlyMap。</p>
<p>ExternalAppendOnlyMap 作为内存缓冲数据的对象如下：</p>
<p> private var currentMap = new SizeTrackingAppendOnlyMap[K, C]<br>如果currentMap 对象向申请不到内存，就会触发spill动作。判定内存是否充足的逻辑和Shuffle Write 完全一致。</p>
<p>Combine做完之后，ExternalAppendOnlyMap 会返回一个Iterator，叫做ExternalIterator,这个Iterator背后的数据源是所有spill文件以及当前currentMap里的数据。</p>
<p>我们进去 ExternalIterator 看看，唯一的一个占用内存的对象是这个优先队列：</p>
<p>   private val mergeHeap = new mutable.PriorityQueue[StreamBuffer]<br>mergeHeap 里元素数量等于所有spill文件个数加一。StreamBuffer 的结构：</p>
<p> private class StreamBuffer(<br>                    val iterator: BufferedIterator[(K, C)],<br>                    val pairs: ArrayBuffer[(K, C)])<br>其中iterator 只是一个对象引用，pairs 应该保存的是iterator里的第一个元素(如果hash有冲突的话，则为多个)</p>
<p>所以mergeHeap 应该不占用什么内存。到这里我们看看应该占用多少内存。依然假设 CoreNum 为 C,则</p>
<p>  C <em> 32k + C  </em> mergeHeap  + C * SizeTrackingAppendOnlyMap<br>所以这一段占用内存较大的依然是 SizeTrackingAppendOnlyMap ，一样的，他的值也符合如下公式</p>
<p> C <em> SizeTrackingAppendOnlyMap &lt; ExecutorHeapMemeory </em> 0.2 * 0.8<br>ExternalAppendOnlyMap 的目的是做Combine,然后如果你还设置了Order,那么接着会启用 ExternalSorter 来完成排序。</p>
<p>经过上文对Shuffle Write的使用，相比大家也对ExternalSorter有一定的了解了，此时应该占用内存的地方最大不超过下面的这个值：</p>
<p> C <em> SizeTrackingAppendOnlyMap  + C </em> PartitionedAppendOnlyMap<br>不过即使如此，因为他们共享一个shuffleMemoryManager，则理论上只有这么大：</p>
<p> C <em> SizeTrackingAppendOnlyMap &lt;  ExecutorHeapMemeory </em> 0.2 * 0.8<br>分析到这里，我们可以做个总结：</p>
<p>Shuffle Read阶段如果内存不足，有两个阶段会落磁盘，分别是Combine 和 Sort 阶段。对应的都会spill小文件，并且产生读。<br>Shuffle Read 阶段如果开启了spill功能，则基本能保证内存控制在 ExecutorHeapMemeory <em> 0.2 </em> 0.8 之内。<br>后话<br>如果大家对Sort Shuffle 落磁盘文件这块感兴趣，还可以看看这篇文章 <a href="http://www.jianshu.com/p/2d837bf2dab6" target="_blank" rel="external">Spark Shuffle Write阶段磁盘文件分析</a><br>转载 <a href="http://www.jianshu.com/p/c83bb237caa8" target="_blank" rel="external">http://www.jianshu.com/p/c83bb237caa8</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="u524D_u8A00"><a href="#u524D_u8A00" class="headerlink" title="前言"></a>前言</h2><p>借用和董神的一段对话说下背景：</p>
<p>shuffle共有三种，别人讨论的是hash shuffl]]>
    </summary>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="shuffle" scheme="http://blog.djstudy.net/tags/shuffle/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[scala入门笔记]]></title>
    <link href="http://blog.djstudy.net/2016/01/24/scala-rumen-biji/"/>
    <id>http://blog.djstudy.net/2016/01/24/scala-rumen-biji/</id>
    <published>2016-01-24T11:09:23.000Z</published>
    <updated>2016-01-27T10:39:15.000Z</updated>
    <content type="html"><![CDATA[<h3 id="Scala_u7B80_u4ECB"><a href="#Scala_u7B80_u4ECB" class="headerlink" title="Scala简介"></a>Scala简介</h3><p>Scala 是一门多范式的编程语言, 由Martin Odersky 于2001年基于Funnel的工作开始设计Scala并于2004年正式发布<br>Scala是一种纯面向对象的语言，每个值都是对象<br>Scala是一门多范式编程语言, 支持命令交互式, 函数式, 面向对象<br>编译型高性能语言(静态)<br>与Java无缝兼容, 可以使用任何Java库</p>
<h3 id="u4EE3_u7801_u98CE_u683C"><a href="#u4EE3_u7801_u98CE_u683C" class="headerlink" title="代码风格"></a>代码风格</h3><ol>
<li>函数和变量以小驼峰命名</li>
<li>类和特质以大驼峰命名</li>
<li>常量使用全大写命名</li>
<li>一般使用两格缩进</li>
<li>Scala大部分情况可以忽略语句末尾的分号<h3 id="Scala_u53D8_u91CF"><a href="#Scala_u53D8_u91CF" class="headerlink" title="Scala变量"></a>Scala变量</h3>Scala中尽量避免使用变量, 函数式编程的一个重要特性是不可变性(不可变变量没有副作用)<br>Scala是静态类型语言, 但是不需要显式的指明变量类型, Scala采用类型推断(Type Inference)<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义一个变量</span></span><br><span class="line"><span class="keyword">var</span> x = <span class="number">0</span></span><br><span class="line"><span class="comment">// 定义</span></span><br><span class="line"><span class="keyword">val</span> y = <span class="number">1</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="Scala_u57FA_u672C_u7C7B_u578B_u548C_u64CD_u4F5C"><a href="#Scala_u57FA_u672C_u7C7B_u578B_u548C_u64CD_u4F5C" class="headerlink" title="Scala基本类型和操作"></a>Scala基本类型和操作</h3><p>String和值类型Byte, Short, Int, Long, Float, Double, Char, Boolean</p>
<ul>
<li>Scala的操作符不是特殊的语言语法, 任何方法都可以是操作符</li>
<li>操作符分为前缀, 中缀, 后缀</li>
<li>Scala中所有操作符都是方法调用<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 前缀</span><br><span class="line">scala&gt; (<span class="number">2.0</span>).unary_-</span><br><span class="line">res1: <span class="type">Double</span> = <span class="number">-2.0</span></span><br><span class="line"># 中缀</span><br><span class="line">scala&gt; x indexOf 'o'</span><br><span class="line">res0: <span class="type">Int</span> = <span class="number">4</span></span><br><span class="line"># 后缀</span><br><span class="line">scala&gt; <span class="keyword">val</span> x = <span class="string">"Hello, World"</span></span><br><span class="line">x: <span class="type">String</span> = <span class="type">Hello</span>, <span class="type">World</span></span><br><span class="line">scala&gt; x.toLowerCase</span><br><span class="line">res0: <span class="type">String</span> = hello, world</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>中缀操作符的两个操作数, 一个在左一个在右<br>前缀操作符方法名在操作符上加了unary_前缀(+, -, !, ~)<br>后缀操作符是不用点或括号调用的不带任何参数的方法<br>算术操作符: +, -, *, /, %<br>关系, 逻辑和位操作: &gt;, &lt;, &gt;=, &lt;=, ==, !=, &amp;&amp;, ||, &amp;, |, ^, ~(反码)<br>位移操作: &lt;&lt;, &gt;&gt;, &gt;&gt;&gt;(无符号右移)</p>
<h3 id="Scala_u51FD_u6570"><a href="#Scala_u51FD_u6570" class="headerlink" title="Scala函数"></a>Scala函数</h3><p>函数式语言的一个主要特征是, 函数是第一类结构<br>函数定义如下图:<br><img src="http://ww4.sinaimg.cn/large/ab508d3djw1eznwrn1zjcj20iv0d6taj.jpg" alt="scala function" title="scala 函数"><br>Unit 的结果类型指的是函数没有返回有用的值</p>
<h3 id="u51FD_u6570_u5F0F_u5BF9_u8C61"><a href="#u51FD_u6570_u5F0F_u5BF9_u8C61" class="headerlink" title="函数式对象"></a>函数式对象</h3><p>object和class的区别在于: object关键字创建一个单例对象<br>主构造器是类的唯一入口, 只有主构造器可以调用超类构造器<br>override关键字用于在重载父类的非抽象成员和成员函数<br>同一个类内函数名相同而参数类型和个数不同的函数重载不需要override<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rational</span> (<span class="params">n: <span class="type">Int</span>, d: <span class="type">Int</span></span>) </span>&#123;</span><br><span class="line">  <span class="comment">//precondition</span></span><br><span class="line">  require(d != <span class="number">0</span>)</span><br><span class="line">  <span class="comment">// 私有成员</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> g = gcd(n.abs, d.abs)</span><br><span class="line">  <span class="keyword">var</span> numer: <span class="type">Int</span> = n / g</span><br><span class="line">  <span class="keyword">var</span> denom: <span class="type">Int</span> = d / g</span><br><span class="line">  <span class="comment">// auxiliary constructor, 相当于python中__init__构造函数</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(n: <span class="type">Int</span>) = <span class="keyword">this</span>(n, <span class="number">1</span>)</span><br><span class="line">  <span class="comment">// 函数重载</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">toString</span> </span>= n + <span class="string">"/"</span> + d;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(other: <span class="type">Rational</span>): <span class="type">Rational</span> =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Rational</span>(numer * other.denom + other.numer * denom, denom * other.denom)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">-</span></span>(other: <span class="type">Rational</span>): <span class="type">Rational</span> =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Rational</span>(numer * other.denom - other.numer * denom, denom * other.denom)</span><br><span class="line">  <span class="comment">// 函数重载</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">-</span></span>(i: <span class="type">Int</span>): <span class="type">Rational</span> =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Rational</span>(numer - i * denom, denom)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">*</span></span>(other: <span class="type">Rational</span>): <span class="type">Rational</span> =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">Rational</span>(numer * other.numer, denom * other.denom)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">lessThan</span></span>(other: <span class="type">Rational</span>): <span class="type">Boolean</span> =</span><br><span class="line">    <span class="keyword">this</span>.numer * other.denom &lt; other.numer * <span class="keyword">this</span>.denom</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">max</span></span>(other: <span class="type">Rational</span>): <span class="type">Rational</span> =</span><br><span class="line">    <span class="keyword">if</span> (lessThan(other)) other <span class="keyword">else</span> <span class="keyword">this</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">gcd</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>): <span class="type">Int</span> =</span><br><span class="line">    <span class="keyword">if</span> (b == <span class="number">0</span>) a <span class="keyword">else</span> gcd(b, a % b)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">var</span> x = <span class="keyword">new</span> <span class="type">Rational</span>(<span class="number">1</span>, <span class="number">3</span>);</span><br><span class="line"><span class="keyword">var</span> y = <span class="keyword">new</span> <span class="type">Rational</span>(<span class="number">5</span>, <span class="number">7</span>);</span><br><span class="line">println(x add y)</span><br><span class="line">println(x * y)</span><br><span class="line">println(x - <span class="number">1</span>)</span><br><span class="line"><span class="comment">// 隐式转换, 放在解释器方位内</span></span><br><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">intToRational</span></span>(x: <span class="type">Int</span>) = <span class="keyword">new</span> <span class="type">Rational</span>(x)</span><br><span class="line">println(<span class="number">1</span> - x)</span><br></pre></td></tr></table></figure></p>
<h3 id="u7EE7_u627F_u548C_u591A_u6001"><a href="#u7EE7_u627F_u548C_u591A_u6001" class="headerlink" title="继承和多态"></a>继承和多态</h3><p>继承<br>多态和动态绑定特性<br>动态绑定的特性即父类指针可以指向子类对象, 通过父类指针调用成员方法时, 会查找实际所指向的对象, 然后调用对象的内的对应方法<br><img src="http://ww4.sinaimg.cn/large/ab508d3djw1ezyaprbvcmj20qo0ecmyq.jpg" alt="继承和多态"><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> el: <span class="type">Element</span> = <span class="keyword">new</span> <span class="type">ArrayElement</span>(<span class="type">Array</span>(<span class="string">"hello"</span>))</span><br><span class="line"><span class="keyword">val</span> e2: <span class="type">ArrayElement</span> = <span class="keyword">new</span> <span class="type">LineElement</span>(<span class="string">"hello"</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="u5185_u5EFA_u63A7_u5236_u7ED3_u6784"><a href="#u5185_u5EFA_u63A7_u5236_u7ED3_u6784" class="headerlink" title="内建控制结构"></a>内建控制结构</h3><p>表示式会产生一个值<br>Scala中if是能返回值的表达式, Scala中没有三元操作符, 但通过if (condition) var1 else var2 可以实现三元操作符的功能<br>while和do-while被称为循环, 不产生有意义的结果<br>Scala中for语句非常强大, for {子句} yield {循环体}<br>match表达式可以产生值, match远强大与其他语言中的switch, 而且不需要显示的声明break<br>变量范围: 大括号引入了一个新的范围, 内部变量会遮盖同名的外部变量<br>占位符语法<br>函数文本(匿名函数, 类似于python中的lamda)<br><img src="http://ww3.sinaimg.cn/large/ab508d3djw1eznwvjaswlj20dv05bgmc.jpg" alt="scala函数文本语法"><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> filename = <span class="keyword">if</span> (!args.isEmpty) args(<span class="number">0</span>) <span class="keyword">else</span> <span class="string">"default.txt"</span></span><br><span class="line"><span class="keyword">var</span> filesList = (<span class="keyword">new</span> <span class="type">File</span>(<span class="string">"."</span>)).listFiles</span><br><span class="line"><span class="comment">// i &lt;- 1 to 4(包含4), i &lt;- 1 until 4 (不包含4)</span></span><br><span class="line"><span class="keyword">for</span> (file &lt;- filesList</span><br><span class="line">     <span class="keyword">if</span> file.isFile;</span><br><span class="line">     <span class="keyword">if</span> file.getName.endsWith(<span class="string">".scala"</span>))  <span class="comment">// 过滤器使用分号隔开</span></span><br><span class="line">    println(file)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">var</span> firstArg = <span class="keyword">if</span>(args.length &gt; <span class="number">0</span>) args(<span class="number">0</span>) <span class="keyword">else</span> <span class="string">""</span></span><br><span class="line"><span class="comment">// 任何种类的常量和其他都可以作为case</span></span><br><span class="line">firstArg <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"text"</span> =&gt; println(<span class="string">"text"</span>)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; println(<span class="string">"default"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 匿名函数的写法(lambda)</span></span><br><span class="line">scala&gt; <span class="keyword">var</span> someNumbers = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">scala&gt; someNumbers.filter(x =&gt; x % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line"><span class="comment">// 占位符语法</span></span><br><span class="line">scala&gt; someNumbers.filter(_ % <span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line"><span class="comment">// 偏函数 partial funciton</span></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(a: <span class="type">Int</span>, b: <span class="type">Int</span>, c: <span class="type">Int</span>) = a + b + c</span><br><span class="line">scala&gt; <span class="keyword">val</span> a = sum(<span class="number">1</span>, _: <span class="type">Int</span>, <span class="number">3</span>)</span><br><span class="line">scala&gt; a(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 变长参数(Array[String])</span></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">echo</span></span>(args: <span class="type">String</span>*) = <span class="keyword">for</span>(arg &lt;- args) println(arg)</span><br><span class="line">scala&gt; echo(<span class="string">"one"</span>)</span><br><span class="line">scala&gt; echo(<span class="string">"one"</span>, <span class="string">"two"</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultConstructor</span> (<span class="params"> name:<span class="type">String</span> , age:<span class="type">Int</span></span>)</span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(name:<span class="type">String</span>)&#123;</span><br><span class="line">    <span class="comment">/*自定义构造器，必需首先调用默认构造器*/</span></span><br><span class="line">    <span class="keyword">this</span>(name , <span class="number">24</span>) ; </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">show</span></span>()&#123;</span><br><span class="line">    println( name + <span class="string">"--&gt;"</span> + age ) ;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>柯里化(carry)<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 普通函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(x: <span class="type">Int</span>, y: <span class="type">Int</span>) = x + y</span><br><span class="line"><span class="comment">// 柯里化函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(x: <span class="type">Int</span>)(y: <span class="type">Int</span>) = x + y</span><br><span class="line"><span class="comment">// 实际执行</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(x: <span class="type">Int</span>) = (y: <span class="type">Int</span>) =&gt; x + y</span><br></pre></td></tr></table></figure></p>
<h3 id="u7279_u8D28_28trait_29"><a href="#u7279_u8D28_28trait_29" class="headerlink" title="特质(trait)"></a>特质(trait)</h3><p>特质就像带有具体方法的java接口<br>特质和抽象类的区别: 抽象类主要用于有明确的父子继承关系的类树, 而特质可以用于任何类<br>特质定义使用trait关键字<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Person</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">detail</span></span>() &#123;</span><br><span class="line">        println(<span class="string">"I'm angry!"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 使用extends或with混入特质, 从特质继承的方法可以像从超类继承的方法使用</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword">extends</span> <span class="title">Person</span> <span class="keyword">with</span> <span class="title">Boy</span> </span>&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="u6570_u636E_u7ED3_u6784"><a href="#u6570_u636E_u7ED3_u6784" class="headerlink" title="数据结构"></a>数据结构</h3><p>Python中常用list, tuple, set, dict<br>Scala对应的数据结构为List, Tuple[X], Set, Map(HashMap)<br>Scala中默认为不可变对象, 操作会生成一个新的对象</p>
<h3 id="u5305"><a href="#u5305" class="headerlink" title="包"></a>包</h3><p>Scala采用java平台的包机制<br>使用import来进行引用<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhihu.antispam</span><br></pre></td></tr></table></figure></p>
<h3 id="u53C2_u8003_u94FE_u63A5"><a href="#u53C2_u8003_u94FE_u63A5" class="headerlink" title="参考链接"></a>参考链接</h3><p><a href="http://stackoverflow.com/questions/1755345/difference-between-object-and-class-in-scala" target="_blank" rel="external">Difference between object and class in Scala</a><br><a href="https://www.zybuluo.com/Great-Chinese/note/254972" target="_blank" rel="external">scala 入门</a><br><a href="http://colobu.com/2016/01/04/Scala-magic-functions/?hmsr=toutiao.io&amp;utm_medium=toutiao.io&amp;utm_source=toutiao.io" target="_blank" rel="external">Scala 魔法函数</a><br><a href="http://blog.yunglinho.com/blog/2012/04/22/dependency-injection-in-scala/" target="_blank" rel="external">Dependency Injection in Scala</a><br><a href="http://www.hawstein.com/posts/databricks-scala-guide.html" target="_blank" rel="external">Databricks Scala 编程风格指南</a><br><a href="http://stackoverflow.com/questions/8000903/what-are-all-the-uses-of-an-underscore-in-scala" target="_blank" rel="external">What are all the uses of an underscore in Scala?</a></p>
<p>转载 <a href="http://andrewliu.in/2016/01/16/Scala%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/" target="_blank" rel="external">http://andrewliu.in/2016/01/16/Scala%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="Scala_u7B80_u4ECB"><a href="#Scala_u7B80_u4ECB" class="headerlink" title="Scala简介"></a>Scala简介</h3><p>Scala 是一门多范式的编程语言, 由Martin Ode]]>
    </summary>
    
      <category term="scala" scheme="http://blog.djstudy.net/tags/scala/"/>
    
      <category term="scala" scheme="http://blog.djstudy.net/categories/scala/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[将 Spark 中的文本转换为 Parquet 以提升性能]]></title>
    <link href="http://blog.djstudy.net/2016/01/24/spark-convert-text-parquet/"/>
    <id>http://blog.djstudy.net/2016/01/24/spark-convert-text-parquet/</id>
    <published>2016-01-24T10:46:44.000Z</published>
    <updated>2016-01-24T11:03:24.000Z</updated>
    <content type="html"><![CDATA[<p>列式存储布局（比如 Parquet）可以加速查询，因为它只检查所有需要的列并对它们的值执行计算，因此只读取一个数据文件或表的小部分数据。Parquet 还支持灵活的压缩选项，因此可以显著减少磁盘上的存储。<br>如果您在 HDFS 上拥有基于文本的数据文件或表，而且正在使用 Spark SQL 对它们执行查询，那么强烈推荐将文本数据文件转换为 Parquet 数据文件，以实现性能和存储收益。当然，转换需要时间，但查询性能的提升在某些情况下可能达到 30 倍或更高，存储的节省可高达 75%！<br>已有文章介绍使用 Parquet 存储为 BigSQL、Hive 和 Impala 带来类似的性能收益，本文将介绍如何编写一个简单的 Scala 应用程序，将现有的基于文本的数据文件或表转换为 Parquet 数据文件，还将展示给 Spark SQL 带来的实际存储节省和查询性能提升。</p>
<h3 id="u8BA9_u6211_u4EEC_u8F6C_u6362_u4E3A_Parquet__u5427_uFF01"><a href="#u8BA9_u6211_u4EEC_u8F6C_u6362_u4E3A_Parquet__u5427_uFF01" class="headerlink" title="让我们转换为 Parquet 吧！"></a>让我们转换为 Parquet 吧！</h3><p>Spark SQL 提供了对读取和写入 Parquet 文件的支持，能够自动保留原始数据的模式。Parquet 模式通过 Data Frame API，使数据文件对 Spark SQL 应用程序 “不言自明”。当然，Spark SQL 还支持读取已存储为 Parquet 的现有 Hive 表，但您需要配置 Spark，以便使用 Hive 的元存储来加载所有信息。在我们的示例中，不涉及 Hive 元存储。<br>以下 Scala 代码示例将读取一个基于文本的 CSV 表，并将它写入 Parquet 表：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert</span></span>(sqlContext: <span class="type">SQLContext</span>, filename: <span class="type">String</span>, schema: <span class="type">StructType</span>, tablename: <span class="type">String</span>) &#123;</span><br><span class="line">     <span class="comment">// import text-based table first into a data frame</span></span><br><span class="line">     <span class="keyword">val</span> df = sqlContext.read.format(<span class="string">"com.databricks.spark.csv"</span>).</span><br><span class="line">       schema(schema).option(<span class="string">"delimiter"</span>, <span class="string">"|"</span>).load(filename)</span><br><span class="line">     <span class="comment">// now simply write to a parquet file</span></span><br><span class="line">     df.write.parquet(<span class="string">"/user/spark/data/parquet/"</span>+tablename)</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// usage exampe -- a tpc-ds table called catalog_page</span></span><br><span class="line"> schema= <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_catalog_page_sk"</span>,        <span class="type">IntegerType</span>,<span class="literal">false</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_catalog_page_id"</span>,        <span class="type">StringType</span>,<span class="literal">false</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_start_date_sk"</span>,          <span class="type">IntegerType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_end_date_sk"</span>,            <span class="type">IntegerType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_department"</span>,             <span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_catalog_number"</span>,         <span class="type">LongType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_catalog_page_number"</span>,    <span class="type">LongType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_description"</span>,            <span class="type">StringType</span>,<span class="literal">true</span>),</span><br><span class="line">         <span class="type">StructField</span>(<span class="string">"cp_type"</span>,                   <span class="type">StringType</span>,<span class="literal">true</span>)))</span><br><span class="line"> convert(sqlContext,</span><br><span class="line">         hadoopdsPath+<span class="string">"/catalog_page/*"</span>,</span><br><span class="line">         schema,</span><br><span class="line">         <span class="string">"catalog_page"</span>)</span><br></pre></td></tr></table></figure></p>
<p>上面的代码将会读取 hadoopdsPath+”/catalog_page/* 中基于文本的 CSV 文件，并将转换的 Parquet 文件保存在 /user/spark/data/parquet/ 下。此外，转换的 Parquet 文件会在 gzip 中自动压缩，因为 Spark 变量 spark.sql.parquet.compression.codec 已在默认情况下设置为 gzip。您还可以将压缩编解码器设置为 uncompressed、snappy 或 lzo。</p>
<h3 id="u8F6C_u6362_1_TB__u6570_u636E_u5C06_u82B1_u8D39_u591A_u957F_u65F6_u95F4_uFF1F"><a href="#u8F6C_u6362_1_TB__u6570_u636E_u5C06_u82B1_u8D39_u591A_u957F_u65F6_u95F4_uFF1F" class="headerlink" title="转换 1 TB 数据将花费多长时间？"></a>转换 1 TB 数据将花费多长时间？</h3><p>50 分钟，在一个 6 数据节点的 Spark v1.5.1 集群上可达到约 20 GB/分的吞吐量。使用的总内存约为 500GB。HDFS 上最终的 Parquet 文件的格式为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/user/spark/data/parquet/catalog_page/part-r-00000-9ff58e65-0674-440a-883d-256370f33c66.gz.parquet</span><br><span class="line">/user/spark/data/parquet/catalog_page/part-r-00001-9ff58e65-0674-440a-883d-256370f33c66.gz.parquet</span><br></pre></td></tr></table></figure></p>
<h3 id="u5B58_u50A8_u8282_u7701"><a href="#u5B58_u50A8_u8282_u7701" class="headerlink" title="存储节省"></a>存储节省</h3><p>以下 Linux 输出显示了 TEXT 和 PARQUET 在 HDFS 上的大小比较：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ hadoop fs -du -h <span class="_">-s</span> /user/spark/hadoopds1000g</span><br><span class="line">    897.9 G  /user/spark/hadoopds1000g</span><br><span class="line">    % hadoop fs -du -h <span class="_">-s</span> /user/spark/data/parquet</span><br><span class="line">    231.4 G  /user/spark/data/parquet</span><br></pre></td></tr></table></figure></p>
<p>1 TB 数据的存储节省了将近 75%！</p>
<h3 id="u67E5_u8BE2_u6027_u80FD_u63D0_u5347"><a href="#u67E5_u8BE2_u6027_u80FD_u63D0_u5347" class="headerlink" title="查询性能提升"></a>查询性能提升</h3><p>Parquet 文件是自描述性的，所以保留了模式。要将 Parquet 文件加载到 DataFrame 中并将它注册为一个 temp 表，可执行以下操作：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = sqlContext.read.parquet(filename)</span><br><span class="line">      df.show</span><br><span class="line">      df.registerTempTable(tablename)</span><br></pre></td></tr></table></figure></p>
<p>要对比性能，然后可以分别对 TEXT 和 PARQUET 表运行以下查询（假设所有其他 tpc-ds 表也都已转换为 Parquet）。您可以利用 spark-sql-perf 测试工具包来执行查询测试。举例而言，现在来看看 TPC-DS 基准测试中的查询 #76，<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">("q76", """</span><br><span class="line">            | <span class="keyword">SELECT</span></span><br><span class="line">            |    channel, col_name, d_year, d_qoy, i_category, <span class="keyword">COUNT</span>(*) sales_cnt,</span><br><span class="line">            |    <span class="keyword">SUM</span>(ext_sales_price) sales_amt</span><br><span class="line">            | <span class="keyword">FROM</span>(</span><br><span class="line">            |    <span class="keyword">SELECT</span></span><br><span class="line">            |        <span class="string">'store'</span> <span class="keyword">as</span> channel, ss_store_sk col_name, d_year, d_qoy, i_category,</span><br><span class="line">            |        ss_ext_sales_price ext_sales_price</span><br><span class="line">            |    <span class="keyword">FROM</span> store_sales, item, date_dim</span><br><span class="line">            |    <span class="keyword">WHERE</span> ss_store_sk <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line">            |      <span class="keyword">AND</span> ss_sold_date_sk=d_date_sk</span><br><span class="line">            |      <span class="keyword">AND</span> ss_item_sk=i_item_sk</span><br><span class="line">            |    <span class="keyword">UNION</span> ALL</span><br><span class="line">            |    <span class="keyword">SELECT</span></span><br><span class="line">            |        <span class="string">'web'</span> <span class="keyword">as</span> channel, ws_ship_customer_sk col_name, d_year, d_qoy, i_category,</span><br><span class="line">            |        ws_ext_sales_price ext_sales_price</span><br><span class="line">            |    <span class="keyword">FROM</span> web_sales, item, date_dim</span><br><span class="line">            |    <span class="keyword">WHERE</span> ws_ship_customer_sk <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line">            |      <span class="keyword">AND</span> ws_sold_date_sk=d_date_sk</span><br><span class="line">            |      <span class="keyword">AND</span> ws_item_sk=i_item_sk</span><br><span class="line">            |    <span class="keyword">UNION</span> ALL</span><br><span class="line">            |    <span class="keyword">SELECT</span></span><br><span class="line">            |        <span class="string">'catalog'</span> <span class="keyword">as</span> channel, cs_ship_addr_sk col_name, d_year, d_qoy, i_category,</span><br><span class="line">            |        cs_ext_sales_price ext_sales_price</span><br><span class="line">            |    <span class="keyword">FROM</span> catalog_sales, item, date_dim</span><br><span class="line">            |    <span class="keyword">WHERE</span> cs_ship_addr_sk <span class="keyword">IS</span> <span class="literal">NULL</span></span><br><span class="line">            |      <span class="keyword">AND</span> cs_sold_date_sk=d_date_sk</span><br><span class="line">            |      <span class="keyword">AND</span> cs_item_sk=i_item_sk) foo</span><br><span class="line">            | <span class="keyword">GROUP</span> <span class="keyword">BY</span> channel, col_name, d_year, d_qoy, i_category</span><br><span class="line">            | <span class="keyword">ORDER</span> <span class="keyword">BY</span> channel, col_name, d_year, d_qoy, i_category</span><br><span class="line">            | <span class="keyword">limit</span> <span class="number">100</span></span><br></pre></td></tr></table></figure></p>
<p>查询时间如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TIME               TEXT     PARQUET</span><br><span class="line">Query time (sec)    698          21</span><br></pre></td></tr></table></figure></p>
<p>参考资料<br><a href="https://developer.ibm.com/hadoop/blog/2015/12/03/parquet-for-spark-sql/" target="_blank" rel="external">英文原文。</a><br>在 <a href="http://www.ibm.com/developerworks/cn/bigdata/" target="_blank" rel="external">developerWorks 大数据和分析专区</a>，了解关于大数据的更多信息，获取技术文档、how-to 文章、培训、下载、产品信息以及其他资源。<br>加入 <a href="http://www.ibm.com/developerworks/cn/community/" target="_blank" rel="external">developerWorks 中文社区</a>，查看开发人员推动的博客、论坛、组和维基，并与其他 developerWorks 用户交流。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>列式存储布局（比如 Parquet）可以加速查询，因为它只检查所有需要的列并对它们的值执行计算，因此只读取一个数据文件或表的小部分数据。Parquet 还支持灵活的压缩选项，因此可以显著减少磁盘上的存储。<br>如果您在 HDFS 上拥有基于文本的数据文件或表，而且正在使用]]>
    </summary>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="parquet" scheme="http://blog.djstudy.net/tags/parquet/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Spark Standalone模式HA环境搭建]]></title>
    <link href="http://blog.djstudy.net/2016/01/24/spark-ha/"/>
    <id>http://blog.djstudy.net/2016/01/24/spark-ha/</id>
    <published>2016-01-24T08:10:30.000Z</published>
    <updated>2016-01-24T08:13:04.000Z</updated>
    <content type="html"><![CDATA[<p>Spark Standalone模式常见的HA部署方式有两种：基于文件系统的HA和基于ZK的HA<br>本篇只介绍基于ZK的HA环境搭建：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ SPARK_HOME/conf/spark-env.sh</span><br></pre></td></tr></table></figure></p>
<p>添加SPARK_DAEMON_JAVA_OPTS的配置信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop000:2181,hadoop001:2181,hadoop002:2181 -Dspark.deploy.zookeeper.dir=/spark"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="u914D_u7F6E_u53C2_u6570_u8BF4_u660E_uFF1A"><a href="#u914D_u7F6E_u53C2_u6570_u8BF4_u660E_uFF1A" class="headerlink" title="配置参数说明："></a>配置参数说明：</h3><p>spark.deploy.recoveryMode: 设置恢复模式为zk，默认为NONE<br>spark.deploy.zookeeper.url: 设置ZK集群的url，形如：192.168.1.100:2181,192.168.1.101:2181<br>spark.deploy.zookeeper.dir: 设置zk保存恢复状态的路径，默认为spark<br>实现HA的原理：利用ZK的Leader Election机制，选择一个Active状态的Master，其余的Master均为Standby状态；当Active状态的Master死掉后，通过ZK选举一个Standby状态的Master为Active状态。</p>
<h3 id="u6D4B_u8BD5_u6B65_u9AA4_uFF1A"><a href="#u6D4B_u8BD5_u6B65_u9AA4_uFF1A" class="headerlink" title="测试步骤："></a>测试步骤：</h3><p>启动standalone集群后，在各个Standby节点上启动start-master.sh，jps观察是否已经正确启动Master进程；<br>将Active状态的Master kill掉，观察8080端口对应的页面，发现已经从Standby状态中选举出一个当作Active状态。<br>采用ZK后由于会有多个Master，在提交任务时不知道哪个为Active状态的Master，可以采用如下的方式提交：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark-shell –master spark://hadoop000:7077,hadoop001:7077,hadoop002:7077 –executor-memory 2g –total-executor-cores 1</span><br><span class="line">详细信息参见官方文档：http://spark.apache.org/docs/latest/spark-standalone.html<span class="comment">#standby-masters-with-zookeeper</span></span><br></pre></td></tr></table></figure></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Spark Standalone模式常见的HA部署方式有两种：基于文件系统的HA和基于ZK的HA<br>本篇只介绍基于ZK的HA环境搭建：<br><figure class="highlight bash"><table><tr><td class="gutter"><pr]]>
    </summary>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="ha" scheme="http://blog.djstudy.net/tags/ha/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[spark on yarn]]></title>
    <link href="http://blog.djstudy.net/2016/01/24/spark-on-yarn/"/>
    <id>http://blog.djstudy.net/2016/01/24/spark-on-yarn/</id>
    <published>2016-01-24T06:43:59.000Z</published>
    <updated>2016-01-24T08:08:20.000Z</updated>
    <content type="html"><![CDATA[<h2 id="u4E3A_u4EC0_u4E48_u8981_u4F7F_u7528YARN_3F"><a href="#u4E3A_u4EC0_u4E48_u8981_u4F7F_u7528YARN_3F" class="headerlink" title="为什么要使用YARN?"></a>为什么要使用YARN?</h2><p>数据共享、资源利用率、更方便的管理集群等。<br>详情参见：<a href="http://www.cnblogs.com/luogankun/p/3887019.html" target="_blank" rel="external">http://www.cnblogs.com/luogankun/p/3887019.html</a></p>
<h2 id="Spark_YARN_u7248_u672C_u7F16_u8BD1"><a href="#Spark_YARN_u7248_u672C_u7F16_u8BD1" class="headerlink" title="Spark YARN版本编译"></a>Spark YARN版本编译</h2><p>编译hadoop对应的支持YARN的Spark版本<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> MAVEN_OPTS=<span class="string">"-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m"</span></span><br><span class="line">$ mvn clean package -DskipTests -Phadoop-2.3 -Dhadoop.version=2.3.0-cdh5.0.0 -Dprotobuf.version=2.5.0 -Pyarn -Phive</span><br></pre></td></tr></table></figure></p>
<p>详情参见：<a href="http://www.cnblogs.com/luogankun/p/3798403.html" target="_blank" rel="external">Spark On YARN</a></p>
<p>Spark的Cluster Manager负责管理启动executor进程，集群可以是Standalone、YARN和Mesos<br>每个SparkContext（换句话说是：Application）对应一个ApplicationMaster（Application启动过程中的第一个容器<br>ApplicationMaster负责和ResourceManager打交道，并请求资源，当获取资源之后通知NodeManager为其启动container； 每个Container中运行一个ExecutorBackend<br>ResourceManager决定哪些Application可以运行、什么时候运行以及在哪些NodeManager上运行； NodeManager的Container上运行executor进程<br>在Standalone模式中有Worker的概念，而在Spark On YARN中没有Worker的概念<br>由于executor是运行在container中，故container内存要大于executor的内存<br>Spark On YARN有两种：</p>
<h3 id="yarn-client"><a href="#yarn-client" class="headerlink" title="yarn-client"></a>yarn-client</h3><p>Client和Driver运行在一起，ApplicationMaster只负责获取资源<br>　　Client会和请求到的资源container通信来调度他们进行工作，也就是说Client不能退出滴；<br>　　日志信息输出能输出在终端控制台上，适用于交互或者调试，也就是希望快速地看到application的输出，比如SparkStreaming<br><img src="/images/yarn-client.png" alt="yarn-client" title="yarn client"></p>
<h3 id="yarn-cluster"><a href="#yarn-cluster" class="headerlink" title="yarn-cluster"></a>yarn-cluster</h3><p>Driver和ApplicationMaster运行在一起；负责向YARN申请资源，并检测作业的运行状况；executor运行在container中<br>　　提交Application之后，即使关掉了Client，作业仍然会继续在YARN上运行<br>　　日志信息不会输出在终端控制台上<br><img src="/images/yarn-cluster.png" alt="yarn-cluster" title="yarn cluster"></p>
<h3 id="u63D0_u4EA4Spark_u4F5C_u4E1A_u5230YARN"><a href="#u63D0_u4EA4Spark_u4F5C_u4E1A_u5230YARN" class="headerlink" title="提交Spark作业到YARN"></a>提交Spark作业到YARN</h3><p>提交命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class &lt;main-class&gt;</span><br><span class="line">  --master &lt;master-url&gt; \</span><br><span class="line">  --deploy-mode &lt;deploy-mode&gt; \</span><br><span class="line">  ... <span class="comment"># other options</span></span><br><span class="line">  &lt;application-jar&gt; \</span><br><span class="line">  [application-arguments]</span><br></pre></td></tr></table></figure></p>
<h3 id="u63D0_u4EA4_u672C_u5730jar"><a href="#u63D0_u4EA4_u672C_u5730jar" class="headerlink" title="提交本地jar"></a>提交本地jar</h3><p>提交到yarn-cluster/yarn-client<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn-cluster \  <span class="comment"># can also be `yarn-client` for client mode</span></span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  /path/to/examples.jar \</span><br></pre></td></tr></table></figure></p>
<p>如果采用的是yarn-cluster的方式运行的话，想停止执行应用，需要去多个node上干掉；而在yarn-client模式运行时，只需要在client上干掉应用即可。<br>提交到standalone<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master spark://207.184.161.138:7077 \</span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --total-executor-cores 100 \</span><br><span class="line">  /path/to/examples.jar \</span><br></pre></td></tr></table></figure></p>
<h3 id="u63D0_u4EA4hdfs_u4E0A_u7684jar"><a href="#u63D0_u4EA4hdfs_u4E0A_u7684jar" class="headerlink" title="提交hdfs上的jar"></a>提交hdfs上的jar</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn-cluster \  <span class="comment"># can also be `yarn-client` for client mode</span></span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  hdfs://hadoop000:8020/lib/examples.jar \</span><br></pre></td></tr></table></figure>
<p>如果没有在spark-env.sh文件中配置HADOOP_CONF_DIR或者YARN_CONF_DIR，可以在提交作业前指定形如<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> HADOOP_CONF_DIR=XXX</span><br><span class="line">$ ./bin/spark-submit \</span><br><span class="line">  --class org.apache.spark.examples.SparkPi \</span><br><span class="line">  --master yarn-cluster \  <span class="comment"># can also be `yarn-client` for client mode</span></span><br><span class="line">  --executor-memory 20G \</span><br><span class="line">  --num-executors 50 \</span><br><span class="line">  /path/to/examples.jar</span><br></pre></td></tr></table></figure></p>
<p>详情参见：<a href="http://spark.apache.org/docs/latest/submitting-applications.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/submitting-applications.html</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="u4E3A_u4EC0_u4E48_u8981_u4F7F_u7528YARN_3F"><a href="#u4E3A_u4EC0_u4E48_u8981_u4F7F_u7528YARN_3F" class="headerlink" title="为什么要使用YA]]>
    </summary>
    
      <category term="yarn" scheme="http://blog.djstudy.net/tags/yarn/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/categories/spark/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ambari hdp中部署apache spark运行spark shell遇到的错误解决]]></title>
    <link href="http://blog.djstudy.net/2016/01/24/hdp-spark-shell-error/"/>
    <id>http://blog.djstudy.net/2016/01/24/hdp-spark-shell-error/</id>
    <published>2016-01-24T05:05:21.000Z</published>
    <updated>2016-01-24T08:04:10.000Z</updated>
    <content type="html"><![CDATA[<p>在运行spark-shell中遇到的ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library 解决方法<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell –driver-library-path :/usr/hdp/2.2.4.2-2/hadoop/lib/native/Linux-amd64-64 /usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jar</span><br></pre></td></tr></table></figure></p>
<p>在运行spark-shell中遇到的Compression codec com.hadoop.compression.lzo.LzoCodec not found 错误可以配置文件spark-defaults.conf<br>spark.executor.extraClassPath /usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jar<br>spark.driver.extraClassPath /usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jar<br>保存文件重启spark服务集群即可。<br>再提供一个Unable to load native-hadoop library 和 Snappy native library not loaded的解决方案。这个问题主要是jre目录下缺少了libgplcompression.so , libhadoop.so和libsnappy.so两个文件。具体是，spark-shell依赖的是scala，scala依赖的是JAVA_HOME下的jdk，libhadoop.so和libsnappy.so两个文件应该放到JAVA_HOME/jre/lib/amd64下面。要注意的是要知道真正依赖到的JAVA_HOME是哪一个，把两个.so放对地方。这两个so：libhadoop.so和libsnappy.so。前一个so可以在HADOOP_HOME下找到，比如hadoop\lib\native\Linux-amd64-64。第二个libsnappy.so需要下载一个snappy-1.1.0.tar.gz，然后./configure，make编译出来。snappy是google的一个压缩算法，在hadoop jira下<a href="https://issues.apache.org/jira/browse/HADOOP-7206记录了这次集成。" target="_blank" rel="external">https://issues.apache.org/jira/browse/HADOOP-7206记录了这次集成。</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>在运行spark-shell中遇到的ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library 解决方法<br><figure class="highlight bash"><table><tr><td ]]>
    </summary>
    
      <category term="yarn" scheme="http://blog.djstudy.net/tags/yarn/"/>
    
      <category term="spark" scheme="http://blog.djstudy.net/tags/spark/"/>
    
      <category term="ambari" scheme="http://blog.djstudy.net/tags/ambari/"/>
    
      <category term="hadoop" scheme="http://blog.djstudy.net/categories/hadoop/"/>
    
  </entry>
  
</feed>
