{"meta":{"title":"东杰书屋","subtitle":"环境不会改变，解决之道在于改变自己。","description":null,"author":"东杰","url":"https://blog.djstudy.net"},"pages":[{"title":"About","date":"2018-04-09T11:01:04.463Z","updated":"2018-04-09T11:01:04.463Z","comments":true,"path":"about/index.html","permalink":"https://blog.djstudy.net/about/index.html","excerpt":"","text":""},{"title":"Categories","date":"2018-04-09T09:54:44.431Z","updated":"2018-04-09T09:54:44.431Z","comments":true,"path":"categories/index.html","permalink":"https://blog.djstudy.net/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2018-04-09T09:54:44.433Z","updated":"2018-04-09T09:54:44.433Z","comments":true,"path":"tags/index.html","permalink":"https://blog.djstudy.net/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"使用openresty获取kibana查询post参数","slug":"openresty-kibana-post","date":"2017-06-06T02:23:11.000Z","updated":"2017-06-06T03:04:35.000Z","comments":true,"path":"2017/06/06/openresty-kibana-post/","link":"","permalink":"https://blog.djstudy.net/2017/06/06/openresty-kibana-post/","excerpt":"","text":"openresty配置放到包含fastcgi_pass或proxy_pass的Location里面1234567891011121314log_format main &apos;$remote_addr | [$time_iso8601] | &quot;$request&quot; | &apos; &apos;$status | $body_bytes_sent | &quot;$http_referer&quot; | &apos; &apos;&quot;$http_user_agent&quot; | &quot;$http_x_forwarded_for&quot; | &quot;$request_body&quot;&apos;;server &#123; listen 80; server_name esclient.test.net; location / &#123; proxy_pass http://192.168.1.47:5601; proxy_http_version 1.1; proxy_set_header Connection &quot;&quot;; proxy_connect_timeout 5s; proxy_read_timeout 10s; access_log /usr/local/openresty/nginx/logs/kibana.access.log main; &#125; &#125; 放到任意Location里面Location里面加上如下语句 12lua_need_request_body on; content_by_lua &apos;local s = ngx.var.request_body&apos;; nginx 直接在配置文章中设置日志分割直接在nginx配置文件中，配置日志循环，而不需使用logrotate或配置cron任务。需要使用到$time_iso8601 内嵌变量来获取时间。$time_iso8601格式如下：2015-08-07T18:12:02+02:00。然后使用正则表达式来获取所需时间的数据。按天分割日志 使用下面的代码块1234567if ($time_iso8601 ~ &quot;^(\\d&#123;4&#125;)-(\\d&#123;2&#125;)-(\\d&#123;2&#125;)&quot;) &#123; set $year $1; set $month $2; set $day $3;&#125; access_log /data/logs/nginx/www.ttlsa.com-$year-$month-$day-access.log; 也可以使用Perl语法来捕获，如下：12if ($time_iso8601 ~ &quot;^(?&lt;year&gt;\\d&#123;4&#125;)-(?&lt;month&gt;\\d&#123;2&#125;)-(?&lt;day&gt;\\d&#123;2&#125;)&quot;) &#123;&#125;access_log /data/logs/nginx/www.ttlsa.com-$year-$month-$day-access.log; 按时、分、秒分割123456789if ($time_iso8601 ~ &quot;^(\\d&#123;4&#125;)-(\\d&#123;2&#125;)-(\\d&#123;2&#125;)T(\\d&#123;2&#125;):(\\d&#123;2&#125;):(\\d&#123;2&#125;)&quot;)&#123; set $year $1; set $month $2; set $day $3; set $hour $4; set $minutes $5; set $seconds $6;&#125;","categories":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://blog.djstudy.net/categories/elasticsearch/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://blog.djstudy.net/tags/elasticsearch/"}]},{"title":"Elasticsearch 分片交互过程详解","slug":"es-shard-interaction","date":"2017-04-27T05:03:47.000Z","updated":"2017-04-27T10:17:57.000Z","comments":true,"path":"2017/04/27/es-shard-interaction/","link":"","permalink":"https://blog.djstudy.net/2017/04/27/es-shard-interaction/","excerpt":"","text":"一、Elasticseach如何将数据存储到分片中问题：当我们要在ES中存储数据的时候，数据应该存储在主分片和复制分片中的哪一个中去；当我们在ES中检索数据的时候，又是怎么判断要查询的数据是属于哪一个分片。 数据存储到分片的过程是一定规则的，并不是随机发生的。 规则：shard = hash(routing) % number_of_primary_shards Routing值可以是一个任意的字符串，默认情况下，它的值为存数数据对应文档 _id 值，也可以是用户自定义的值。Routing这个字符串通过一个hash的函数处理，并返回一个数值，然后再除以索引中主分片的数目，所得的余数作为主分片的编号，取值一般在0到number_of_primary_shards - 1的这个范围中。通过这种方法计算出该数据是存储到哪个分片中。 正是这种路由机制，导致了主分片的个数为什么在索引建立之后不能修改。对已有索引主分片数目的修改直接会导致路由规则出现严重问题，部分数据将无法被检索。 二、主分片与复制分片如何交互为了说明这个问题，我用一个例子来说明。在上面这个例子中，有三个ES的node，其中每一个index中包含两个primary shard，每个primary shard拥有一个replica shard。下面从几种常见的数据操作来说明二者之间的交互情况。 1、索引与删除一个文档 这两种过程均可以分为三个过程来描述：阶段1：客户端发送了一个索引或者删除的请求给node 1。 阶段2：node 1通过请求中文档的 _id 值判断出该文档应该被存储在shard 0 这个分片中，并且node 1知道shard 0的primary shard位于node 3这个节点上。因此node 1会把这个请求转发到node 3。 阶段3：node 3在shard 0 的primary shard上执行请求。如果请求执行成功，它node 3将并行地将该请求发给shard 0的其余所有replica shard上，也就是存在于node 1和node 2中的replica shard。如果所有的replica shard都成功地执行了请求，那么将会向node 3回复一个成功确认，当node 3收到了所有replica shard的确认信息后，则最后向用户返回一个Success的消息。 2、更新一个文档该过程可以分为四个阶段来描述：阶段1：客户端向node 1发送一个文档更新的请求。 阶段2：同样的node 1通过请求中文档的 _id 值判断出该文档应该被存储在shard 0 这个分片中，并且node 1知道shard 0的primary shard位于node 3这个节点上。因此node 1会把这个请求转发到node 3。 阶段3：node 3从文档所在的primary shard中获取到它的JSON文件，并修改其中的_source中的内容，之后再重新索引该文档到其primary shard中。 阶段4：如果node 3成功地更新了文档，node 3将会把文档新的版本并行地发给其余所有的replica shard所在node中。这些node也同样重新索引新版本的文档，执行后则向node 3确认成功，当node 3接收到所有的成功确认之后，再向客户端发送一个更新成功的信息。 3、检索文档CRUD这些操作的过程中一般都是结合一些唯一的标记例如：_index，_type，以及routing的值，这就意味在执行操作的时候都是确切的知道文档在集群中的哪个node中，哪个shard中。 而检索过程往往需要更多的执行模式，因为我们并不清楚所要检索的文档具体位置所在， 它们可能存在于ES集群中个任何位置。因此，一般情况下，检索的执行不得不去询问index中的每一个shard。 但是，找到所有匹配检索的文档仅仅只是检索过程的一半，在向客户端返回一个结果列表之前，必须将各个shard发回的小片的检索结果，拼接成一个大的已排好序的汇总结果列表。正因为这个原因，检索的过程将分为查询阶段与获取阶段（Query Phase and Fetch Phase）。 Query Phase在最初的查询过程中，查询请求会广播到index中的每一个primary shard和replica shard中，每一个shard会在本地执行检索，并建立一个优先级队列（priority queue）。这个优先级队列是一个根据文档匹配度这个指标所排序列表，列表的长度由分页参数from和size两个参数所决定。例如： 下面从一个例子中说明这个过程： Query Phase阶段可以再细分成3个小的子阶段： 子阶段1：客户端发送一个检索的请求给node 3，此时node 3会创建一个空的优先级队列并且配置好分页参数from与size。 子阶段2：node 3将检索请求发送给该index中个每一个shard（这里的每一个意思是无论它是primary还是replica，它们的组合可以构成一个完整的index数据）。每个shard在本地执行检索，并将结果添加到本地优先级队列中。 子阶段3：每个shard返回本地优先级序列中所记录的_id与sort值，并发送node 3。Node 3将这些值合并到自己的本地的优先级队列中，并做全局的排序。 Fetch PhaseQuery Phase主要定位了所要检索数据的具体位置，但是我们还必须取回它们才能完成整个检索过程。而Fetch Phase阶段的任务就是将这些定位好的数据内容取回并返回给客户端。 同样也用一个例子来说明这个过程： Fetch Phase过程可以分为三个子过程来描述： 子阶段1：node 3获取了所有待检索数据的定位之后，发送一个mget的请求给与数据相关的shard。 子阶段2：每个收到node 3的get请求的shard将读取相关文档_source中的内容，并将它们返回给node 3。 子阶段3：当node 3获取到了所有shard返回的文档后，node 3将它们合并成一条汇总的结果，返回给客户端。","categories":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://blog.djstudy.net/categories/elasticsearch/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://blog.djstudy.net/tags/elasticsearch/"}]},{"title":"hbase中的RIT的那些事","slug":"hbase-rit","date":"2016-09-28T03:53:10.000Z","updated":"2016-09-28T04:01:22.000Z","comments":true,"path":"2016/09/28/hbase-rit/","link":"","permalink":"https://blog.djstudy.net/2016/09/28/hbase-rit/","excerpt":"","text":"相信长时间运维HBase集群的童鞋肯定都会对RIT（Region-In-Transition，很多参考资料误解为Region-In-Transaction，需要注意）有一种咬牙切齿的痛恨感，一旦Region处于长时间的RIT就会有些不知所措，至少以前的我就是这样过来的。正所谓“恐惧来源于未知”，不知所措意味着我们对RIT知之甚少，然而“凡事都有因果，万事皆有源头”，处于RIT状态的Region只是肉眼看到的一个结果，为什么会处于RIT状态才是问题探索的根本，也是解决问题的关键。本文就基于hbase 0.98.9版本对RIT的工作机制以及实现原理进行普及性的介绍，同时在此基础上通过真实案例讲解如何正确合理地处理处于RIT状态的Region。一方面希望大家能够更好的了解RIT机制，另一方面希望通过本文的学习之后可以不再’惧怕’RIT，正确认识处于RIT状态的Region。 Region-In-Trasition机制从字面意思来看，Region-In-Transition说的是Region变迁机制，实际上是指在一次特定操作行为中Region状态的变迁，那这里就涉及这么几个问题：Region存在多少种状态？HBase有哪些操作会触发Region状态变迁？一次正常操作过程中Region状态变迁的完整流程是怎么样的？如果Region状态在变迁的过程中出现异常又会怎么样？ Region存在多少种状态？有哪些操作会触发状态变迁？HBase在RegionState类中定义了Region的主要状态，主要有如下：上图中实际上定义了四种会触发Region状态变迁的操作以及操作对应的Region状态。其中特定操作行为通常包括assign、unassign、split以及merge等，而很多其他操作都可以拆成unassign和assign，比如move操作实际上是先unassign再assign； Region状态迁移是如何发生的？这个过程有点类似于状态机，也是通过事件驱动的。和Region状态一样，HBase还定义了很多事件（具体见EventType类）。此处以unassign过程为例说明事件是如何驱动状态变迁的，见下图： 上图所示是Region在close时的状态变迁图，其中红字部分就是发生的各种事件。可见，如果发生M_ZK_REGION_CLOSING事件，Region就会从OPEN状态迁移到PENDING_CLOSE状态，而发生RS_ZK_REGION_CLOSING事件，Region会从PENDING_CLOSE状态迁移到CLOSING状态，以此类推，发生RS_ZK_REGION_CLOSED事件，Region就会从CLOSING状态迁移到CLOSED状态。当然，除了这些事件之外，HBase还定义了很多其他事件，在此就不一一列举。截至到此，我们知道Region是一个有限状态机，那这个状态机是如何正常工作的，HMaster、RegionServer、Zookeeper又在状态机工作过程中扮演了什么角色，那就接着往下看～ 一次正常操作过程中Region状态变迁的完整流程是怎么样的？接下来本节以unassign操作为例对这个流程进行解析： 整个unassign操作是一个比较复杂的过程，涉及HMaster、RegionServer和Zookeeper三个组件： HMaster负责维护Region在整个操作过程中的状态变化，起到一个枢纽的作用。它有两个重要的HashMap数据结构，分别为regionStates和regionsInTransition，前者用来存储整个集群中所有Region及其当时状态，而后者主要存储在变迁过程中的Region及其状态，后者是前者的一个子集，不包含OPEN状态的Regions； RegionServer负责接收HMaster的指令执行具体unassign操作，实际上就是关闭region操作； Zookeeper负责存储操作过程中的事件，它有一个路径为/hbase/region-in-transition的节点。一旦一个Region发生unssign操作，就会在这个节点下生成一个子节点，子节点的内容是一个“事件”经过序列化的字符串，并且Master会监听在这个子节点上，一旦发生任何事件，Master就会监听到并更新Region的状态。 下图是整个流程示意图： HMaster先执行事件M_ZK_REGION_CLOSING并更新RegionStates，将该Region的状态改为PENDING_CLOSE，并在regionsInTransition中插入一条记录； 发送一条RPC命令给拥有该Region的RegionServer，责令其关闭该Region; RegionServer接收到HMaster发送过来的命令之后，首先生成一个RS_ZK_REGION_CLOSING事件，更新到Zookeeper，Master监听到ZK节点变动之后更新regionStates，将该Region的状态改为CLOSING; RegionServer执行真正的Region关闭操作：如果该Region正在执行flush或者compaction，等待操作完成；否则将该Region下的所有Memstore强制flush; 完成之后生成事件RS_ZK_REGION_CLOSED，更新到Zookeeper，Master监听到ZK节点变动之后更新regionStates，将该Region的状态改为CLOSED; 到这里，基本上将unssign操作过程中涉及到的Region状态变迁解释清楚了，当然，其他诸如assign操作基本类似，在此不再赘述。这里其实还有一个问题，即关于HMaster上所有Region状态是否需要持久化的问题，刚开始接触这个问题的时候想想并不需要，这些处于RIT的状态信息完全可以通过Zookeeper上/region-in-transition的子节点信息构建出来。然而，在阅读HBase Book的相关章节时，看到如下信息： 于是就充满了疑惑，一方面Master更新hbase:meta是一个远程操作，代价相对很大；另一方面Region状态内存更新和远程更新保证一致性比较困难；再者，Zookeeper上已经有相应RIT信息，再持久化一份并没有太大意义。为了对其进行确认，就查阅跟踪了一下源码，发现是否持久化取决于一个参数：hbase.assignment.usezk，默认情况下该参数为true，表示使用zk情况下并不会对Region状态进行持久化（详见RegionStateStore类），可见HBase Book的那段说明存在问题，在此特别说明～ 如果Region状态在变迁的过程中出现异常会怎么样？再回顾unassign的整个过程就会发现一次完整操作涉及太多流程，任何异常都可能会导致Region处于较长时间的RIT状态，好在HBase针对常见的异常做了最基本的容错处理： Master宕机重启：Master在宕机之后会丢失所有内存中的信息，也包括RIT信息以及Region状态信息，因此在重启之后会第一时间重建这些信息。重启之后会遍历Zookeeper上/hbase/regions-in-transition节点下的所有子节点，解析所有子节点对应的最后一个‘事件’，解析完成之后一方面借此重建全局的Region状态，另一方面根据状态机转移图对处于RIT状态的Region进行处理。比如如果发现当前Region的状态是PENDING_CLOSE，Master就会再次据此向RegionServer发送’关闭Region’的RPC命令。 其他异常宕机：HBase会在后台开启一个线程定期检查内存中处于RIT中的Region，一旦这些Region处于RIT状态的时长超过一定的阈值（由参数hbase.master.assignment.timeoutmonitor.timeout定义，默认600000ms）就会重新执行unassign或者assign操作。比如如果当前Region的状态是PENDING_CLOSE，而且处于该状态的时间超过了600000ms，Master就会重新执行unassign操作，向RegionServer再次发送’关闭Region’的RPC命令。 可见，HBase提供了基本的重试机制，保证在一些短暂异常的情况下能够通过不断重试拉起那些处于RIT状态的Region，进而保证操作的完整性和状态的一致性。然而不幸的是，因为各种各样的原因，很多Region还是会掉入长时间的RIT状态，甚至是永久的RIT状态，必须人为干预才能解决，下面一节内容让我们看看都有哪些常见的场景会导致Region会处于永久RIT状态，以及遇到这类问题应该如何解决。 永久RIT状态案例分析通过RIT机制的了解，其实可以发现处于RIT状态Region并不是什么怪物，大部分处于RIT状态的Region都是短暂的，即使在大多数短暂异常的情况下HBase也提供了重试机制保证Region能够很快恢复正常。然而在一些特别极端的场景下还是会发生一些异常导致部分Region掉入永久的RIT状态，进而会引起表读写阻塞甚至整个集群的读写阻塞。下面我们举两个相关的案例进行说明： 案例一：Compaction永久阻塞现象：线上一个集群因为未知原因忽然就卡住了，读写完全进不来了；另外还有很多处于PENDING_CLOSE状态的Region。 分析：集群卡住常见原因无非两个，一是Memstore总消耗内存大小超过了上限进而触发RegionServer级别flush，此时系统会阻塞集群执行长时间flush操作；二是storefile数量过多超过设定的上限阈值（参见：hbase.hstore.blockingStoreFiles），此时系统会阻塞所有flush请求而执行compaction。 诊断：（1）首先查看了各个RegionServer上的Memstore使用大小，并没有达到设定的upperLimit。 （2）再查看了一下所有RegionServer的storefile数量，瞬间石化了，store数为250的RegionServer上storefile数量竟然达到了1.5w+，很多单个store的storefile都超过了设定阈值100 （3）初步怀疑是因为storefile数量过多引起的，看到这么多storefile的第一反应是手动执行major_compaction，然而所有的compact命令好像都没有起任何作用 （4）无意中发现所有RegionServer的Compaction任务都是同一张表music_actions的，而且Compaction时间都基本持续了一两天。到此基本可以确认是因为表music_actions的Compaction任务长时间阻塞，占用了所有的Compaction线程资源，导致集群中所有其他表都无法执行Compaction任务，最后导致StoreFile大量堆积 （5）那为什么会存在PENDING_CLOSE状态的Region呢？经查看，这些处于PENDING_CLOSE状态的Region全部来自于表music_actions，进一步诊断确认是由于在执行graceful_stop过程中unassign时遇到Compaction长时间阻塞导致RegionServer无法执行Region关闭（参考上文unassign过程），因而掉入了永久RIT 解决方案：（1）这个问题中RIT和集群卡住原因都在于music_actions这张表的Compaction阻塞，因此需要定位Compaction阻塞的具体原因。经过一段时间的定位初步怀疑是因为这张表的编码导致，anyway，具体原因不重要，因为一旦Compaction阻塞，好像是没办法通过正常命令解除这种阻塞的。临时有用的办法是增大集群的Compaction线程，以期望有更多空闲线程可以处理集群中其他Compaction任务，消化大量堆积的StoreFiles （2）而永久性消灭这种Compaction阻塞只能先将这张表数据迁移出来，然后将这张表暴力删除。暴力删除就是先将HDFS对应文件删除，再将hbase:meta中该表对应的相关数据清除，最后重启整个集群即可。这张表删除之后使用hbck检查一致性之后，集群Compaction阻塞现象就消失了，集群就完全恢复正常。 案例二：HDFS文件异常现象：线上集群很多RegionServer短时间内频频宕机，有几个Region处于FAILED_OPEN状态 分析诊断： （1）查看系统监控以及RegionServer日志，确认RegionServer频繁宕机是因为大量CLOSE_WAIT状态的短连接导致。监控显示短时间内（4h）CLOSE_WAIT的数量从0增长到6w+。 （2）再查看RegionServer日志查看到如下日志： 1234562016-07-27 09:42:14,932 [RS_OPEN_REGION-inspur250.photo.163.org,60020,1469581282053-0] ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler - Failed open of region=news_user_actions,|u:cfcd208495d565ef66e7dff9f98764da|1462799167|30671473410714402,1469522128310.3b3ae24c65fc5094bc2acfebaa7a56de., starting to roll back the global memstore size.java.io.IOException: java.io.IOException: java.io.FileNotFoundException: File does not exist: /hbase/news_user_actions/b7b3faab86527b88a92f2a248a54d3dc/meta/0f47cda55fa44cf9aa2599079894aed62016-07-27 09:42:14,934 [RS_OPEN_REGION-inspur250.photo.163.org,60020,1469581282053-0] INFO org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler - Opening of region &#123;NAME =&gt; &apos;news_user_actions,|u:cfcd208495d565ef66e7dff9f98764da|1462799167|30671473410714402,1469522128310.3b3ae24c65fc5094bc2acfebaa7a56de.&apos;, STARTKEY =&gt; &apos;|u:cfcd208495d565ef66e7dff9f98764da|1462799167|30671473410714402&apos;, ENDKEY =&gt; &apos;|u:d0&apos;, ENCODED =&gt; 3b3ae24c65fc5094bc2acfebaa7a56de,&#125; failed, marking as FAILED_OPEN in ZK 日志显示，Region ‘3b3ae24c65fc5094bc2acfebaa7a56de’打开失败，因此状态被设置为FAILED_OPEN，原因初步认为是FileNotFoundException导致，找不到的文件是Region ‘b7b3faab86527b88a92f2a248a54d3dc’ 下的一个文件，这两者之间有什么联系呢？ （3）使用hbck检查了一把，得到如下错误信息：1ERROR: Found lingering reference file hdfs://mycluster/hbase/news_user_actions/3b3ae24c65fc5094bc2acfebaa7a56de/meta/0f47cda55fa44cf9aa2599079894aed6.b7b3faab86527b88a92f2a248a54d3dc 看到这里就一下恍然大悟，从引用文件可以看出来，Region ‘3b3ae24c65fc5094bc2acfebaa7a56de’是‘ b7b3faab86527b88a92f2a248a54d3dc’的子Region，熟悉Split过程的童鞋就会知道，父Region分裂成两个子Region其实并没有涉及到数据文件的分裂，而是会在子Region的HDFS目录下生成一个指向父Region目录的引用文件，直到子Region执行Compaction操作才会将父Region的文件合并过来。 到这里，就可以理解为什么子Region会长时间处于FAILED_OPEN状态：因为子Region引用了父Region的文件，然而父Region的文件因为未知原因丢失了，所以子Region在打开的时候因为找不到引用文件因而会失败。而这种异常并不能通过简单的重试可以解决，所以会长时间掉入RIT状态。 （4）现在基本可以通过RegionServer日志和hbck日志确定Region处于FAILED_OPEN的原因是因为子Region所引用的父Region的文件丢失导致。那为什么会出现CLOSE_WAIT数量暴涨的问题呢？经确认是因为Region在打开的时候会读取Region对应HDFS相关文件，但因为引用文件丢失所以读取失败，读取失败之后系统会不断重试，每次重试都会同datanode建立短连接，这些短连接因为hbase的bug一直得不到合理处理就会引起CLOSEE_WAIT数量暴涨。 解决方案：删掉HDFS上所有检查出来的引用文件即可 案例分析经过上面两个案例的讲解其实看出得出这么几点： 永久性掉入RIT状态其实出现的概率并不高，都是在一些极端情况下才会出现。绝大部分RIT状态都是暂时的。 一旦掉入永久性RIT状态，说明一定有根本性的问题原因，只有定位出这些问题才能彻底解决问题 如果Region长时间处于PENDING_CLOSE或者CLOSING状态，一般是因为RegionServer在关闭Region的时候遇到了长时间Compaction任务或Flush任务，所以如果Region在做类似于Major_Compact的操作时尽量不要执行unassign操作，比如move操作、disable操作等；而如果Region长时间处于FAILED_OPEN状态，一般是因为HDFS文件出现异常所致，可以通过RegionServer日志以及hbck定位出来 写在文章最后RIT在很多运维HBase的人看来是一个很神秘的东西，这是因为RIT很少出现，而一旦出现就很致命，运维起来往往不知所措。本文就希望能够打破这种神秘感，还原它的真实本性。文章第一部分通过层层递进的方式介绍了Region-In-Transition机制，第二部分通过生产环境的真实案例分析永久性RIT出现的场景以及应对的方案。希望大家能够更多的了解RIT，通过不断的运维实践最后再也不用惧怕它～～ 转载 http://hbasefly.com/2016/09/08/hbase-rit/","categories":[{"name":"big data","slug":"big-data","permalink":"https://blog.djstudy.net/categories/big-data/"}],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://blog.djstudy.net/tags/hbase/"},{"name":"rit","slug":"rit","permalink":"https://blog.djstudy.net/tags/rit/"}]},{"title":"comb sort(梳排序)","slug":"comb-sort","date":"2016-09-21T01:06:53.000Z","updated":"2016-09-21T01:17:17.000Z","comments":true,"path":"2016/09/21/comb-sort/","link":"","permalink":"https://blog.djstudy.net/2016/09/21/comb-sort/","excerpt":"","text":"Comb sort is a comparison sorting algorithm improves on Bubble sort. The basic idea is to eliminate turtles, or small values near the end of the list, since in a bubble sort these slow the sorting down tremendously.Worst case performance - O(n^2) C++1234567891011121314151617181920212223242526void comb_sort(int array[], int length)&#123; int gap = length; bool swapped = true; while((gap &gt; 1) || (swapped == true)) &#123; gap /= 1.25; if (gap &lt; 1) &#123; gap = 1; &#125; int i = 0; swapped = false; while (i + gap &lt; length) &#123; if (array[i] &gt; array[i + gap]) &#123; int temp = array[i]; array[i] = array[i + gap]; array[i + gap] = temp; swapped = true; &#125; i++; &#125; &#125;&#125; Java12345678910111213141516171819202122 public int[] comb_sort(int[] array) &#123; int gap = array.length; boolean swapped = true; while ((gap &gt; 1) || (swapped == true)) &#123; gap /= 1.25; if (gap &lt; 1) &#123; gap = 1; &#125; int i = 0; swapped = false; while (i + gap &lt; array.length) &#123; if (array[i] &gt; array[i + gap]) &#123; int temp = array[i]; array[i] = array[i + gap]; array[i + gap] = temp; swapped = true; &#125; i++; &#125; &#125; return array;&#125; Matlab12345678910111213141516171819function array = comb_sort(array) gap = numel(array); swapped = true; while (gap &gt; 1) || swapped gap = floor(gap/1.25); if gap &lt; 1 gap = 1; end i = 1; swapped = false; while (i+gap-1) &lt; numel(array) if array(i) &gt; array(i+gap) array([i i+gap]) = array([i+gap i]); swapped = true; end i = i + 1; end endend Python import math def combsort(input): gap = len(input) swaps = True while gap &gt; 1 or swaps: gap = max(1, int(gap / 1.25)) # minimum gap is 1 swaps = False for i in range(len(input) - gap): j = i+gap if input[i] &gt; input[j]: input[i], input[j] = input[j], input[i] swaps = True","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://blog.djstudy.net/categories/数据结构与算法/"}],"tags":[{"name":"sort","slug":"sort","permalink":"https://blog.djstudy.net/tags/sort/"}]},{"title":"hbase 压缩合并中的minor与major区别","slug":"hbase-minor-vs-major-compaction","date":"2016-07-27T00:23:59.000Z","updated":"2016-07-27T00:33:36.000Z","comments":true,"path":"2016/07/27/hbase-minor-vs-major-compaction/","link":"","permalink":"https://blog.djstudy.net/2016/07/27/hbase-minor-vs-major-compaction/","excerpt":"","text":"HRegoin Server上的storefile文件是被后台线程监控的，以确保这些文件保持在可控状态。磁盘上的storefile的数量会随着越来越多的memstore被刷新而变等于来越多——每次刷新都会生成一个storefile文件。当storefile数量满足一定条件时（可以通过配置参数类调整），会触发文件合并操作——minor compaction，将多个比较小的storefile合并成一个大的storefile文件，直到合并的文件大到超过单个文件配置允许的最大值时会触发一次region的自动分割，即region split操作，将一个region平分成2个，具体过程以后再说，这里不再赘述。 合并操作有两种类型：轻量级的的minor compaction和重量级的major compaction。Minor compaction主要负责将符合条件的最早生成的几个storefile合并生成一个大的storefile文件，它不会删除被标记为”删除”的数据和以过期的数据，并且执行过一次minor合并操作后还会有多个storefile文件。Minor compaction一次合并的文件数量由hbase.hstore.compaction.min(执行minor compaction的最少文件数)配置参数决定，该参数值的默认配置是3。该参数配置太大则会延迟触发minor合并操作，并且一次合并的文件数太多会占用更多的资源和执行更长的时间，这会带来不好的用户体验——毕竟hbase是要提供实时响应的。在一次minor操作一次最多允许10个文件，通过hbase.hstore.compaction.max参数设置，任何一个大于hbase.hstore.compaction.min.size值的storefile文件将自动成为将要被合并的storefile，hbase.hstore.compaction.min.size属性值与被用来设置将memstore执行flush操作的配置属性hbase.hregion.memstore.flush.size的值(默认为128MB)相同。Minor compaction操作有一个时间轴而的概念，那就是每次合并操作都是按storefile的生成时间有旧到新来合并文件的。如此下图所示： 对比Minor compaction，Major compaction操作会把所有的storefile合并成一个单一的storefile文件，在文件合并期间系统会删除标记为”删除”标记的数据和过期失效的数据，同时会block所有客户端对该操作所属的region的请求直到合并完毕，最后删除已合并的storefile文件。 到底如何决定触发那种类型的major类型的compaction操作呢？这是在compaction检查执行时被自动决定的。compaction检查可以通过以下三种条件触发：1、每当memstore被刷新到磁盘后触发；2、通过hbase shell命令行调用或API调用触发；3、通过一个叫CompacionChecker的后台线程触发。每一个region都运行着一个这样的后台线程。CompacionChecker会定期的执行compation检查，时间间隔可以通过hbase.server.thread.wakefrequency来配置。可以通过hbase shell命令行调用或majorCompact()API调用，从而强迫major合并操作执行，否则服务器会首先基于hbase.hregion.majorcompaction(24小时)的配置来检查是否要执行major合并操作。由于Major compaction在执行期间会阻塞所有客户端的请求直到合并完毕，因此最好在服务器空闲时通过手工或脚本的方式调用执行，以提高客户体验。 以下是两种compaction的区别： 转载 http://flyingdutchman.iteye.com/blog/1846031","categories":[{"name":"big data","slug":"big-data","permalink":"https://blog.djstudy.net/categories/big-data/"}],"tags":[{"name":"hbase","slug":"hbase","permalink":"https://blog.djstudy.net/tags/hbase/"},{"name":"compaction","slug":"compaction","permalink":"https://blog.djstudy.net/tags/compaction/"}]},{"title":"elasticsearch中的refresh和flush区别","slug":"es-flush-vs-refresh","date":"2016-07-26T23:18:40.000Z","updated":"2016-07-26T23:23:03.000Z","comments":true,"path":"2016/07/27/es-flush-vs-refresh/","link":"","permalink":"https://blog.djstudy.net/2016/07/27/es-flush-vs-refresh/","excerpt":"","text":"elasticsearch中有两个比较重要的操作：refresh 和 flushrefresh操作当我们向ES发送请求的时候，我们发现es貌似可以在我们发请求的同时进行搜索。而这个实时建索引并可以被搜索的过程实际上是一次es 索引提交（commit）的过程，如果这个提交的过程直接将数据写入磁盘（fsync）必然会影响性能，所以es中设计了一种机制，即：先将index-buffer中文档（document）解析完成的segment写到filesystem cache之中，这样避免了比较损耗性能io操作，又可以使document可以被搜索。以上从index-buffer中取数据到filesystem cache中的过程叫做refresh。 refresh操作可以通过API设置：POST /index/_settings{“refresh_interval”: “10s”}当我们进行大规模的创建索引操作的时候，最好将将refresh关闭。POST /index/_settings{“refresh_interval”: “-1″}es默认的refresh间隔时间是1s，这也是为什么ES可以进行近乎实时的搜索。 flush操作与translog我们可能已经意识到如果数据在filesystem cache之中是很有可能在意外的故障中丢失。这个时候就需要一种机制，可以将对es的操作记录下来，来确保当出现故障的时候，保留在filesystem的数据不会丢失，并在重启的时候可以从这个记录中将数据恢复过来。elasticsearch提供了translog来记录这些操作。当向elasticsearch发送创建document索引请求的时候，document数据会先进入到index buffer之后，与此同时会将操作记录在translog之中，当发生refresh时（数据从index buffer中进入filesystem cache的过程）translog中的操作记录并不会被清除，而是当数据从filesystem cache中被写入磁盘之后才会将translog中清空。而从filesystem cache写入磁盘的过程就是flush。可能有点晕，我画了一个图帮大家理解这个过程： 总结一下translog的功能：1.保证在filesystem cache中的数据不会因为elasticsearch重启或是发生意外故障的时候丢失。2.当系统重启时会从translog中恢复之前记录的操作。3.当对elasticsearch进行CRUD操作的时候，会先到translog之中进行查找，因为tranlog之中保存的是最新的数据。4.translog的清除时间时进行flush操作之后（将数据从filesystem cache刷入disk之中）。再总结一下flush操作的时间点： 1.es的各个shard会每个30分钟进行一次flush操作。2.当translog的数据达到某个上限的时候会进行一次flush操作。有关于translog和flush的一些配置项： index.translog.flush_threshold_ops:当发生多少次操作时进行一次flush。默认是 unlimited。index.translog.flush_threshold_size:当translog的大小达到此值时会进行一次flush操作。默认是512mb。index.translog.flush_threshold_period:在指定的时间间隔内如果没有进行flush操作，会进行一次强制flush操作。默认是30m。index.translog.interval:多少时间间隔内会检查一次translog，来进行一次flush操作。es会随机的在这个值到这个值的2倍大小之间进行一次操作，默认是5s。参考：http://www.elastic.co/guide/en/elasticsearch/guide/current/near-real-time.html http://www.elastic.co/guide/en/elasticsearch/guide/current/translog.html 转载 http://www.yubingzhe.com/143.html","categories":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://blog.djstudy.net/categories/elasticsearch/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://blog.djstudy.net/tags/elasticsearch/"}]},{"title":"scala学习笔记(三)","slug":"scala-note-3","date":"2016-07-10T13:08:00.000Z","updated":"2016-07-10T13:17:52.000Z","comments":true,"path":"2016/07/10/scala-note-3/","link":"","permalink":"https://blog.djstudy.net/2016/07/10/scala-note-3/","excerpt":"","text":"如何使用 scala 的range 函数在 scala 应用中可以看到有不同的方法使用 range Range 在常用的数据结构中经常用到，用于循环中的迭代元素，并且提供了几个强大的方法1234567891011121314151617181920212223242526272829303132333435scala&gt; 1 to 10res0: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; 1 until 10res1: scala.collection.immutable.Range = Range(1, 2, 3, 4, 5, 6, 7, 8, 9)scala&gt; 1 to 10 by 2res2: scala.collection.immutable.Range = Range(1, 3, 5, 7, 9)scala&gt; 'a' to 'c'res3: collection.immutable.NumericRange.Inclusive[Char] = NumericRange(a, b, c)scala&gt; val x = (1 to 10).toListx: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; val x = (1 to 10).toArrayx: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)scala&gt; val x = (1 to 10).toSetx: scala.collection.immutable.Set[Int] = Set(5, 10, 1, 6, 9, 2, 7, 3, 8, 4)scala&gt; val x = Array.range(1, 10)x: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)scala&gt; val x = Vector.range(1, 10)x: collection.immutable.Vector[Int] = Vector(1, 2, 3, 4, 5, 6, 7, 8, 9)scala&gt; val x = List.range(1, 10)x: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9)scala&gt; val x = List.range(0, 10, 2)x: List[Int] = List(0, 2, 4, 6, 8)scala&gt; val x = collection.mutable.ArrayBuffer.range('a', 'd')x: scala.collection.mutable.ArrayBuffer[Char] = ArrayBuffer(a, b, c) 也可以用tabulate方法实现填充集合12345678scala&gt; val x = List.tabulate(5)(_ + 1)x: List[Int] = List(1, 2, 3, 4, 5)scala&gt; val x = List.tabulate(5)(_ + 2)x: List[Int] = List(2, 3, 4, 5, 6)scala&gt; val x = Vector.tabulate(5)(_ * 2)x: scala.collection.immutable.Vector[Int] = Vector(0, 2, 4, 6, 8)","categories":[{"name":"scala","slug":"scala","permalink":"https://blog.djstudy.net/categories/scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"https://blog.djstudy.net/tags/scala/"}]},{"title":"scala学习笔记(二)","slug":"scala-note-2","date":"2016-03-19T02:23:48.000Z","updated":"2016-03-21T13:05:41.000Z","comments":true,"path":"2016/03/19/scala-note-2/","link":"","permalink":"https://blog.djstudy.net/2016/03/19/scala-note-2/","excerpt":"","text":"scala访问修饰符包，类或对象的成员可以标记访问修饰符private和protected，如果我们不使用这两种关键字，那么访问将被默认设置为public。这些修饰 限制为成员的代码的某些区域访问。 私有成员123456789class Outer &#123; class Inner &#123; private def f() &#123; println(\"f\") &#125; class InnerMost &#123; f() // OK &#125; &#125; (new Inner).f() // Error: f is not accessible&#125; 在Scala中，访问 (new Inner).f() 是非法的，因为f被声明为private内部类并且访问不是在内部类内。与此相反，到f第一接入类最内层是确定的，因为该访问包含在类内的主体。 Java将允许这两种访问，因为它可以让其内部类的外部类访问私有成员。 保护成员在 scala 中，对保护（Protected）成员的访问比 java 更严格一些。因为它只允许保护成员在定义了该成员的的类的子类中被访问。而在java中，用protected关键字修饰的成员，除了定义了该成员的类的子类可以访问，同一个包里的其他类也可以进行访问。1234567891011package p &#123; class Super &#123; protected def f() &#123; println(\"f\") &#125; &#125; class Sub extends Super &#123; f() &#125; class Other &#123; (new Super).f() // Error: f is not accessible &#125;&#125; 上例中，Sub 类对 f 的访问没有问题，因为 f 在 Super 中被声明为 protected，而 Sub 是 Super 的子类。相反，Other 对 f 的访问不被允许，因为 other 没有继承自 Super。而后者在 java 里同样被认可，因为 Other 与 Sub 在同一包里。 公共(Public)成员Scala中，如果没有指定任何的修饰符，则默认为 public。这样的成员在任何地方都可以被访问。123456789class Outer &#123; class Inner &#123; def f() &#123; println(\"f\") &#125; class InnerMost &#123; f() // 正确 &#125; &#125; (new Inner).f() // 正确因为 f() 是 public&#125; 作用域保护Scala中，访问修饰符可以通过使用限定词强调。格式为:private[x] 或 protected[x]这里的x指代某个所属的包、类或单例对象。如果写成private[x],读作”这个成员除了对[…]中的类或[…]中的包中的类及它们的伴生对像可见外，对其它所有类都是private。这种技巧在横跨了若干包的大型项目中非常有用，它允许你定义一些在你项目的若干子包中可见但对于项目外部的客户却始终不可见的东西。1234567891011121314151617package bobsrocckets&#123; package navigation&#123; private[bobsrockets] class Navigator&#123; protected[navigation] def useStarChart()&#123;&#125; class LegOfJourney&#123; private[Navigator] val distance = 100 &#125; private[this] var speed = 200 &#125; &#125; package launch&#123; import navigation._ object Vehicle&#123; private[launch] val guide = new Navigator &#125; &#125;&#125; 上述例子中，类Navigator被标记为private[bobsrockets]就是说这个类对包含在bobsrockets包里的所有的类和对象可见。比如说，从Vehicle对象里对Navigator的访问是被允许的，因为对象Vehicle包含在包launch中，而launch包在bobsrockets中，相反，所有在包bobsrockets之外的代码都不能访问类Navigator。","categories":[{"name":"scala","slug":"scala","permalink":"https://blog.djstudy.net/categories/scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"https://blog.djstudy.net/tags/scala/"}]},{"title":"scala学习笔记(一)","slug":"scala-note-1","date":"2016-03-16T08:05:41.000Z","updated":"2016-03-21T13:02:36.000Z","comments":true,"path":"2016/03/16/scala-note-1/","link":"","permalink":"https://blog.djstudy.net/2016/03/16/scala-note-1/","excerpt":"","text":"基础语法区分大小写 - Scala是大小写敏感的，这意味着标识Hello 和 hello在Scala中会有不同的含义类名 - 对于所有的类名的第一个字母要大写。如果需要使用几个单词来构成一个类的名称，每个单词的第一个字母要大写。示例：class MyFirstScalaClass方法名称 - 所有的方法名称的第一个字母用小写。如果若干单词被用于构成方法的名称，则每个单词的第一个字母应大写。示例：def myMethodName()程序文件名 - 程序文件的名称应该与对象名称完全匹配。保存文件时，应该保存它使用的对象名称（记住Scala是区分大小写），并追加“.scala”为文件扩展名。 （如果文件名和对象名称不匹配，程序将无法编译）。 示例: 假设“HelloWorld”是对象的名称。那么该文件应保存为’HelloWorld.scala“def main(args: Array[String]) - Scala程序从main()方法开始处理，这是每一个Scala程序的强制程序入口部分。 字符串Scala中单引号和双引号包裹是有区别的，单引号用于字符，双引号用于字符串。12345678scala&gt; val c1 = 'c'c1: Char = cscala&gt; val 字符2 = '杨'字符2: Char = 杨scala&gt; val s1 = \"scala基础语法\"s1: String = scala基础语法 Scala修饰符:所有的Scala的组件需要名称。使用对象，类，变量和方法名被称为标识符。关键字不能用作标识符和标识是区分大小写的。Scala支持以下四种类型标识符： 文字标识符字母数字标识符开始以字母或下划线，可以使用字母，数字或下划线。“$”字符在Scala中是保留关键字，标识符不能使用。以下是合法的字母标识符： age, salary, _value, __1_value以下是非法标识符： $salary, 123abc, -salary 运算符标识运算符识别符由一个或多个运算符字符。操作字符是可打印的ASCII字符，如+, :, ?, ~ 或#。以下是合法的运算符标识： ++ ::: &lt;?&gt; :&gt;Scala编译器将在内部“轧”操作符标识符使它们成为合法的Java标识符，并嵌入$字符。例如，所述标识符:-&gt;将内部表示为$colon$minus$greater。 混合标识符混合标识符由一个字母数字识别符，随后是一个下划线和运算符标识。以下是合法的混合标识符： unary+, myvar=在这里，作为一个方法名unary+定义了一个一元+运算符和myvar=用来作为方法名称定义了一个赋值运算符。 立即数标识符一个文字标识是包含在反引号(. . .)的任意字符串。以下是合法的文字标识：12345scala&gt; def g(x : Int) = 5 match &#123; case `x` =&gt; \"yup\"; case _ =&gt; \"nope\"&#125;g: (x: Int)java.lang.Stringscala&gt; g(5)res3: java.lang.String = yup Scala 包定义包Scala 使用 package 关键字定义包，在Scala将代码定义到某个包中有两种方式：第一种方法和 Java 一样，在文件的头定义包名，这种方法就后续所有代码都放在该报中。 比如：12package com.runoobclass HelloWorld 第二种方法有些类似 C#，如：123package com.runoob &#123; class HelloWorld &#125; 第二种方法，可以在一个文件中定义多个包。 引用Scala 使用 import 关键字引用包。1234567import java.awt.Color // 引入Color import java.awt._ // 引入包内所有成员 def handler(evt: event.ActionEvent) &#123; // java.awt.event.ActionEvent ... // 因为引入了java.awt，所以可以省去前面的部分&#125; import语句可以出现在任何地方，而不是只能在文件顶部。import的效果从开始延伸到语句块的结束。这可以大幅减少名称冲突的可能性。如果想要引入包中的几个成员，可以使用selector（选取器）：1234567import java.awt.&#123;Color, Font&#125; // 重命名成员import java.util.&#123;HashMap =&gt; JavaHashMap&#125; // 隐藏成员import java.util.&#123;HashMap =&gt; _, _&#125; // 引入了util包的所有成员，但是HashMap被隐藏了 注意：默认情况下，Scala 总会引入 java.lang. 、 scala. 和 Predef._，这里也能解释，为什么以scala开头的包，在使用时都是省去scala.的。","categories":[{"name":"scala","slug":"scala","permalink":"https://blog.djstudy.net/categories/scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"https://blog.djstudy.net/tags/scala/"}]},{"title":"Optimizing Elasticsearch-How Many Shards per Index","slug":"how-many-shards-index","date":"2016-02-27T02:58:40.000Z","updated":"2016-02-27T02:58:52.000Z","comments":true,"path":"2016/02/27/how-many-shards-index/","link":"","permalink":"https://blog.djstudy.net/2016/02/27/how-many-shards-index/","excerpt":"","text":"A key question in the minds of most Elasticsearch users when they create an index is “How many shards should I use?” In this article, we explain the design tradeoffs and performance consequences of choosing different values for the number of shards. Continue reading if you want to learn how to demystify and optimize your sharding strategy. Why Bother?This is an important topic, and many users are apprehensive as they approach it – and for good reason. A major mistake in shard allocation could cause scaling problems in a production environment that maintains an ever-growing dataset. On the other hand, we know that there is little Elasticsearch documentation on this topic. Most users just want answers – and they want specific answers, not vague number ranges and warnings for arbitrarily large numbers. Well, we have some answers. After covering a few definitions and some clarifications, we present several common use cases and provide our recommendations for each. DefinitionsIf you’re fairly new to Elasticsearch, it’s important that you understand the basic jargon and grasp the elemental concepts. (If you have some expertise with ES, you might want to skip to the next section.) Consider this simple diagram of an Elasticsearch cluster:”&gt;”&gt;Remember these definitions while refering to this diagram: “&gt; cluster – An Elasticsearch cluster consists of one or more nodes and is identifiable by its cluster name. node – A single Elasticsearch instance. In most environments, each node runs on a separate box or virtual machine. index – In Elasticsearch, an index is a collection of documents. shard – Because Elasticsearch is a distributed search engine, an index is usually split into elements known as shards that are distributed across multiple nodes. Elasticsearch automatically manages the arrangement of these shards. It also rebalances the shards as necessary, so users need not worry about the details. replica – By default, Elasticsearch creates five primary shards and one replica for each index. This means that each index will consist of five primary shards, and each shard will have one copy. Allocating multiple shards and replicas is the essence of the design for distributed search capability, providing for high availability and quick access in searches against the documents within an index. The main difference between a primary and a replica shard is that only the primary shard can accept indexing requests. Both replica and primary shards can serve querying requests. In the diagram above, we have an Elasticsearch cluster consisting of two nodes in a default shard configuration. Elasticsearch automatically arranges the five primary shards split across the two nodes. There is one replica shard that corresponds to each primary shard, but the arrangement of these replica shards is altogether different from that of the primary shards. Again, think distribution. Allow us to clarify: Remember, the number_of_shards value pertains to indexes—not to the cluster as whole. This value specifies the number of shards for each index(not the total primary shards in the cluster). A Word about ReplicasWe don’t elaborate in this article on Elasticsearch replicas. That is an entirely separate topic that we cover elsewhere. Replicas are primarily for search performance, and a user can add or remove them at any time. As we explain in that article, additional replicas give you additional capacity, higher throughput, and stronger failover. Allocate Shards CarefullyAfter you configure an Elasticsearch cluster, it’s critically important to realize that you cannot modify the shard allocation later. If you later find it necessary to change the number of shards, then you would need to reindex all the source documents. (Although reindexing is a long process, it can be done without downtime). The primary shard configuration is quite analogous to a hard disk partition, in which a repartition of raw disk space requires a user to back up, configure a new partition, and rewrite data onto the new partition. Small Static Dataset, 2-3 GBThe key consideration as you allocate shards is your expectation for the growth of your dataset. We quite often see the tendency to unnecessarily overallocate on shard count. Since share count such a hot topic within the ES community, users may assume that overallocation is a safe bet. (By overallocation, we simply mean specifying more shards per index than is necessary for the current size (document count) for a particular dataset.) Elastic was promoting this idea in the early days, but then many users began taking it too far—such as allocating 1,000 shards. Elastic now provides a bit more cautious rationale: “A little overallocation is good. A kagillion shards is bad. It is difficult to define what constitutes too many shards, as it depends on their size and how they are being used. A hundred shards that are seldom used may be fine, while two shards experiencing very heavy usage could be too many.”Remember that there is an additional cost for each shard that you allocate: “&gt; “&gt; Since a shard is essentially a Lucene index, it consumes file handles, memory, and CPU resources.Each search request will touch a copy of every shard in the index, which isn’t a problem when the shards are spread across several nodes. Contention arises and performance decreases when the shards are competing for the same hardware resources.Elasticsearch uses term frequency statistics to calculate relevance, but these statistics correspond to individual shards. Maintaining only a small amount of data across a many shards will tend to result in poor document relevance.Our customers expect their businesses to grow and their datasets to expand accordingly. There is therefore always a need for contingency planning. Many users convince themselves that they’ll encounter explosive growth (although most never actually see an unmanageable spike). In addition, we all want to minimize downtime and avoid resharding. If you worry about rapid data growth, then we suggest a focus on a simple constraint: the maximum JVM heap size recommendation for Elasticsearch is approximately 30-32GB. This is a solid estimate on the limit of your absolute maximum shard size. For example, if you really think it possible that you could reach 200GB (but not much further without other infrastructure changes), then we recommend an allocation of 7 shards, or 8 shards at most. By all means, don’t allocate for an inappropriately high goal of 10 terabytes that you might attain three years from now. It’s likely that you’ll see some performance strain—sooner than you like. Although we aren’t explaining replicas in detail here, we do recommend that you plan for a modest number of shards and consider increasing the number of replicas. If you’re configuring a new environment, then perhaps you want to have a look at our replicated clusters. With a replicated cluster, you get a three-node cluster that includes one replica with an option to easily increase the number of replicas as your requirements change. Large and Growing DatasetWe strongly encourage you to rely on over-allocation for large datasets—but only modestly. You can still use the 30GB maximum shard size guideline that we give above. We do, however, suggest that you continue to picture the ideal scenario as being one shard per index, per node. A good launch point for capacity planning is to allocate shards with a factor of 1.5 to 3 times the number of nodes in your initial configuration. If you’re starting with 3 nodes, then we recommend that you specify at most 3 x 3 = 9 shards. Your shard size may be getting too high if you’re discovering issues through the cluster stats APIs or encountering minor performance degradations. If this is the case, simply add a node and ES will will rebalance the shards acccordingly. Once again, please note that we’re omitting the specification of replicas from our discussion here. The same ideal shard guideline of one shard per index per node also holds true for replica shards. So if you need only one replica, then you’ll need twice as many nodes. Two replicas would require three times the number of nodes. For more details, see our article on Replicated Clusters. “&gt; LogstashDo you accummulate daily indices and yet incur only small search loads? Perhaps these indices number in the hundreds, but each index is 1GB or smaller. For these and similar problem spaces, our simple recommendation is that you choose one shard. If you roll with the defaults for Logstash (daily indices) and ES (5 shards), you could generate up to 890 shards in 6 months. Further, your cluster will be hurting—unless you have 15 nodes or more. Think about it: most Logstash users are infrequent searchers, performing fewer than one query per minute. Accordingly, we recommend a simple economical setup. Since search performance isn’t a primary requirement for such cases, we don’t need multiple replicas. A single replica is enough for basic redundancy. The data-to-memory ratio can also be quite high. If you go with a single shard per index, then you could probably run a Logstash configuration for 6 months on a three-node cluster. Ideally, you’d use at least 4GB, but we’d recommend 8GB because 8GB is where network speed starts to get significantly better on most cloud platforms and much less resource-sharing. ConclusionWe reiterate that shards consume resources and require processing overhead. To compile results from an index consisting of more than one shard, Elasticsearch must query each shard individually (although in parallel), and then it must perform operations on the aggregated results. Because of this, a machine with more IO headroom (SSDs) and a multi-core processor can definitely benefit from sharding, but you must consider the size, volatility, and future states of your dataset. While there is no one-size-for-all with respect to shard allocation, we hope that you can benefit from this discussion.","categories":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://blog.djstudy.net/categories/elasticsearch/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://blog.djstudy.net/tags/elasticsearch/"},{"name":"optimizing","slug":"optimizing","permalink":"https://blog.djstudy.net/tags/optimizing/"}]},{"title":"Service discovery with consul and consul-template","slug":"service-discovery-consul-and-consul-template","date":"2016-02-19T03:27:26.000Z","updated":"2016-02-19T03:37:46.000Z","comments":true,"path":"2016/02/19/service-discovery-consul-and-consul-template/","link":"","permalink":"https://blog.djstudy.net/2016/02/19/service-discovery-consul-and-consul-template/","excerpt":"","text":"I talked in the past about an “Ops Design Pattern: local haproxy talking to service layer”. I described how we used a local haproxy on pretty much all nodes at a given layer of our infrastructure (webapp, API, e-commerce) to talk to services offered by the layer below it. So each webapp server has a local haproxy that talks to all API nodes it sends requests to. Similarly, each API node has a local haproxy that talks to all e-commerce nodes it needs info from. This seemed like a good idea at a time, but it turns out it has a couple of annoying drawbacks:each local haproxy runs health checks against N nodes, so if you have M nodes running haproxy, each of the N nodes will receive M health checks; if M and N are large, then you have a health check storm on your handsto take a node out of a cluster at any given layer, we tag it as ‘inactive’ in Chef, then run chef-client on all nodes that run haproxy and talk to the inactive node at layers above it; this gets old pretty fast, especially when you’re doing anything that might conflict with Chef and that the chef-client run might overwrite (I know, I know, you’re not supposed to do anything of that nature, but we are all human :-)For the second point, we are experimenting with haproxyctl so that we don’t have to run chef-client on every node running haproxy. But it still feels like a heavy-handed approach. If I were to do this again (which I might), I would still have an haproxy instance in front of our webapp servers, but for communicating from one layer of services to another I would use a proper service discovery tool such as grampa Apache ZooKeeper or the newer kids on the block, etcd from CoreOS and consul from HashiCorp. I settled on consul for now, so in this post I am going to show how you can use consul in conjunction with the recently released consul-template to discover services and to automate configuration changes. At the same time, I wanted to experiment a bit with Ansible as a configuration management tool. So the steps I’ll describe were actually automated with Ansible, but I’ll leave that for another blog post. The scenario I am going to describe involves 2 haproxy instances, each pointing to 2 Wordpress servers running Apache, PHP and MySQL, with Varnish fronting the Wordpress application. One of the 2 Wordpress servers is considered primary as far as haproxy is concerned, and the other one is a backup server, which will only get requests if the primary server is down. All servers are running Ubuntu 12.04. Install and run the consul agent on all nodes The agent will start in server mode on the 2 haproxy nodes, and in agent mode on the 2 Wordpress nodes. I first deployed consul to the 2 haproxy nodes. I used a modified version of the ansible-consul role from jivesoftware. The configuration file /etc/consul.cfg for the first server (lb1) is: { “domain”: “consul.”, “data_dir”: “/opt/consul/data”, “log_level”: “INFO”, “node_name”: “lb1”, “server”: true, “bind_addr”: “10.0.0.1”, “datacenter”: “us-west-1b”, “bootstrap”: true, “rejoin_after_leave”: true} (and similar for lb2, with only node_name and bind_addr changed to lb2 and 10.0.0.2 respectively) The ansible-consul role also creates a consul user and group, and an upstart configuration file like this:1# cat /etc/init/consul.conf 1234567891011121314# Consul Agent (Upstart unit)description \"Consul Agent\"start on (local-filesystems and net-device-up IFACE!=lo)stop on runlevel [06]exec sudo -u consul -g consul /opt/consul/bin/consul agent -config-dir /etc/consul.d -config-file=/etc/consul.conf &gt;&gt; /var/log/consul 2&gt;&amp;1respawnrespawn limit 10 10kill timeout 10To start/stop consul, I use:# start consul# stop consul Note that “server” is set to true and “bootstrap” is also set to true, which means that each consul server will be the leader of a cluster with 1 member, itself. To join the 2 servers into a consul cluster, I did the following:join lb1 to lb2: on lb1 run consul join 10.0.0.2tail /var/log/consul on lb1, note messages complaining about both consul servers (lb1 and lb2) running in bootstrap modestop consul on lb1: stop consuledit /etc/consul.conf on lb1 and set “bootstrap”: falsestart consul on lb1: start consultail /var/log/consul on both lb1 and lb2; it should show no more errorsrun consul info on both lb1 and lb2; the output should show server=true on both nodes, but leader=true only on lb2Next I ran the consul agent in regular non-server mode on the 2 Wordpress nodes. The configuration file /etc/consul.cfg on node wordpress1 was: { “domain”: “consul.”, “data_dir”: “/opt/consul/data”, “log_level”: “INFO”, “node_name”: “wordpress1”, “server”: false, “bind_addr”: “10.0.1.1”, “datacenter”: “us-west-1b”, “rejoin_after_leave”: true} (and similar for wordpress2, with the node_name set to wordpress2 and bind_addr set to 10.0.1.2) After starting up the agents via upstart, I joined them to lb2 (although the could be joined to any of the existing members of the cluster). I ran this on both wordpress1 and wordpress2:123456789# consul join 10.0.0.2At this point, running consul members on any of the 4 nodes should show all 4 members of the cluster:Node Address Status Type Build Protocollb1 10.0.0.1:8301 alive server 0.4.0 2wordpress2 10.0.1.2:8301 alive client 0.4.0 2lb2 10.0.0.2:8301 alive server 0.4.0 2wordpress1 10.0.1.1:8301 alive client 0.4.0 2 Install and run dnsmasq on all nodes The ansible-consul role does this for you. Consul piggybacks on DNS resolution for service naming, and by default the domain names internal to Consul start with consul. In my case they are configured in consul.cfg via “domain”: “consul.” The dnsmasq configuration file for consul is:123# cat /etc/dnsmasq.d/10-consulserver=/consul./127.0.0.1#8600 This causes dnsmasq to provide DNS resolution for domain names starting with consul. by querying a DNS server on 127.0.0.1 running on port 8600 (which is the port the local consul agent listens on to provide DNS resolution). To start/stop dnsmasq, use: service dnsmasq start | stop. Now that dnsmasq is running, you can look up names that end in .node.consul from any member node of the consul cluster (there are 4 member nodes in my cluster, 2 servers and 2 agents). For example, I ran this on lb2:123456789101112131415161718$ dig wordpress1.node.consul; &lt;&lt;&gt;&gt; DiG 9.8.1-P1 &lt;&lt;&gt;&gt; wordpress1.node.consul;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 2511;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0;; QUESTION SECTION:;wordpress1.node.consul. IN A;; ANSWER SECTION:wordpress1.node.consul. 0 IN A 10.0.1.1;; Query time: 1 msec;; SERVER: 127.0.0.1#53(127.0.0.1);; WHEN: Fri Nov 14 00:09:16 2014;; MSG SIZE rcvd: 76 Configure services and checks on consul agent nodes Internal DNS resolution within the .consul domain becomes even more useful when nodes define services and checks. For example, the 2 Wordpress nodes run varnish and apache (on port 80 and port 443) so we can define 3 services as JSON files in /etc/consul.d. On wordpress1, which is our active/primary node in haproxy, I defined these services:1234567891011121314151617181920212223242526272829303132333435363738394041424344$ cat http_service.json&#123; \"service\": &#123; \"name\": \"http\", \"tags\": [\"primary\"], \"port\":80, \"check\": &#123; \"id\": \"http_check\", \"name\": \"HTTP Health Check\", \"script\": \"curl -H 'Host=www.mydomain.com' http://localhost\", \"interval\": \"5s\" &#125; &#125;&#125;$ cat ssl_service.json&#123; \"service\": &#123; \"name\": \"ssl\", \"tags\": [\"primary\"], \"port\":443, \"check\": &#123; \"id\": \"ssl_check\", \"name\": \"SSL Health Check\", \"script\": \"curl -k -H 'Host=www.mydomain.com' https://localhost:443\", \"interval\": \"5s\" &#125; &#125;&#125;$ cat varnish_service.json&#123; \"service\": &#123; \"name\": \"varnish\", \"tags\": [\"primary\"], \"port\":6081 , \"check\": &#123; \"id\": \"varnish_check\", \"name\": \"Varnish Health Check\", \"script\": \"curl http://localhost:6081\", \"interval\": \"5s\" &#125; &#125;&#125; Each service we defined has a name, a port and a check with its own ID, name, script that runs whenever the check is executed, and an interval that specifies how often the check is run. In the examples above I specified simple curl commands against the ports that these services are running on. Note also that each service has a list of tags associated with it. In my case, the services on wordpress1 have the tag “primary”. The services defined on wordpress2 are identical to the ones on wordpress1 with the only difference being the tag, which on wordpress2 is “backup”. After restarting consul on wordpress1 and wordpress2, the following service-related DNS names are available for resolution on all nodes in the consul cluster (I am going to include only relevant portions of the dig output):12345$ dig varnish.service.consul;; ANSWER SECTION:varnish.service.consul. 0 IN A 10.0.1.1varnish.service.consul. 0 IN A 10.0.1.2 This name resolves in DNS round-robin fashion to the IP addresses of all nodes that are running the varnish service, regardless of their tags and regardless of the data centers that their nodes run in. In our case, it resolves to the IP addresses of wordpress1 and wordpress2. Note that the IP address of a given node only appears in the DNS result set if the service running on that node has a healty check. If the check fails, then consul’s DNS service will not include the IP of the node in the result set. This is very important for the dynamic discovery of healthy services.12345$ dig varnish.service.us-west-1b.consul;; ANSWER SECTION:varnish.service.us-west-1b.consul. 0 IN A 10.0.1.2varnish.service.us-west-1b.consul. 0 IN A 10.0.1.1 If we include the data center (in our case us-west-1b) in the DNS name we query, then only the services running on nodes in that data center will be returned in the result set. In our case though, all nodes run in the us-west-1b data center, so this query returns, like the previous one, the IP addresses of wordpress1 and wordpress2. Note that the IPs can be returned in any order, because of DNS round-robin. In this case the IP of wordpress2 was first.123456789$ dig SRV varnish.service.consul;; ANSWER SECTION:varnish.service.consul. 0 IN SRV 1 1 6081 wordpress1.node.us-west-1b.consul.varnish.service.consul. 0 IN SRV 1 1 6081 wordpress2.node.us-west-1b.consul.;; ADDITIONAL SECTION:wordpress1.node.us-west-1b.consul. 0 IN A 10.0.1.1wordpress2.node.us-west-1b.consul. 0 IN A 10.0.1.2 A useful feature of the consul DNS service is that it returns the port number that a given service runs on when queried for an SRV record. So this query returns the names and IPs of the nodes that the varnish service runs on, as well as the port number, which in this case is 6081. The application querying for the SRV record needs to interpret this extra piece of information, but this is very useful for the discovery of internal services that might run on non-standard port numbers.123456789$ dig primary.varnish.service.consul;; ANSWER SECTION:primary.varnish.service.consul. 0 IN A 10.0.1.1$ dig backup.varnish.service.consul;; ANSWER SECTION:backup.varnish.service.consul. 0 IN A 10.0.1.2 The 2 DNS queries above show that it’s possible to query a service by its tag, in our case ‘primary’ vs. ‘backup’. The result set will contain the IP addresses of the nodes tagged with the specific tag and running the specific service we asked for. This feature will prove useful when dealing with consul-template in haproxy, as I’ll show later in this post. Load balance across services It’s easy now to see how an application can take advantage of the internal DNS service provided by consul and load balance across services. For example, an application that needs to load balance across the 2 varnish services on wordpress1 and wordpress2 would use varnish.service.consul as the DNS name it talks to when it needs to hit varnish. Every time this DNS name is resolved, a random node from wordpress1 and wordpress2 is returned via the DNS round-robin mechanism. If varnish were to run on a non-standard port number, the application would need to issue a DNS request for the SRV record in order to obtain the port number as well as the IP address to hit. Note that this method of load balancing has health checks built in. If the varnish health check fails on one of the nodes providing the varnish service, that node’s IP address will not be included in the DNS result set returned by the DNS query for that service. Also note that the DNS query can be customized for the needs of the application, which can query for a specific data center, or a specific tag, as I showed in the examples above. Force a node out of service I am still looking for the best way to take nodes in and out of service for maintenance or other purposes. One way I found so far is to deregister a given service via the Consul HTTP API. Here is an example of a curl command that accomplishes that, executed on node wordpress1:123456789101112131415$ curl -v http://localhost:8500/v1/agent/service/deregister/varnish* About to connect() to localhost port 8500 (#0)* Trying 127.0.0.1... connected&gt; GET /v1/agent/service/deregister/varnish HTTP/1.1&gt; User-Agent: curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3&gt; Host: localhost:8500&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Date: Mon, 17 Nov 2014 19:01:06 GMT&lt; Content-Length: 0&lt; Content-Type: text/plain; charset=utf-8&lt;* Connection #0 to host localhost left intact* Closing connection #0 The effect of this command is that the varnish service on node wordpress1 is ‘deregistered’, which for my purposes means ‘marked as down’. DNS queries for varnish.service.consul will only return the IP address of wordpress2:1234$ dig varnish.service.consul;; ANSWER SECTION:varnish.service.consul. 0 IN A 10.0.1.2 We can also use the Consul HTTP API to verify that the varnish service does not appear in the list of active services on node wordpress1. We’ll use the /agent/services API call and we’ll save the output to a file called services.out, then we’ll use the jq tool to pretty-print the output:123456789101112131415161718192021$ curl -v http://localhost:8500/v1/agent/services -o services.out$ jq . &lt;&lt;&lt; `cat services.out`&#123; \"http\": &#123; \"ID\": \"http\", \"Service\": \"http\", \"Tags\": [ \"primary\" ], \"Port\": 80 &#125;, \"ssl\": &#123; \"ID\": \"ssl\", \"Service\": \"ssl\", \"Tags\": [ \"primary\" ], \"Port\": 443 &#125;&#125; Note that only the http and ssl services are shown. Force a node back in service Again, I am still looking for the best way to mark as service as ‘up’ once it was marked as ‘down’. One way would be to register the service via the Consul HTTP API, and that requires issuing a POST request with the payload being the JSON configuration file for that service. Another way is to just restart the consul agent on the node in question. This will register the service that had been deregistered previously. Install and configure consul-template For the next few steps, I am going to show how to use consul-template in conjuction with consul for discovering services and configuring haproxy based on the discovered services. I automated the installation and configuration of consul-template via an Ansible role that I put on Github, but I am going to discuss the main steps here. See also the instructions on the consul-template Github page. In my Ansible role, I copy the consul-template binary to the target node (in my case the 2 haproxy nodes lb1 and lb2), then create a directory structure /opt/consul-template/{bin,config,templates}. The consul-template configuration file is /opt/consul-template/config/consul-template.cfg and it looks like this in my case:12345678$ cat config/consul-template.cfgconsul = \"127.0.0.1:8500\"template &#123; source = \"/opt/consul-template/templates/haproxy.ctmpl\" destination = \"/etc/haproxy/haproxy.cfg\" command = \"service haproxy restart\"&#125; Note that consul-template needs to be able to talk a consul agent, which in my case is the local agent listening on port 8500. The template that consul-template maintains is defined in another file, /opt/consul-template/templates/haproxy.ctmpl. What consul-template does is monitor changes to that file via changes to the services referenced in the file. Upon any such change, consul-template will generate a new target file based on the template and copy it to the destination file, which in my case is the haproxy config file /etc/haproxy/haproxy.cfg. Finally, consul-template will executed a command, which in my case is the restarting of the haproxy service. Here is the actual template file for my haproxy config, which is written in the Go template format:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253$ cat /opt/consul-template/templates/haproxy.ctmplglobal log 127.0.0.1 local0 maxconn 4096 user haproxy group haproxydefaults log global mode http option dontlognull retries 3 option redispatch timeout connect 5s timeout client 50s timeout server 50s balance roundrobin# Set up application listeners here.frontend http maxconn &#123;&#123;key \"service/haproxy/maxconn\"&#125;&#125; bind 0.0.0.0:80 default_backend servers-http-varnishbackend servers-http-varnish balance roundrobin option httpchk GET / option httplog&#123;&#123;range service \"primary.varnish\"&#125;&#125; server &#123;&#123;.Node&#125;&#125; &#123;&#123;.Address&#125;&#125;:&#123;&#123;.Port&#125;&#125; weight 1 check port &#123;&#123;.Port&#125;&#125;&#123;&#123;end&#125;&#125;&#123;&#123;range service \"backup.varnish\"&#125;&#125; server &#123;&#123;.Node&#125;&#125; &#123;&#123;.Address&#125;&#125;:&#123;&#123;.Port&#125;&#125; backup weight 1 check port &#123;&#123;.Port&#125;&#125;&#123;&#123;end&#125;&#125;frontend https maxconn &#123;&#123;key \"service/haproxy/maxconn\"&#125;&#125; mode tcp bind 0.0.0.0:443 default_backend servers-httpsbackend servers-https mode tcp option tcplog balance roundrobin&#123;&#123;range service \"primary.ssl\"&#125;&#125; server &#123;&#123;.Node&#125;&#125; &#123;&#123;.Address&#125;&#125;:&#123;&#123;.Port&#125;&#125; weight 1 check port &#123;&#123;.Port&#125;&#125;&#123;&#123;end&#125;&#125;&#123;&#123;range service \"backup.ssl\"&#125;&#125; server &#123;&#123;.Node&#125;&#125; &#123;&#123;.Address&#125;&#125;:&#123;&#123;.Port&#125;&#125; backup weight 1 check port &#123;&#123;.Port&#125;&#125;&#123;&#123;end&#125;&#125; To the trained eye, this looks like a regular haproxy configuration file, with the exception of the portions bolded above. These are Go template snippets which rely on a couple of template functions exposed by consul-template above and beyond what the Go templating language offers. Specifically, the key function queries a key stored in the Consul key/value store and outputs the value associated with that key (or an empty string if the value doesn’t exist). The service function queries a consul service by its DNS name and returns a result set used inside the range statement. The variables inside the result set can be inspected for properties such as Node, Address and Port, which correspond to the Consul service node name, IP address and port number for that particular service. In my example above, I use the value of the key service/haproxy/maxconn as the value of maxconn. In the http-varnish backend, I used 2 sets of services names, primary.varnish and backup.varnish, because I wanted to differentiate in haproxy.cfg between the primary server (wordpress1 in my case) and the backup server (wordpress2). In the ssl backend, I did the same but with the ssl service. Everything so far would work fine with the exception of the key/value pair represented by the key service/haproxy/maxconn. To define that pair, I used the Consul key/value store API (this can be run on any member of the Consul cluster):123456789101112131415161718192021222324252627282930313233$ cat set_haproxy_maxconn.sh#!/bin/bashMAXCONN=4000curl -X PUT -d \"$MAXCONN\" http://localhost:8500/v1/kv/service/haproxy/maxconnTo verify that the value was set, I used:$ cat query_consul_kv.sh#!/bin/bashcurl -v http://localhost:8500/v1/kv/?recurse$ ./query_consul_kv.sh* About to connect() to localhost port 8500 (#0)* Trying 127.0.0.1... connected&gt; GET /v1/kv/?recurse HTTP/1.1&gt; User-Agent: curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3&gt; Host: localhost:8500&gt; Accept: */*&gt;&lt; HTTP/1.1 200 OK&lt; Content-Type: application/json&lt; X-Consul-Index: 30563&lt; X-Consul-Knownleader: true&lt; X-Consul-Lastcontact: 0&lt; Date: Mon, 17 Nov 2014 23:01:07 GMT&lt; Content-Length: 118&lt;* Connection #0 to host localhost left intact* Closing connection #0[&#123;\"CreateIndex\":10995,\"ModifyIndex\":30563,\"LockIndex\":0,\"Key\":\"service/haproxy/maxconn\",\"Flags\":0,\"Value\":\"NDAwMA==\"&#125;] At this point, everything is ready for starting up the consul-template service (in Ubuntu), I did it via this Upstart configuration file:12345678910111213# cat /etc/init/consul-template.conf# Consul Template (Upstart unit)description \"Consul Template\"start on (local-filesystems and net-device-up IFACE!=lo)stop on runlevel [06]exec /opt/consul-template/bin/consul-template -config=/opt/consul-template/config/consul-template.cfg &gt;&gt; /var/log/consul-template 2&gt;&amp;1respawnrespawn limit 10 10kill timeout 10# start consul-template Once consul-template starts, it will peform the actions corresponding to the functions defined in the template file /opt/consul-template/templates/haproxy.ctmpl. In my case, it will query Consul for the value of the key service/haproxy/maxconn and for information about the 2 Consul services varnish.service and ssl.service. It will then save the generated file to /etc/haproxy/haproxy.cfg and it will restart the haproxy service. The relevant snippets from haproxy.cfg are:1234567891011121314151617181920212223242526272829303132frontend http maxconn 4000 bind 0.0.0.0:80 default_backend servers-httpbackend servers-http balance roundrobin option httpchk GET / option httplog server wordpress1 10.0.1.1:6081 weight 1 check port 6081 server wordpress2 10.0.1.2:6081 backup weight 1 check port 6081andfrontend https maxconn 4000 mode tcp bind 0.0.0.0:443 default_backend servers-httpsbackend servers-https mode tcp option tcplog balance roundrobin server wordpress1 10.0.1.1:443 weight 1 check port 443 server wordpress2 10.0.1.2:443 backup weight 1 check port 443 I’ve been running this as a test on lb2. I don’t consider my setup quite production-ready because I don’t have monitoring in place, and I also want to experiment with consul security tokens for better security. But this is a pattern that I think will work.","categories":[{"name":"distribution ","slug":"distribution","permalink":"https://blog.djstudy.net/categories/distribution/"}],"tags":[{"name":"consul","slug":"consul","permalink":"https://blog.djstudy.net/tags/consul/"}]},{"title":"Presto, Parquet & Airpal","slug":"presto-parquet-airpal","date":"2016-02-04T10:53:35.000Z","updated":"2016-02-04T11:43:10.000Z","comments":true,"path":"2016/02/04/presto-parquet-airpal/","link":"","permalink":"https://blog.djstudy.net/2016/02/04/presto-parquet-airpal/","excerpt":"","text":"I came across a blog post from Brandon Harris recently where he discussed a credit card fraud detection project he’d been working on with a team at the University of Chicago. In the post he described how Presto and Parquet-formatted files had gone a long way to speeding up ad-hoc queries against a ~250GB dataset he’s working with. Presto was born at Facebook and was open sourced within a year of it’s inception. It’s a distributed query engine capable of running interactive queries against big data sources. There’s support for data sources such as Hive, Kafka, PostgreSQL, Redis and Cassandra among many others. Netflix has blogged about their positive experiences with Presto on a 10PB Data Warehouse they’ve got that’s happily handling 2,500 ad-hoc queries a day. In Brandon’s blog post there is a chart showing a query that’s executed in Hive against data stored in CSV format taking 130 seconds and then the same query run via Presto against data stored in Parquet format taking less than 5 seconds. I trust the measurements of his queries are accurate but what I’m interested in is what is involved in getting an environment up to run these sorts of queries. As of this writing Bigtop’s Presto support isn’t ready (though pull requests are being worked on) so to get an environment up and running locally I’ll have to perform some of the installation steps manually. Launching a Hadoop Cluster in Docker ContainersThis process begins with a fresh Ubuntu 15 installation acting as the host for Docker containers that a Hadoop cluster will live within. I discuss getting Ubuntu 15 ready to run Docker in my Hadoop Up and Running blog post. With Docker ready I’ll checkout the Bigtop git repository and launch Ubuntu 14.04-based containers (as of this writing this is the latest supported version of Ubuntu on Intel-based systems).1234$ git clone https://github.com/apache/bigtop.git$ cd bigtop/bigtop-deploy/vm/vagrant-puppet-docker/$ sudo docker pull bigtop/deploy:ubuntu-14.04$ vi vagrantconfig.yaml 1234567891011121314151617docker: memory_size: \"4096\" image: \"bigtop/deploy:ubuntu-14.04\"boot2docker: memory_size: \"4096\" number_cpus: \"1\"repo: \"http://bigtop-repos.s3.amazonaws.com/releases/1.0.0/ubuntu/trusty/x86_64\"distro: debiancomponents: [hadoop, yarn, hive]namenode_ui_port: \"50070\"yarn_ui_port: \"8088\"hbase_ui_port: \"60010\"enable_local_repo: falsesmoke_test_components: [mapreduce, pig]jdk: \"openjdk-7-jdk\" While I was writing this blog post Bigtop 1.1 was being cut and the resources from their 1.1.0 endpoint were returning HTTP 403 messages so I’ve stuck with the 1.0.0 endpoints for now.12$ sudo ./docker-hadoop.sh --create 3$ sudo vagrant ssh bigtop1 Getting Hive’s Metastore Up and RunningBy default the Derby embedded database driver is enabled in the boilerplate Hive configurations provided by Bigtop. This driver can only allow one process at a time to connect to it. If you use it Hive’s Metastore server won’t communicate properly with Presto and you’ll get “does not exist” messages every time you try to access a table.12SELECT * FROM track_metadata_csv;... Table hive.default.track_metadata_csv does not exist For this reason I’ve installed MySQL and used it as the data backend for Hive’s Metastore.1234567$ apt-get install \\ libmysql-java \\ mysql-server$ ln -s /usr/share/java/mysql-connector-java.jar \\ /usr/lib/hive/lib/mysql-connector-java.jar$ /etc/init.d/mysql start$ mysql -uroot -proot -e'CREATE DATABASE hcatalog;' When you install MySQL you’ll be prompted to set a root login and password. I’ve set both values to root in this example. Below is the Hive site configuration file. 1$ vi /etc/hive/conf/hive-site.xml 123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version=\"1.0\"?&gt;&lt;?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;bigtop1.docker&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.execution.engine&lt;/name&gt; &lt;value&gt;mr&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost/hcatalog?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.hwi.war.file&lt;/name&gt; &lt;value&gt;/usr/lib/hive/lib/hive-hwi.war&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; I then launched Hive and made sure the Metastore’s database exists.1$ hive 1CREATE DATABASE IF NOT EXISTS metastore_db; After that I closed out of Hive and launched it’s Metastore service in the background. You’ll see it binded to port 9083 if it’s running.12$ hive --service metastore &amp;$ netstat -an | grep 9083 Getting some data to play withI need some data to play with in this exercise so I dumped the Million Song Dataset to CSV and imported it into HDFS 12$ apt-get install sqlite3$ wget -c http://labrosa.ee.columbia.edu/millionsong/sites/default/files/AdditionalFiles/track_metadata.db 1234567891011$ sqlite3 track_metadata.db &lt;&lt;!.headers off.mode csv.output track_metadata.csvSELECT track_id, artist_id, artist_familiarity, artist_hotttnesss, duration, yearFROM songs; 123$ hadoop fs -put \\ track_metadata.csv \\ /tmp/track_metadata.csv With the CSV file sitting in HDFS I’ll create a Hive table for it. Once that table is created I can create a second, Parquet-formatted table and import the data from the first table into the second.1$ hive 12345678910111213141516171819202122232425CREATE TABLE track_metadata_csv ( track_id VARCHAR(18), artist_id VARCHAR(18), artist_familiarity DECIMAL(16, 15), artist_hotttnesss DECIMAL(16, 15), duration DECIMAL(12, 8), year SMALLINT)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';LOAD DATA INPATH '/tmp/track_metadata.csv'INTO TABLE track_metadata_csv;CREATE TABLE track_metadata_parquet ( track_id VARCHAR(18), artist_id VARCHAR(18), artist_familiarity DECIMAL(16, 15), artist_hotttnesss DECIMAL(16, 15), duration DECIMAL(12, 8), year SMALLINT)STORED AS parquet;INSERT INTO TABLE track_metadata_parquetSELECT * FROM track_metadata_csv; I now have that table’s metadata stored in Hive’s Metastore and a Parquet-formatted file of that data sitting on HDFS:1234567891011121314$ hadoop fs -ls \\ /user/hive/warehouse/track_metadata_parquetFound 1 items-rw-r--r-- 3 root hadoop 45849172 2016-01-31 13:31 /user/hive/warehouse/track_metadata_parquet/000000_0$ mysqldump \\ -uroot \\ -proot \\ --no-create-info \\ --skip-add-locks \\ --skip-disable-keys \\ --skip-comments \\ hcatalog | \\ grep ^INSERT | \\ sort 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657INSERT INTO `CDS` VALUES (1),(2);INSERT INTO `COLUMNS_V2` VALUES (1,NULL,'artist_familiarity','decimal(16,15)',2), (1,NULL,'artist_hotttnesss','decimal(16,15)',3), (1,NULL,'artist_id','varchar(18)',1), (1,NULL,'duration','decimal(12,8)',4), (1,NULL,'track_id','varchar(18)',0), (1,NULL,'year','smallint',5), (2,NULL,'artist_familiarity','decimal(16,15)',2), (2,NULL,'artist_hotttnesss','decimal(16,15)',3), (2,NULL,'artist_id','varchar(18)',1), (2,NULL,'duration','decimal(12,8)',4), (2,NULL,'track_id','varchar(18)',0), (2,NULL,'year','smallint',5);INSERT INTO `DBS` VALUES (1,'Default Hive database','hdfs://bigtop1.docker:8020/user/hive/warehouse','default','public','ROLE'), (2,NULL,'hdfs://bigtop1.docker:8020/user/hive/warehouse/metastore_db.db','metastore_db','root','USER');INSERT INTO `GLOBAL_PRIVS` VALUES (1,1454255378,1,'admin','ROLE','admin','ROLE','All');INSERT INTO `ROLES` VALUES (1,1454255378,'admin','admin'), (2,1454255378,'public','public');INSERT INTO `SDS` VALUES (1,1,'org.apache.hadoop.mapred.TextInputFormat','\\0','\\0','hdfs://bigtop1.docker:8020/user/hive/warehouse/track_metadata_csv',-1,'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',1), (2,2,'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat','\\0','\\0','hdfs://bigtop1.docker:8020/user/hive/warehouse/track_metadata_parquet',-1,'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',2);INSERT INTO `SEQUENCE_TABLE` VALUES ('org.apache.hadoop.hive.metastore.model.MColumnDescriptor',6), ('org.apache.hadoop.hive.metastore.model.MDatabase',6), ('org.apache.hadoop.hive.metastore.model.MGlobalPrivilege',6), ('org.apache.hadoop.hive.metastore.model.MRole',6), ('org.apache.hadoop.hive.metastore.model.MSerDeInfo',6), ('org.apache.hadoop.hive.metastore.model.MStorageDescriptor',6), ('org.apache.hadoop.hive.metastore.model.MTable',6), ('org.apache.hadoop.hive.metastore.model.MVersionTable',6);INSERT INTO `SERDES` VALUES (1,NULL,'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'), (2,NULL,'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe');INSERT INTO `SERDE_PARAMS` VALUES (1,'field.delim',','), (1,'serialization.format',','), (2,'serialization.format','1');INSERT INTO `TABLE_PARAMS` VALUES (1,'COLUMN_STATS_ACCURATE','true'), (1,'numFiles','2'), (1,'numRows','0'), (1,'rawDataSize','0'), (1,'totalSize','161179824'), (1,'transient_lastDdlTime','1454255466'), (2,'COLUMN_STATS_ACCURATE','true'), (2,'numFiles','2'), (2,'numRows','2000000'), (2,'rawDataSize','12000000'), (2,'totalSize','136579903'), (2,'transient_lastDdlTime','1454255498');INSERT INTO `TBLS` VALUES (1,1454255462,1,0,'root',0,1,'track_metadata_csv','MANAGED_TABLE',NULL,NULL), (2,1454255471,1,0,'root',0,2,'track_metadata_parquet','MANAGED_TABLE',NULL,NULL);INSERT INTO `VERSION` VALUES (1,'0.14.0','Set by MetaStore'); Presto Up and RunningPresto requires Java 8 so I’ll install that first.123$ add-apt-repository ppa:webupd8team/java$ apt-get update$ apt-get install oracle-java8-installer I have yet to see recent Debian packages for Presto so I’ll download the binaries instead.123$ cd ~$ wget -c https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.133/presto-server-0.133.tar.gz$ tar xzf presto-server-0.133.tar.gz Presto requires a data folder for it to store locks, logs and a few other items and also requires a number of configuration files before it can begin to work properly.123$ mkdir -p /root/datap$ mkdir -p ~/presto-server-0.133/etc/catalog$ cd ~/presto-server-0.133/etc Normally I wouldn’t suggest creating a data folder in the root partition but this work is all taking place in a Docker container that has exposed 13GB of space on the root partition.1234567$ df -HFilesystem Size Used Avail Use% Mounted onnone 13G 6.4G 5.6G 54% /tmpfs 4.2G 8.2k 4.2G 1% /devtmpfs 4.2G 0 4.2G 0% /sys/fs/cgroup/dev/sda1 13G 6.4G 5.6G 54% /vagrantshm 68M 0 68M 0% /dev/shm Next I need to create six configuration files. Below is an outline of where they live within the ~/presto-server-0.133/etc folder:123456789$ tree ~/presto-server-0.133/etcetc|-- catalog| |-- hive.properties| `-- jmx.properties|-- config.properties|-- jvm.config|-- log.properties`-- node.properties Here are the commands to set the contents on each of the configuration files.1$ vi ~/presto-server-0.133/etc/config.properties 1234567coordinator=truenode-scheduler.include-coordinator=truehttp-server.http.port=8080query.max-memory=800MBquery.max-memory-per-node=200MBdiscovery-server.enabled=truediscovery.uri=http://127.0.0.1:8080 1$ vi ~/presto-server-0.133/etc/jvm.config 12345678-server-Xmx800M-XX:+UseG1GC-XX:G1HeapRegionSize=32M-XX:+UseGCOverheadLimit-XX:+ExplicitGCInvokesConcurrent-XX:+HeapDumpOnOutOfMemoryError-XX:OnOutOfMemoryError=kill -9 %p 123456$ vi ~/presto-server-0.133/etc/log.propertiescom.facebook.presto=INFO$ vi ~/presto-server-0.133/etc/node.propertiesnode.environment=devnode.id=ffffffff-ffff-ffff-ffff-ffffffffffffnode.data-dir=/root/datap The JMX connector provides the ability to query Java Management Extensions (JMX) information from all nodes in a Presto cluster. Presto itself is heavily instrumented via JMX.12$ vi ~/presto-server-0.133/etc/catalog/jmx.propertiesconnector.name=jmx This file allows Presto to know where our Hive Metastore is and which connector to use to communicate with it.123$ vi ~/presto-server-0.133/etc/catalog/hive.propertieshive.metastore.uri=thrift://bigtop1.docker:9083connector.name=hive-hadoop2 With those in place you can launch Presto’s server.1$ ~/presto-server-0.133/bin/launcher start It should expose a web frontend on port 8080 if it’s running properly.1$ curl localhost:8080 Presto’s CLI Up and RunningI’ll download the CLI JAR file for Presto, rename it to presto and then I can use it to connect to the Presto Server.12345$ cd ~$ wget -c https://repo1.maven.org/maven2/com/facebook/presto/presto-cli/0.133/presto-cli-0.133-executable.jar$ mv presto-cli-0.133-executable.jar presto$ chmod +x presto$ ./presto --server localhost:8080 --catalog hive --schema default I’ll then see if Presto can see the schemas in the Hive Metastore. show schemas from hive; Schema default information_schema metastore_db(3 rows) Executing Queries in PrestoWith the CLI communicating with the server properly I’ll run two queries. The first will count how many records per year exist in our million song database using the data in the CSV-backed table and the second will do the same against the Parquet-backed table.1234567891011121314151617181920212223242526272829303132333435363738SELECT year, count(*)FROM track_metadata_csvGROUP BY yearORDER BY year; year | _col1------+-------- 0 | 968848 1922 | 12 1924 | 10 ... 2008 | 69540 2009 | 62102 2010 | 18794 2011 | 2(90 rows)Query 20160131_160009_00002_s4z65, FINISHED, 1 nodeSplits: 6 total, 6 done (100.00%)0:09 [2M rows, 154MB] [221K rows/s, 17MB/s]SELECT year, count(*)FROM track_metadata_parquetGROUP BY yearORDER BY year; year | _col1------+-------- 0 | 968848 1922 | 12 1924 | 10 ... 2008 | 69540 2009 | 62102 2010 | 18794 2011 | 2(90 rows)Query 20160131_160411_00005_s4z65, FINISHED, 1 nodeSplits: 5 total, 5 done (100.00%)0:04 [3M rows, 87MB] [799K rows/s, 23.2MB/s] The query finished in 9 seconds with the CSV-formatted data and in 4 seconds with the Parquet-formatted data. I grepped the server log to see the time lines recorded by the Query Monitor.12345$ grep 20160131_160009_00002_s4z65 ~/datap/var/log/server.log...elapsed 3886.00ms :: planning 644.26ms :: scheduling 1099.00ms :: running 2094.00ms :: finishing 49.00ms...$ grep 20160131_160411_00005_s4z65 ~/datap/var/log/server.log...elapsed 1406.00ms :: planning 91.68ms :: scheduling 55.00ms :: running 1221.00ms :: finishing 38.00ms... I’m not surprised that the columnar-based, Parquet-format-backed query was twice as fast but I’m not sure how the timings the Query Monitor recorded relate to the times reported by the CLI. It gives me something to dig into at a later point. I took a look at the query plans to see if they offer anything of interest. In the case of these two queries they were identical bar the table names.1234567891011121314151617181920212223242526272829303132333435363738EXPLAINSELECT year, count(*)FROM track_metadata_csvGROUP BY yearORDER BY year; ---------------------------------------------------------------------------------------------------------------------------------------------- - Output[year, _col1] =&gt; [year:bigint, count:bigint] _col1 := count - Sort[year ASC_NULLS_LAST] =&gt; [year:bigint, count:bigint] - Exchange[GATHER] =&gt; year:bigint, count:bigint - Aggregate(FINAL)[year] =&gt; [year:bigint, count:bigint] count := \"count\"(\"count_9\") - Exchange[REPARTITION] =&gt; year:bigint, count_9:bigint - Aggregate(PARTIAL)[year] =&gt; [year:bigint, count_9:bigint] count_9 := \"count\"(*) - TableScan[hive:hive:default:track_metadata_csv, originalConstraint = true] =&gt; [year:bigint] LAYOUT: hive year := HiveColumnHandle&#123;clientId=hive, name=year, hiveType=smallint, hiveColumnIndex=5, partitionKey=false&#125;EXPLAINSELECT year, count(*)FROM track_metadata_parquetGROUP BY yearORDER BY year; Query Plan---------------------------------------------------------------------------------------------------------------------------------------------- - Output[year, _col1] =&gt; [year:bigint, count:bigint] _col1 := count - Sort[year ASC_NULLS_LAST] =&gt; [year:bigint, count:bigint] - Exchange[GATHER] =&gt; year:bigint, count:bigint - Aggregate(FINAL)[year] =&gt; [year:bigint, count:bigint] count := \"count\"(\"count_9\") - Exchange[REPARTITION] =&gt; year:bigint, count_9:bigint - Aggregate(PARTIAL)[year] =&gt; [year:bigint, count_9:bigint] count_9 := \"count\"(*) - TableScan[hive:hive:default:track_metadata_parquet, originalConstraint = true] =&gt; [year:bigint] LAYOUT: hive year := HiveColumnHandle&#123;clientId=hive, name=year, hiveType=smallint, hiveColumnIndex=5, partitionKey=false&#125; Airpal: A Web Interface for PrestoIn March 2015 AirBNB announced Airpal, a web-based query tool that works with Presto. Beyond a visual interface to run queries it offers access controls, metadata exploration, query progress tracking and CSV exporting of results. These are the steps I took to install this software and launch it within the same Docker container as Presto. The first thing I had to do was to get a copy of Airpal’s git repository and build the Airpal JAR file.12345678$ apt-get install \\ build-essential \\ git \\ gradle \\ mysql-server$ git clone https://github.com/airbnb/airpal.git$ cd airpal$ ./gradlew clean shadowJar I then created a MySQL database for Airpal to store it’s configuration and logs.1234$ mysql \\ -uroot \\ -proot \\ -e'CREATE DATABASE `airpal` CHARACTER SET utf8 COLLATE utf8_general_ci;' Airpal needs a configuration file to tell it’s JAR file how to, among other things, connect with the MySQL data backend.1$ vi reference.yml 12345678910111213141516171819202122232425262728293031# Logging settingslogging: loggers: org.apache.shiro: INFO # The default level of all loggers. Can be OFF, ERROR, WARN, INFO, DEBUG, TRACE, or ALL. level: INFO# HTTP-specific options.server: applicationConnectors: - type: http port: 8081 idleTimeout: 10 seconds adminConnectors: - type: http port: 8082shiro: iniConfigs: [\"classpath:shiro_allow_all.ini\"]dataSourceFactory: driverClass: com.mysql.jdbc.Driver user: root password: root url: jdbc:mysql://127.0.0.1:3306/airpal# The URL to the Presto coordinator.prestoCoordinator: http://127.0.0.1:8080 With that in place I setup Airpal’s database schema and launched the server in the background.123456789$ java -Duser.timezone=UTC \\ -cp build/libs/airpal-*-all.jar \\ com.airbnb.airpal.AirpalApplication \\ db migrate reference.yml$ java -server \\ -Duser.timezone=UTC \\ -cp build/libs/airpal-*-all.jar \\ com.airbnb.airpal.AirpalApplication \\ server reference.yml &amp; Their is a health check endpoint on port 8082.12345678910111213$ curl -s http://127.0.0.1:8082/healthcheck | \\ python -m json.tool&#123; \"deadlocks\": &#123; \"healthy\": true &#125;, \"mysql\": &#123; \"healthy\": true &#125;, \"presto\": &#123; \"healthy\": true &#125;&#125; If that all looks well the web interface is configured to be exposed on port 8081.1$ curl http://127.0.0.1:8081/app Thank you for taking the time to read this post. If you’re considering using Digital Ocean, the hosting provider this blog is hosted on, please consider using this link to sign up.转 http://tech.marksblogg.com/presto-parquet-airpal.html","categories":[{"name":"big data","slug":"big-data","permalink":"https://blog.djstudy.net/categories/big-data/"}],"tags":[{"name":"presto","slug":"presto","permalink":"https://blog.djstudy.net/tags/presto/"},{"name":"parquet","slug":"parquet","permalink":"https://blog.djstudy.net/tags/parquet/"}]},{"title":"Spark Sort Based Shuffle内存分析","slug":"spark-sort-shuffle-analyse","date":"2016-01-27T09:51:18.000Z","updated":"2016-01-27T12:00:22.000Z","comments":true,"path":"2016/01/27/spark-sort-shuffle-analyse/","link":"","permalink":"https://blog.djstudy.net/2016/01/27/spark-sort-shuffle-analyse/","excerpt":"","text":"前言借用和董神的一段对话说下背景： shuffle共有三种，别人讨论的是hash shuffle，这是最原始的实现，曾经有两个版本，第一版是每个map产生r个文件，一共产生mr个文件，由于产生的中间文件太大影响扩展性，社区提出了第二个优化版本，让一个core上map共用文件，减少文件数目，这样共产生corer个文件，好多了，但中间文件数目仍随任务数线性增加，仍难以应对大作业，但hash shuffle已经优化到头了。为了解决hash shuffle性能差的问题，又引入sort shuffle，完全借鉴mapreduce实现，每个map产生一个文件，彻底解决了扩展性问题目前Sort Based Shuffle 是作为默认Shuffle类型的。Shuffle 是一个很复杂的过程，任何一个环节都足够写一篇文章。所以这里，我尝试换个方式，从实用的角度出发，让读者有两方面的收获： 剖析哪些环节，哪些代码可能会让内存产生问题控制相关内存的参数有时候，我们宁可程序慢点，也不要OOM，至少要先跑步起来，希望这篇文章能够让你达成这个目标。 同时我们会提及一些类名，这些类方便你自己想更深入了解时，可以方便的找到他们，自己去探个究竟。 Shuffle 概览Spark 的Shuffle 分为 Write,Read 两阶段。我们预先建立三个概念： Write 对应的是ShuffleMapTask,具体的写操作ExternalSorter来负责 Read 阶段由ShuffleRDD里的HashShuffleReader来完成。如果拉来的数据如果过大，需要落地，则也由ExternalSorter来完成的 所有Write 写完后，才会执行Read。 他们被分成了两个不同的Stage阶段。 也就是说，Shuffle Write ,Shuffle Read 两阶段都可能需要落磁盘，并且通过Disk Merge 来完成最后的Sort归并排序。 Shuffle Write 内存消耗分析Shuffle Write 的入口链路为： org.apache.spark.scheduler.ShuffleMapTask —&gt; org.apache.spark.shuffle.sort.SortShuffleWriter —&gt; org.apache.spark.util.collection.ExternalSorter 会产生内存瓶颈的其实就是 org.apache.spark.util.collection.ExternalSorter。我们看看这个复杂的ExternalSorter都有哪些地方在占用内存： 第一个地： 1private var map = new PartitionedAppendOnlyMap[K, C] 我们知道，数据都是先写内存，内存不够了，才写磁盘。这里的map就是那个放数据的内存了。 这个PartitionedAppendOnlyMap内部维持了一个数组，是这样的：1private var data = new Array[AnyRef](2 * capacity) 也就是他消耗的并不是Storage的内存，所谓Storage内存，指的是由blockManager管理起来的内存。 PartitionedAppendOnlyMap 放不下，要落地，那么不能硬生生的写磁盘，所以需要个buffer,然后把buffer再一次性写入磁盘文件。这个buffer是由参数 spark.shuffle.file.buffer=32k控制的。数据获取的过程中，序列化反序列化，也是需要空间的，所以Spark 对数量做了限制，通过如下参数控制： spark.shuffle.spill.batchSize=10000假设一个Executor的可使用的Core为 C个，那么对应需要的内存消耗为： C 32k + C 10000个Record + C PartitionedAppendOnlyMap这么看来，写文件的buffer不是问题，而序列化的batchSize也不是问题，几万或者十几万个Record 而已。那C PartitionedAppendOnlyMap 到底会有多大呢？我先给个结论: C PartitionedAppendOnlyMap &lt; ExecutorHeapMemeory 0.2 * 0.8怎么得到上面的结论呢？核心店就是要判定PartitionedAppendOnlyMap 需要占用多少内存，而它到底能占用内存，则由触发写磁盘动作决定，因为一旦写磁盘，PartitionedAppendOnlyMap所占有的内存就会被释放。下面是判断是否写磁盘的逻辑代码： estimatedSize = map.estimateSize()if (maybeSpill(map, estimatedSize)) { map = new PartitionedAppendOnlyMap[K, C]} 每放一条记录，就会做一次内存的检查，看PartitionedAppendOnlyMap 到底占用了多少内存。如果真是这样，假设检查一次内存1ms, 1kw 就不得了的时间了。所以肯定是不行的，所以 estimateSize其实是使用采样算法来做的。 第二个，我们也不希望mayBeSpill太耗时,所以 maybeSpill 方法里就搞了很多东西，减少耗时。我们看看都设置了哪些防线 首先会判定要不要执行内部逻辑： elementsRead % 32 == 0 &amp;&amp; currentMemory &gt;= myMemoryThreshold每隔32次会进行一次检查，并且要当前PartitionedAppendOnlyMap currentMemory &gt; myMemoryThreshold 才会进一步判定是不是要spill. 其中 myMemoryThreshold可通过如下配置获得初始值 spark.shuffle.spill.initialMemoryThreshold = 5 1024 1024接着会向 shuffleMemoryManager 要 2 * currentMemory - myMemoryThreshold 的内存，shuffleMemoryManager 是被Executor 所有正在运行的Task(Core) 共享的，能够分配出去的内存是： ExecutorHeapMemeory 0.2 0.8上面的数字可通过下面两个配置来更改： spark.shuffle.memoryFraction=0.2spark.shuffle.safetyFraction=0.8如果无法获取到足够的内存，就会触发真的spill操作了。 看到这里，上面的结论就显而易见了。 然而，这里我们忽略了一个很大的问题，就是 estimatedSize = map.estimateSize()为什么说它是大问题，前面我们说了，estimateSize 是近似估计，所以有可能估的不准，也就是实际内存会远远超过预期。 具体的大家可以看看 org.apache.spark.util.collection.SizeTracker 我这里给出一个结论： 如果你内存开的比较大，其实反倒风险更高，因为estimateSize 并不是每次都去真实的算缓存。它是通过采样来完成的，而采样的周期不是固定的，而是指数增长的，比如第一次采样完后，PartitionedAppendOnlyMap 要经过1.1次的update/insert操作之后才进行第二次采样，然后经过1.1*.1.1次之后进行第三次采样，以此递推，假设你内存开的大，那PartitionedAppendOnlyMap可能要经过几十万次更新之后之后才会进行一次采样，然后才能计算出新的大小，这个时候几十万次更新带来的新的内存压力，可能已经让你的GC不堪重负了。 当然，这是一种折中，因为确实不能频繁采样。 如果你不想出现这种问题，要么自己替换实现这个类，要么将 spark.shuffle.safetyFraction=0.8设置的更小一些。 Shuffle Read 内存消耗分析Shuffle Read 的入口链路为： org.apache.spark.rdd.ShuffledRDD—&gt; org.apache.spark.shuffle.sort.HashShuffleReader —&gt; org.apache.spark.util.collection.ExternalAppendOnlyMap —&gt; org.apache.spark.util.collection.ExternalSorterShuffle Read 会更复杂些，尤其是从各个节点拉取数据。但这块不是不是我们的重点。按流程，主要有： 获取待拉取数据的迭代器使用AppendOnlyMap/ExternalAppendOnlyMap 做combine如果需要对key排序，则使用ExternalSorter其中1后续会单独列出文章。3我们在write阶段已经讨论过。所以这里重点是第二个步骤，combine阶段。 如果你开启了 spark.shuffle.spill=true则使用ExternalAppendOnlyMap，否则使用AppendOnlyMap。两者的区别是，前者如果内存不够，则落磁盘，会发生spill操作，后者如果内存不够，直接OOM了。 这里我们会重点分析ExternalAppendOnlyMap。 ExternalAppendOnlyMap 作为内存缓冲数据的对象如下： private var currentMap = new SizeTrackingAppendOnlyMap[K, C]如果currentMap 对象向申请不到内存，就会触发spill动作。判定内存是否充足的逻辑和Shuffle Write 完全一致。 Combine做完之后，ExternalAppendOnlyMap 会返回一个Iterator，叫做ExternalIterator,这个Iterator背后的数据源是所有spill文件以及当前currentMap里的数据。 我们进去 ExternalIterator 看看，唯一的一个占用内存的对象是这个优先队列： private val mergeHeap = new mutable.PriorityQueue[StreamBuffer]mergeHeap 里元素数量等于所有spill文件个数加一。StreamBuffer 的结构： private class StreamBuffer( val iterator: BufferedIterator[(K, C)], val pairs: ArrayBuffer[(K, C)])其中iterator 只是一个对象引用，pairs 应该保存的是iterator里的第一个元素(如果hash有冲突的话，则为多个) 所以mergeHeap 应该不占用什么内存。到这里我们看看应该占用多少内存。依然假设 CoreNum 为 C,则 C 32k + C mergeHeap + C * SizeTrackingAppendOnlyMap所以这一段占用内存较大的依然是 SizeTrackingAppendOnlyMap ，一样的，他的值也符合如下公式 C SizeTrackingAppendOnlyMap &lt; ExecutorHeapMemeory 0.2 * 0.8ExternalAppendOnlyMap 的目的是做Combine,然后如果你还设置了Order,那么接着会启用 ExternalSorter 来完成排序。 经过上文对Shuffle Write的使用，相比大家也对ExternalSorter有一定的了解了，此时应该占用内存的地方最大不超过下面的这个值： C SizeTrackingAppendOnlyMap + C PartitionedAppendOnlyMap不过即使如此，因为他们共享一个shuffleMemoryManager，则理论上只有这么大： C SizeTrackingAppendOnlyMap &lt; ExecutorHeapMemeory 0.2 * 0.8分析到这里，我们可以做个总结： Shuffle Read阶段如果内存不足，有两个阶段会落磁盘，分别是Combine 和 Sort 阶段。对应的都会spill小文件，并且产生读。Shuffle Read 阶段如果开启了spill功能，则基本能保证内存控制在 ExecutorHeapMemeory 0.2 0.8 之内。后话如果大家对Sort Shuffle 落磁盘文件这块感兴趣，还可以看看这篇文章 Spark Shuffle Write阶段磁盘文件分析转载 http://www.jianshu.com/p/c83bb237caa8","categories":[{"name":"spark","slug":"spark","permalink":"https://blog.djstudy.net/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://blog.djstudy.net/tags/spark/"},{"name":"shuffle","slug":"shuffle","permalink":"https://blog.djstudy.net/tags/shuffle/"}]},{"title":"scala入门笔记","slug":"scala-rumen-biji","date":"2016-01-24T11:09:23.000Z","updated":"2016-01-27T10:39:15.000Z","comments":true,"path":"2016/01/24/scala-rumen-biji/","link":"","permalink":"https://blog.djstudy.net/2016/01/24/scala-rumen-biji/","excerpt":"","text":"Scala简介Scala 是一门多范式的编程语言, 由Martin Odersky 于2001年基于Funnel的工作开始设计Scala并于2004年正式发布Scala是一种纯面向对象的语言，每个值都是对象Scala是一门多范式编程语言, 支持命令交互式, 函数式, 面向对象编译型高性能语言(静态)与Java无缝兼容, 可以使用任何Java库 代码风格 函数和变量以小驼峰命名 类和特质以大驼峰命名 常量使用全大写命名 一般使用两格缩进 Scala大部分情况可以忽略语句末尾的分号Scala变量Scala中尽量避免使用变量, 函数式编程的一个重要特性是不可变性(不可变变量没有副作用)Scala是静态类型语言, 但是不需要显式的指明变量类型, Scala采用类型推断(Type Inference)1234//定义一个变量var x = 0// 定义val y = 1 Scala基本类型和操作String和值类型Byte, Short, Int, Long, Float, Double, Char, Boolean Scala的操作符不是特殊的语言语法, 任何方法都可以是操作符 操作符分为前缀, 中缀, 后缀 Scala中所有操作符都是方法调用1234567891011# 前缀scala&gt; (2.0).unary_-res1: Double = -2.0# 中缀scala&gt; x indexOf 'o'res0: Int = 4# 后缀scala&gt; val x = \"Hello, World\"x: String = Hello, Worldscala&gt; x.toLowerCaseres0: String = hello, world 中缀操作符的两个操作数, 一个在左一个在右前缀操作符方法名在操作符上加了unary_前缀(+, -, !, ~)后缀操作符是不用点或括号调用的不带任何参数的方法算术操作符: +, -, *, /, %关系, 逻辑和位操作: &gt;, &lt;, &gt;=, &lt;=, ==, !=, &amp;&amp;, ||, &amp;, |, ^, ~(反码)位移操作: &lt;&lt;, &gt;&gt;, &gt;&gt;&gt;(无符号右移) Scala函数函数式语言的一个主要特征是, 函数是第一类结构函数定义如下图:Unit 的结果类型指的是函数没有返回有用的值 函数式对象object和class的区别在于: object关键字创建一个单例对象主构造器是类的唯一入口, 只有主构造器可以调用超类构造器override关键字用于在重载父类的非抽象成员和成员函数同一个类内函数名相同而参数类型和个数不同的函数重载不需要override1234567891011121314151617181920212223242526272829303132333435class Rational (n: Int, d: Int) &#123; //precondition require(d != 0) // 私有成员 private val g = gcd(n.abs, d.abs) var numer: Int = n / g var denom: Int = d / g // auxiliary constructor, 相当于python中__init__构造函数 def this(n: Int) = this(n, 1) // 函数重载 override def toString = n + \"/\" + d; def add(other: Rational): Rational = new Rational(numer * other.denom + other.numer * denom, denom * other.denom) def -(other: Rational): Rational = new Rational(numer * other.denom - other.numer * denom, denom * other.denom) // 函数重载 def -(i: Int): Rational = new Rational(numer - i * denom, denom) def *(other: Rational): Rational = new Rational(numer * other.numer, denom * other.denom) def lessThan(other: Rational): Boolean = this.numer * other.denom &lt; other.numer * this.denom def max(other: Rational): Rational = if (lessThan(other)) other else this private def gcd(a: Int, b: Int): Int = if (b == 0) a else gcd(b, a % b)&#125;var x = new Rational(1, 3);var y = new Rational(5, 7);println(x add y)println(x * y)println(x - 1)// 隐式转换, 放在解释器方位内implicit def intToRational(x: Int) = new Rational(x)println(1 - x) 继承和多态继承多态和动态绑定特性动态绑定的特性即父类指针可以指向子类对象, 通过父类指针调用成员方法时, 会查找实际所指向的对象, 然后调用对象的内的对应方法12val el: Element = new ArrayElement(Array(\"hello\"))val e2: ArrayElement = new LineElement(\"hello\") 内建控制结构表示式会产生一个值Scala中if是能返回值的表达式, Scala中没有三元操作符, 但通过if (condition) var1 else var2 可以实现三元操作符的功能while和do-while被称为循环, 不产生有意义的结果Scala中for语句非常强大, for {子句} yield {循环体}match表达式可以产生值, match远强大与其他语言中的switch, 而且不需要显示的声明break变量范围: 大括号引入了一个新的范围, 内部变量会遮盖同名的外部变量占位符语法函数文本(匿名函数, 类似于python中的lamda)123456789101112131415161718192021222324252627282930313233343536var filename = if (!args.isEmpty) args(0) else \"default.txt\"var filesList = (new File(\".\")).listFiles// i &lt;- 1 to 4(包含4), i &lt;- 1 until 4 (不包含4)for (file &lt;- filesList if file.isFile; if file.getName.endsWith(\".scala\")) // 过滤器使用分号隔开 println(file) var firstArg = if(args.length &gt; 0) args(0) else \"\"// 任何种类的常量和其他都可以作为casefirstArg match &#123; case \"text\" =&gt; println(\"text\") case _ =&gt; println(\"default\")&#125;// 匿名函数的写法(lambda)scala&gt; var someNumbers = List(1, 2, 3, 4)scala&gt; someNumbers.filter(x =&gt; x % 2 == 0)// 占位符语法scala&gt; someNumbers.filter(_ % 2 == 0)// 偏函数 partial funcitonscala&gt; def sum(a: Int, b: Int, c: Int) = a + b + cscala&gt; val a = sum(1, _: Int, 3)scala&gt; a(2)// 变长参数(Array[String])scala&gt; def echo(args: String*) = for(arg &lt;- args) println(arg)scala&gt; echo(\"one\")scala&gt; echo(\"one\", \"two\")class DefaultConstructor ( name:String , age:Int)&#123; def this(name:String)&#123; /*自定义构造器，必需首先调用默认构造器*/ this(name , 24) ; &#125; def show()&#123; println( name + \"--&gt;\" + age ) ; &#125;&#125; 柯里化(carry)123456// 普通函数def sum(x: Int, y: Int) = x + y// 柯里化函数def sum(x: Int)(y: Int) = x + y// 实际执行def sum(x: Int) = (y: Int) =&gt; x + y 特质(trait)特质就像带有具体方法的java接口特质和抽象类的区别: 抽象类主要用于有明确的父子继承关系的类树, 而特质可以用于任何类特质定义使用trait关键字12345678trait Person() &#123; def detail() &#123; println(\"I'm angry!\") &#125;&#125;// 使用extends或with混入特质, 从特质继承的方法可以像从超类继承的方法使用class Student extends Person with Boy &#123;&#125; 数据结构Python中常用list, tuple, set, dictScala对应的数据结构为List, Tuple[X], Set, Map(HashMap)Scala中默认为不可变对象, 操作会生成一个新的对象 包Scala采用java平台的包机制使用import来进行引用1package com.zhihu.antispam 参考链接Difference between object and class in Scalascala 入门Scala 魔法函数Dependency Injection in ScalaDatabricks Scala 编程风格指南What are all the uses of an underscore in Scala? 转载 http://andrewliu.in/2016/01/16/Scala%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/","categories":[{"name":"scala","slug":"scala","permalink":"https://blog.djstudy.net/categories/scala/"}],"tags":[{"name":"scala","slug":"scala","permalink":"https://blog.djstudy.net/tags/scala/"}]},{"title":"将 Spark 中的文本转换为 Parquet 以提升性能","slug":"spark-convert-text-parquet","date":"2016-01-24T10:46:44.000Z","updated":"2016-01-24T11:03:24.000Z","comments":true,"path":"2016/01/24/spark-convert-text-parquet/","link":"","permalink":"https://blog.djstudy.net/2016/01/24/spark-convert-text-parquet/","excerpt":"","text":"列式存储布局（比如 Parquet）可以加速查询，因为它只检查所有需要的列并对它们的值执行计算，因此只读取一个数据文件或表的小部分数据。Parquet 还支持灵活的压缩选项，因此可以显著减少磁盘上的存储。如果您在 HDFS 上拥有基于文本的数据文件或表，而且正在使用 Spark SQL 对它们执行查询，那么强烈推荐将文本数据文件转换为 Parquet 数据文件，以实现性能和存储收益。当然，转换需要时间，但查询性能的提升在某些情况下可能达到 30 倍或更高，存储的节省可高达 75%！已有文章介绍使用 Parquet 存储为 BigSQL、Hive 和 Impala 带来类似的性能收益，本文将介绍如何编写一个简单的 Scala 应用程序，将现有的基于文本的数据文件或表转换为 Parquet 数据文件，还将展示给 Spark SQL 带来的实际存储节省和查询性能提升。 让我们转换为 Parquet 吧！Spark SQL 提供了对读取和写入 Parquet 文件的支持，能够自动保留原始数据的模式。Parquet 模式通过 Data Frame API，使数据文件对 Spark SQL 应用程序 “不言自明”。当然，Spark SQL 还支持读取已存储为 Parquet 的现有 Hive 表，但您需要配置 Spark，以便使用 Hive 的元存储来加载所有信息。在我们的示例中，不涉及 Hive 元存储。以下 Scala 代码示例将读取一个基于文本的 CSV 表，并将它写入 Parquet 表：1234567891011121314151617181920212223def convert(sqlContext: SQLContext, filename: String, schema: StructType, tablename: String) &#123; // import text-based table first into a data frame val df = sqlContext.read.format(\"com.databricks.spark.csv\"). schema(schema).option(\"delimiter\", \"|\").load(filename) // now simply write to a parquet file df.write.parquet(\"/user/spark/data/parquet/\"+tablename) &#125; // usage exampe -- a tpc-ds table called catalog_page schema= StructType(Array( StructField(\"cp_catalog_page_sk\", IntegerType,false), StructField(\"cp_catalog_page_id\", StringType,false), StructField(\"cp_start_date_sk\", IntegerType,true), StructField(\"cp_end_date_sk\", IntegerType,true), StructField(\"cp_department\", StringType,true), StructField(\"cp_catalog_number\", LongType,true), StructField(\"cp_catalog_page_number\", LongType,true), StructField(\"cp_description\", StringType,true), StructField(\"cp_type\", StringType,true))) convert(sqlContext, hadoopdsPath+\"/catalog_page/*\", schema, \"catalog_page\") 上面的代码将会读取 hadoopdsPath+”/catalog_page/* 中基于文本的 CSV 文件，并将转换的 Parquet 文件保存在 /user/spark/data/parquet/ 下。此外，转换的 Parquet 文件会在 gzip 中自动压缩，因为 Spark 变量 spark.sql.parquet.compression.codec 已在默认情况下设置为 gzip。您还可以将压缩编解码器设置为 uncompressed、snappy 或 lzo。 转换 1 TB 数据将花费多长时间？50 分钟，在一个 6 数据节点的 Spark v1.5.1 集群上可达到约 20 GB/分的吞吐量。使用的总内存约为 500GB。HDFS 上最终的 Parquet 文件的格式为：12/user/spark/data/parquet/catalog_page/part-r-00000-9ff58e65-0674-440a-883d-256370f33c66.gz.parquet/user/spark/data/parquet/catalog_page/part-r-00001-9ff58e65-0674-440a-883d-256370f33c66.gz.parquet 存储节省以下 Linux 输出显示了 TEXT 和 PARQUET 在 HDFS 上的大小比较：1234$ hadoop fs -du -h -s /user/spark/hadoopds1000g 897.9 G /user/spark/hadoopds1000g % hadoop fs -du -h -s /user/spark/data/parquet 231.4 G /user/spark/data/parquet 1 TB 数据的存储节省了将近 75%！ 查询性能提升Parquet 文件是自描述性的，所以保留了模式。要将 Parquet 文件加载到 DataFrame 中并将它注册为一个 temp 表，可执行以下操作：123val df = sqlContext.read.parquet(filename) df.show df.registerTempTable(tablename) 要对比性能，然后可以分别对 TEXT 和 PARQUET 表运行以下查询（假设所有其他 tpc-ds 表也都已转换为 Parquet）。您可以利用 spark-sql-perf 测试工具包来执行查询测试。举例而言，现在来看看 TPC-DS 基准测试中的查询 #76，12345678910111213141516171819202122232425262728293031(\"q76\", \"\"\" | SELECT | channel, col_name, d_year, d_qoy, i_category, COUNT(*) sales_cnt, | SUM(ext_sales_price) sales_amt | FROM( | SELECT | 'store' as channel, ss_store_sk col_name, d_year, d_qoy, i_category, | ss_ext_sales_price ext_sales_price | FROM store_sales, item, date_dim | WHERE ss_store_sk IS NULL | AND ss_sold_date_sk=d_date_sk | AND ss_item_sk=i_item_sk | UNION ALL | SELECT | 'web' as channel, ws_ship_customer_sk col_name, d_year, d_qoy, i_category, | ws_ext_sales_price ext_sales_price | FROM web_sales, item, date_dim | WHERE ws_ship_customer_sk IS NULL | AND ws_sold_date_sk=d_date_sk | AND ws_item_sk=i_item_sk | UNION ALL | SELECT | 'catalog' as channel, cs_ship_addr_sk col_name, d_year, d_qoy, i_category, | cs_ext_sales_price ext_sales_price | FROM catalog_sales, item, date_dim | WHERE cs_ship_addr_sk IS NULL | AND cs_sold_date_sk=d_date_sk | AND cs_item_sk=i_item_sk) foo | GROUP BY channel, col_name, d_year, d_qoy, i_category | ORDER BY channel, col_name, d_year, d_qoy, i_category | limit 100 查询时间如下：12TIME TEXT PARQUETQuery time (sec) 698 21 参考资料英文原文。在 developerWorks 大数据和分析专区，了解关于大数据的更多信息，获取技术文档、how-to 文章、培训、下载、产品信息以及其他资源。加入 developerWorks 中文社区，查看开发人员推动的博客、论坛、组和维基，并与其他 developerWorks 用户交流。","categories":[{"name":"spark","slug":"spark","permalink":"https://blog.djstudy.net/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://blog.djstudy.net/tags/spark/"},{"name":"parquet","slug":"parquet","permalink":"https://blog.djstudy.net/tags/parquet/"}]},{"title":"Spark Standalone模式HA环境搭建","slug":"spark-ha","date":"2016-01-24T08:10:30.000Z","updated":"2016-01-24T08:13:04.000Z","comments":true,"path":"2016/01/24/spark-ha/","link":"","permalink":"https://blog.djstudy.net/2016/01/24/spark-ha/","excerpt":"","text":"Spark Standalone模式常见的HA部署方式有两种：基于文件系统的HA和基于ZK的HA本篇只介绍基于ZK的HA环境搭建：1$ SPARK_HOME/conf/spark-env.sh 添加SPARK_DAEMON_JAVA_OPTS的配置信息：1export SPARK_DAEMON_JAVA_OPTS=\"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop000:2181,hadoop001:2181,hadoop002:2181 -Dspark.deploy.zookeeper.dir=/spark\" 配置参数说明：spark.deploy.recoveryMode: 设置恢复模式为zk，默认为NONEspark.deploy.zookeeper.url: 设置ZK集群的url，形如：192.168.1.100:2181,192.168.1.101:2181spark.deploy.zookeeper.dir: 设置zk保存恢复状态的路径，默认为spark实现HA的原理：利用ZK的Leader Election机制，选择一个Active状态的Master，其余的Master均为Standby状态；当Active状态的Master死掉后，通过ZK选举一个Standby状态的Master为Active状态。 测试步骤：启动standalone集群后，在各个Standby节点上启动start-master.sh，jps观察是否已经正确启动Master进程；将Active状态的Master kill掉，观察8080端口对应的页面，发现已经从Standby状态中选举出一个当作Active状态。采用ZK后由于会有多个Master，在提交任务时不知道哪个为Active状态的Master，可以采用如下的方式提交：12spark-shell –master spark://hadoop000:7077,hadoop001:7077,hadoop002:7077 –executor-memory 2g –total-executor-cores 1详细信息参见官方文档：http://spark.apache.org/docs/latest/spark-standalone.html#standby-masters-with-zookeeper","categories":[{"name":"spark","slug":"spark","permalink":"https://blog.djstudy.net/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://blog.djstudy.net/tags/spark/"},{"name":"ha","slug":"ha","permalink":"https://blog.djstudy.net/tags/ha/"}]},{"title":"spark on yarn","slug":"spark-on-yarn","date":"2016-01-24T06:43:59.000Z","updated":"2016-01-24T08:08:20.000Z","comments":true,"path":"2016/01/24/spark-on-yarn/","link":"","permalink":"https://blog.djstudy.net/2016/01/24/spark-on-yarn/","excerpt":"","text":"为什么要使用YARN?数据共享、资源利用率、更方便的管理集群等。详情参见：http://www.cnblogs.com/luogankun/p/3887019.html Spark YARN版本编译编译hadoop对应的支持YARN的Spark版本12$ export MAVEN_OPTS=\"-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m\"$ mvn clean package -DskipTests -Phadoop-2.3 -Dhadoop.version=2.3.0-cdh5.0.0 -Dprotobuf.version=2.5.0 -Pyarn -Phive 详情参见：Spark On YARN Spark的Cluster Manager负责管理启动executor进程，集群可以是Standalone、YARN和Mesos每个SparkContext（换句话说是：Application）对应一个ApplicationMaster（Application启动过程中的第一个容器ApplicationMaster负责和ResourceManager打交道，并请求资源，当获取资源之后通知NodeManager为其启动container； 每个Container中运行一个ExecutorBackendResourceManager决定哪些Application可以运行、什么时候运行以及在哪些NodeManager上运行； NodeManager的Container上运行executor进程在Standalone模式中有Worker的概念，而在Spark On YARN中没有Worker的概念由于executor是运行在container中，故container内存要大于executor的内存Spark On YARN有两种： yarn-clientClient和Driver运行在一起，ApplicationMaster只负责获取资源 Client会和请求到的资源container通信来调度他们进行工作，也就是说Client不能退出滴； 日志信息输出能输出在终端控制台上，适用于交互或者调试，也就是希望快速地看到application的输出，比如SparkStreaming yarn-clusterDriver和ApplicationMaster运行在一起；负责向YARN申请资源，并检测作业的运行状况；executor运行在container中 提交Application之后，即使关掉了Client，作业仍然会继续在YARN上运行 日志信息不会输出在终端控制台上 提交Spark作业到YARN提交命令1234567./bin/spark-submit \\ --class &lt;main-class&gt; --master &lt;master-url&gt; \\ --deploy-mode &lt;deploy-mode&gt; \\ ... # other options &lt;application-jar&gt; \\ [application-arguments] 提交本地jar提交到yarn-cluster/yarn-client123456./bin/spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master yarn-cluster \\ # can also be `yarn-client` for client mode --executor-memory 20G \\ --num-executors 50 \\ /path/to/examples.jar \\ 如果采用的是yarn-cluster的方式运行的话，想停止执行应用，需要去多个node上干掉；而在yarn-client模式运行时，只需要在client上干掉应用即可。提交到standalone123456./bin/spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master spark://207.184.161.138:7077 \\ --executor-memory 20G \\ --total-executor-cores 100 \\ /path/to/examples.jar \\ 提交hdfs上的jar123456$ ./bin/spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master yarn-cluster \\ # can also be `yarn-client` for client mode --executor-memory 20G \\ --num-executors 50 \\ hdfs://hadoop000:8020/lib/examples.jar \\ 如果没有在spark-env.sh文件中配置HADOOP_CONF_DIR或者YARN_CONF_DIR，可以在提交作业前指定形如1234567$ export HADOOP_CONF_DIR=XXX$ ./bin/spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master yarn-cluster \\ # can also be `yarn-client` for client mode --executor-memory 20G \\ --num-executors 50 \\ /path/to/examples.jar 详情参见：http://spark.apache.org/docs/latest/submitting-applications.html","categories":[{"name":"spark","slug":"spark","permalink":"https://blog.djstudy.net/categories/spark/"}],"tags":[{"name":"yarn","slug":"yarn","permalink":"https://blog.djstudy.net/tags/yarn/"},{"name":"spark","slug":"spark","permalink":"https://blog.djstudy.net/tags/spark/"}]},{"title":"ambari hdp中部署apache spark运行spark shell遇到的错误解决","slug":"hdp-spark-shell-error","date":"2016-01-24T05:05:21.000Z","updated":"2016-01-24T08:04:10.000Z","comments":true,"path":"2016/01/24/hdp-spark-shell-error/","link":"","permalink":"https://blog.djstudy.net/2016/01/24/hdp-spark-shell-error/","excerpt":"","text":"在运行spark-shell中遇到的ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library 解决方法1$ bin/spark-shell –driver-library-path :/usr/hdp/2.2.4.2-2/hadoop/lib/native/Linux-amd64-64 /usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jar 在运行spark-shell中遇到的Compression codec com.hadoop.compression.lzo.LzoCodec not found 错误可以配置文件spark-defaults.confspark.executor.extraClassPath /usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jarspark.driver.extraClassPath /usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jar保存文件重启spark服务集群即可。再提供一个Unable to load native-hadoop library 和 Snappy native library not loaded的解决方案。这个问题主要是jre目录下缺少了libgplcompression.so , libhadoop.so和libsnappy.so两个文件。具体是，spark-shell依赖的是scala，scala依赖的是JAVA_HOME下的jdk，libhadoop.so和libsnappy.so两个文件应该放到JAVA_HOME/jre/lib/amd64下面。要注意的是要知道真正依赖到的JAVA_HOME是哪一个，把两个.so放对地方。这两个so：libhadoop.so和libsnappy.so。前一个so可以在HADOOP_HOME下找到，比如hadoop\\lib\\native\\Linux-amd64-64。第二个libsnappy.so需要下载一个snappy-1.1.0.tar.gz，然后./configure，make编译出来。snappy是google的一个压缩算法，在hadoop jira下https://issues.apache.org/jira/browse/HADOOP-7206记录了这次集成。","categories":[{"name":"hadoop","slug":"hadoop","permalink":"https://blog.djstudy.net/categories/hadoop/"}],"tags":[{"name":"yarn","slug":"yarn","permalink":"https://blog.djstudy.net/tags/yarn/"},{"name":"spark","slug":"spark","permalink":"https://blog.djstudy.net/tags/spark/"},{"name":"ambari","slug":"ambari","permalink":"https://blog.djstudy.net/tags/ambari/"}]}]}